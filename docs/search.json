[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "documentation.html",
    "href": "documentation.html",
    "title": "Documentation",
    "section": "",
    "text": "Github: Sign up for github account and email me to give you access to GCAM-KAIST\nGit client: download a git client of your choice.\n\nTortoiseGit : old school client\nGithub desktop : standard github client\ngit for windows : command line based professional git client\n\nXML marker 1.1: make sure you get the older 1.1 version\nR\n\n\n\n\n\nGCAM tutorial\nGit\nGCAM\ngcamdata\nUseful guide/PPT slides from gcam-training\nvideos\n\n\n\n\n\n\nGCAM\nDownload standard GCAM release\nSet up some prerequisite apps and run!\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nif anything on github is confusing, just ask chatGPT!\n\n\n\nGo to GCAM-KAIST.\nPick a branch to start from (currently: hcm/proj/kaist7update).\nCreate your own branch [you own this. No one else will make changes to your own branch].\ngit clone [this creates a full repo of GCAM-KAIST].\ngit checkout [your branch name] [this sets your branch on your own computer].\nTry making a small inconsequential change. Such as adding a small xml file in the input/policy folder.\ngit add [file you added].\ngit commit [this will commit all files you added/changed].\ngit push.\nConfirm that your commit has been correctly pushed to github.\n(if you are happy with your development, and would like it to be applied to ALL GCAM-KAIST users) create a pull request (PR) [back into the main GCAM-KAIST repo].\nAssign me (d3y419) as the reviewer. I’ll review the PR and merge as needed.\n\n\n\n\nAs we embark on an exciting and crucial phase of updating our energy assessment model from GCAM-KAIST 2.0 to GCAM-KAIST 7.0, the following instructions and guidelines will ensure a smooth transition. Each one of the team is responsible for a particular XML file which is identified in the following link.\n\nDownload the Required Versions\n\nGCAM 7.0 Official Release by JCRI: You can download this latest version from the following link.\nGCAM-KAIST 2.0: This is our specialized version at KAIST. Please download it using the link .\n\nAnalyze the Differences\n\nXML File Comparison: Please compare the XML files between the two versions as per your specific task in the google sheet. Document the Differences: Note any discrepancies, changes, or new features. This will help us understand what needs to be transferred or adapted.\n\nUpdate GCAM 7.0 with GCAM-KAIST 2.0 Data\n\nModify the Original GCAM 7.0 XML Files: Replace the necessary data with the corresponding data from GCAM-KAIST 2.0. Follow the Evolution Process: This is vital to complete the transformation from GCAM-KAIST 2.0 to GCAM-KAIST 7.0 by checking the “comparison with GCAM-KAIST2” column in google sheet. After running the scenario, you can check if the results are different from GCAM-KAIST2.0 or not.\n\nPrepare for Tomorrow’s Meeting\n\nClarification of Differences: Be prepared to discuss the differences you’ve identified and how you’ve addressed them."
  },
  {
    "objectID": "GGS621.html",
    "href": "GGS621.html",
    "title": "GGS621",
    "section": "",
    "text": "Syllabus"
  },
  {
    "objectID": "GGS621.html#ammonia-for-shipping-fuels",
    "href": "GGS621.html#ammonia-for-shipping-fuels",
    "title": "GGS621",
    "section": "1.2 Ammonia for shipping fuels",
    "text": "1.2 Ammonia for shipping fuels\nTitle: Using ammonia as a shipping fuel could disturb the nitrogen cycle(Wolfram et al. 2022)\n\n\nSummarize the key findings\n\nAmmonia(NH3) has been proposed as a shipping fuel. But potential adverse side-effects is not deeply discussed. Also,excessive reactive notrigen(Nr, 반응성 질소) in water and air leads to environmental damages such as eutrophication(부영양화) and air pollution.\nNr usually emis N2O as a by-product, and N2O itself ia a potent GHG with a global warming potential of about 265-298 over a 100-year time horizon.\nIf nitrogen releases from ammonia are not tightly controlled, the scale of the demands of maritime shipping fuel are such that the technology could significantly alter the global nitrogen cycle.\n\n\n\nStrengths\n\nStress the potential adverse side-effects of using ammonia that is not considered enough.\nCalculated the amount of Nr caused by NH3 shipping fuels and that would negate climate benefits of NH3.\nFound a key question of what portion of the NH3 and NOx emissions will indirectly resolve to N2O on a multi-year timescale.\n\n\n\nWeakness\n\nOnly focuses on maritime emissions. But there are other N2O emission sectors.\nDoes not calculate what portion of the NH3 and NOx emissions will indirectly resolve to N2O on a multi-year timescale.\nNo punchline figure.\n\n\n\nHow to improve upon it for my own research\n\nTrack other fuels that cause N2O emission in other emission sectors, such as agriculture, land use, transportation, industry, etc."
  },
  {
    "objectID": "GGS621.html#direct-air-capture-with-carbon-storagedaccs",
    "href": "GGS621.html#direct-air-capture-with-carbon-storagedaccs",
    "title": "GGS621",
    "section": "1.3 Direct air capture with carbon storage(DACCS)",
    "text": "1.3 Direct air capture with carbon storage(DACCS)\n\n\nThe role of direct air capture and negative emissions technologies in the shared socioeconomic pathways towards +1.5 ◦C and +2 ◦C futures(Fuhrman et al. 2021)\n\n\n\n\n\n\n\nShared Socioeconomic Pathway\n\n\n\nThe shared socioeconomic pathway (SSP) framework defines five storylines that differ in the challenges for mitigation and adaptation, resulting in different levels of long-term warming in the absence of global climate policies\n\n\n\nSummarize the key findings\n\nIAM scenarios to date have relied almost on bioenergy with carbon capture and storage (BECCS) and afforestation/reforestation for negative emissions. But, land-intensive strategies could have large impacts on global agricultural and natural biological system.\nThis study use GCAM to understand the role of direct air capture with carbon storage (DACCS) across all 5 SSPs for the below 2 ◦C and below 1.5 ◦C end-of-century warming goals.\n\n\n\n\n\n\n\nTwo constraints\n\n\n\nImposed on end-of-century radiative forcing increases from the pre-industrial levels: +2.6 W m−2, consistent with limiting warming in 2100 to below +2◦C, and +1.9 W m−2(below 1.5◦C in 2100).\nThe discount rate.\n\n\n\nDACCS could play up to a tens of GtCO2 yr−1 role in many of these scenarios, particularly those with delayed climate policy and/or higher challenges to emissions mitigation.\n\n\n\nStrengths\n\nTwo different(high and low) DACCS technology details are considered.\n\nAssesed a DACCS process requiring high temperature heat from natural gas combustion, electricity, and water could contribute to both ambitious near-term and delayed mitigation scenarios that limit end-of century warming to below +1.5 ◦C.\nThe low-temperature DACCS process is assumed to use solid sorbents and not require water input\n\nExogenous treatment of GCAM allows sensitivity analysis of cost or efficiency targets for different technologries.\n\nOther IAMs (e.g. WITCH, MERGE-ETL) endogenize these changes in cost and performance in an attempt to capture technological development in response to economic incentives\n\n\nWeakness\n\nThese scenarios relied almost solely on BECCS and afforestation for negative emissions because structures for modeling alternative pathways were not included, constituting a limitation in these scenario designs.\n\n\n\n\n\nHow to improve upon it for my own research"
  },
  {
    "objectID": "GGS621.html#paper-title",
    "href": "GGS621.html#paper-title",
    "title": "GGS621",
    "section": "3.1 paper title",
    "text": "3.1 paper title\n\n\n\n\nSummarize the key findings\n\n\n\n\n\n\n\nStrengths\n\n\n\n\n\n\n\nWeakness\n\n\n\n\n\n\n\nHow to improve upon it for my own research"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "안지석",
    "section": "",
    "text": "Post With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nDec 19, 2022\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nDec 16, 2022\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "ITM501.html",
    "href": "ITM501.html",
    "title": "Innovation Management lecture",
    "section": "",
    "text": "매주 수업과 관련된 주요 개념 및 논문 1편 요약 제출 (분량 무관) - KLMS에 숙제 제출 및 강의자료 다운!/\n\n금요일 자정까지 제출\n모든 분들은 한 학기 2번씩 발표하게 됨 (5-7분)\n요약, 가설, 연구방법, 결과, 결론, 개인 의견\n\nMidterm 10/21 휴강\n실제 시험은 11/18 or 11/25 (시험의 비중은 60%) - handwriting\n\n서술 : 핵심 이론에 대한 설명 15문제\n에세이 : 주어진 기사에 숨겨있는 혁신원리, 이론을 찾아내고, 관련 현상 설명\n\nFinal report는 지금 신경 쓰지 마세요.\n\n특허 자료를 통해 수업시간에 배운 이론 실증\n제출 시 표절검사 결과 함께 제출 e.g., copykiller . 표절률 10~15%까지 허용\n11월 정도에 기말 주제 공지 예정\n\n\n\n\n\n\n혁신 역량의 차이\n\n디지털 경제는 기존 경제와 완전히 다르다. 디지털 기술의 속성을 모르면 디지털 경제을 알 수 없다. 디지털은 한계체증을 한다(marginal utility, 한계체감이 제로로간다). -> 그래서 플랫폼 기업들이 생긴다. 네이버가 고객 한명을 더 받는데 비용이 얼마나 들까? 거의 0 일것이다.\n테슬라가 노리는 것? 전기차 -> OS게임. 안드로이드/IOS 처럼… 네비게이션을 사면 자동차를 주는 세상?\n삼천리자전거. 여태까지는 B2C에 최적화되었음. 하지만 디지털 경제의 공유사업을 통해 공유자전거가 생기고 나서 B2B가 되었음. Customer가 아닌 플랫폼 기업Business에 팔아야 하는 것임.\n디지털 위에서는 패러다임이 바뀐다. 그래서 기술경영이 중요. 기술(제조)과 경영.\n\n혁신도 하나의 이론. Innovation Theories.\n\n세계 탑 저널은 혁신이론을 기반으로 좋은 페이퍼가 나오고 있음.\n사회과학은 누적 지식을 바탕으로 원전(original text)을 깔 수있음. 아담스미스의 국부론은 지금 시대에 맞지 않아! 라는 얘기 가능.\n혁신의 아버지 조지프 슘페터. 슘페터로 부터 시작. 미국 대공황. 그 시대는 사회주의 - 자본주의가 격동할 때며, 누가 정답인지 모를 때. 소련의 사회주의, 미국의 자본주의 둘 다 잘 살았고 두 체제의 우월성이 확연하지 않았음. 미국의 fordism을 다른 국가들이 보고 자본주의 체제를 따라가게 됨.\n케인즈 - 수요, 슘페터 - 공급. 케인즈 수요정책, 통화정책. 슘페터는 기업의 혁신. 케인즈 소득주도, 슘페터 혁신 성장론.\n혁신성장은 구조적으로 기업의 체질을 바꿔야 하기 때문에 효과를 보기에 오래 걸린다. 정권 초기에는 혁신 얘기를 하고, 정권 말기에는 숫자로 나타나야 하니까 수요/통화정책을 다룬다. 이념 얘기는 아니다.\n국가의 통화량을 얘기할 때 MV=PY. 통화량 법칙. 통화량을 높이면 GDP가 성장하는 것처럼 보인다. 국가적으로 볼 때 fake 쓰기 좋다.\n\nM: 통화량, V: 화폐유통속도(거의 상수), P:가격(물가상승률, 이자율), Y:생산량(GDP)\n\n시장은 한번도 완전한 적이 없다. 개입에 관해 결정하는 것이 정책. 정책은 어떻게 균형상태를 이끌어 갈 것인가. 시장이 실패하기 쉬운 곳에 정책이 많이 들어간다. 20년 전만 해도 자유주의 팽배하였으나, 지금은 변화함.\nHistory of Innovation Studies 강의 슬라이드\n\n\nScience & Technology\nIn Social Science (Innovation study)\n\n혁신이론의 순서. 혁신연구는 과학에서 기술로 가고, 기술에서 사업화로 가야함. 과학을 통해 기술을 만들고, 기술을 통해 사업화를 이루어내야 함.\n과학의 주체는 어디? 대학. 과학의 측정? 논문으로. 과학을 기술로 바꾸는 역할? 기업 혹은 기업부설연구소. 기술의 측정? 특허. 출연연구소의 역할은 과학을-> 기술로.\nClarivate(Web of Science) KAIST 학생 사용 가능!! 제일 좋은 특허 사이트.\n\n다음주 숙제\n\n슘페터 가설이란?\n슘페터 가설과 관련된 논문 1편 요약 1p 이상\n\n\n\n\n\n\nIntroduction\n\n(The author’s name(s) and the title of the article) Determinants of innovation in energy intensive industry and implications for energy policy, 2015년 Energy Policy에 발간된 논문(Song and Oh 2015)\n(The author’s main point) This study aims to emphasize the importance of innovation and analyze the impact of Research and Development (R&D) on both product innovation and process innovation in energy intensive industry(EII) and compare it to less EII.\n(A thesis statement that previews your analysis) It was Schumpeter who brought determinants of innovation into the context of mainstream economics. Schumpeter(Schumpeter 1934) argued that large firms with monopolistic power in a concentrated market are more likely to promote technological innovation. This is the famous Schumpeterian hypothesis, which presents firm size and market structure as the main determinants of innovation.\n\nSummary\n\n(The main points of the article) Use data from the 2008 Korea Innovation Survey to estimate the impact of key economic variables (e.g. firm size and market structure) that are proposed in the Schumpeterian hypothesis, along with other economic variables on innovation in EII.\nBased on the empirical results, we uncover the implications that energy policies have on facilitating innovation in EII.\nR&D will surely facilitate innovation in EII.\n\nCritique\n\n(Limitations) Followed by the OECD Oslo manual, the KIS2008 measures technological innovation using a dummy variable to check whether a firm performs product or process innovations. The outputs of innovations are very heterogeneous in terms of quality.\nThe insignificant effect of market concentration on product and process innovations in EII may be caused by the relatively short sample period.\nTo extend the results of the study, further studies can examine this through different analytical methods, such as a panel regression or a Logit model.\n\nConclusion\n\nR&D personnel ratio has a positive and statistically significant effect on both product and process innovations. While only R&D intensity appears to have a significant effect on process innovation in EII. This means that for improving process innovation, increasing both manpower and monetary expenditures in R&D programs is necessary.\nproduct innovation can be enhanced by adding R&D manpower with relatively less financial investment. It is clear that process innovation is more costly and time consuming than product innovation.\nAs for the relationship between firm size and innovation, interestingly, firm size has no impact on product innovation, but a positive effect on process innovation.\n\nThe Probit model is described as the relationship between the explanatory variables and the dependent variable that has a value of 0 or 1\n\nDeterminants of innovation in energy intensive industry and implications for energy policy(위 homework 한글로 작성한 것임)\n\n들어가며\n\n본 논문 분석은 슘페터 가설과 관련된 논문을 요약하는 2023년 가을학기 이노베이션 경영 1주차 과제이며, 분석 대상 논문은 에너지집약산업에서 연구개발의 영향과 혁신의 중요성을 분석한 논문으로 2015년 Energy Policy에 개제되었고 인용수는 ’23년 9월 현재 89회를 기록하고 있다.\n\n주요 내용요약\n\n본 연구는 연구개발(Research & Development, R&D)이 에너지집약산업(Energy Intensive Industry, EII)에서의 제품 혁신(process innovation)과 공정 혁신(process innovation)에 미치는 영향을 분석하고, 혁신의 중요성을 강조한다. 그리고 이 영향을 비 에너지집약산업과 비교한다.\n혁신의 결정요인을 주류 경제학으로 끌고 온 사람은 슘페터이다. 슘페터는 집중된 시장에서 독점력을 가진 대기업이 기술 혁신을 촉진할 가능성성이 높다고 주장하였다. 이는 유명한 슘페터 가설로, 기업의 규모와 시장 구조를 혁신의 주요 결정요인으로 제시하였다.\n2008년 한국의 기술혁신조사의 데이터를 사용하여 에너지집악산업의 혁신에 대한 경제 변수와 함께 슘페터 가설에서 제안된 주요 경제 변수(예: 기업 규모 및 시장 구조)가 혁신에 미치는 영향을 추정한다.\n실증적 결과를 바탕으로 우리는 에너지 정책이 에너지집약산업의 혁신 촉진에 미치는 영향을 밝히며, R&D는 에너지집약산업의 혁신을 촉진할 것이라는 점을 밝힌다.\n\n결과\n\n에너지집약산업과 비에너지집약산업에 회귀분석을 수행하였고, 분석에 사용된 변수는 firm size, squared firm size, market concentration, export ratio, capital ratio, advertising intensity, R&D intensity, R&D personnel ratio로 구분된다. 분석 결과는 다음과 같이 4가지로 구분하여 제시한다.\nR&D 집약도와 제품 혁신, R&D 인력과 제품 혁신, R&D 집약도와 공정 혁신, R&D 인력과 공정 혁신\nR&D 인력 비율은 제품 및 공정 혁신 모두에 통계적으로 유의미한 영향을 미치나, R&D 집약도는 에너지집약산업의 공정 혁신에만 중요한 영향을 미치는 것으로 보인다. 이는 공정 혁신을 개선하기 위해서는 R&D 프로그램에서 인력과 금전적 지출을 모두 늘려야 함을 의미한다. 반면, 제품 혁신은 상대적으로 적은 재정 투자로 R&D 인력을 추가함으로써 강화할 수 있다. 공정 혁신이 제품 혁신보다 비용과 시간이 더 많이 소요되는 것이다.\n기업 규모와 혁신의 관계를 살펴보면, 흥미롭게도 기업 규모는 제품 혁신에는 영향을 미치지 않지만 공정 혁신에는 긍정적인 영향을 미친다. 큰 규모의 재정적 투자가 필요한 R&D는 중소기업들에게는 쉽지 않을 것이기 때문에, 공정 혁신을 달성하기는 보다는 제품 혁신을 달성해야 할 것이다.\n\n결론\n\nOECD 오슬로 매뉴얼에 따라 KIS2008에서는 기업이 제품 혁신을 수행하는지, 공정 혁신을 수행하는지 확인하기 위해 더미변수를 사용하여 기술 혁신을 측정하였다. 이렇게 측정된 혁신의 결과는 신뢰도 측면에서 매우 이질적일 수 있다. 또한, 에너지집약산업의 혁신에 대한 시장 집중도의 미미한 영향은 상대적으로 짧은 표본 기간으로 인해 발생한 것일 수도 있다.\nHermansen (2011)에 따르면 소기업과 대기업은 중기업보다 혁신을 추진하기 쉬우며, 기업의 규모에 따라 혁신활동에 미치는 영향이 다를 것이다. 향후 연구에서는 기업의 규모와 산업에 따른 혁신의 결정요인들을 분석해볼 수 있을 것이다. 또한, 패널 회귀 또는 로짓 모델과 같은 다양한 분석 방법을 통해 이를 조사를 통해 연구 결과를 확장할 수 있을 것이다."
  },
  {
    "objectID": "ITM501.html#lecture",
    "href": "ITM501.html#lecture",
    "title": "ITM501 - Innovation Management",
    "section": "2.1 Lecture",
    "text": "2.1 Lecture\n\nWhy do firms differ -> 혁신역량의 차이.\n\n디지털 경제는 기존 경제와 완전히 다르다. 디지털 기술의 속성을 모르면 디지털 경제을 알 수 없다. 디지털은 한계체증을 한다(marginal utility, 한계체감이 제로로간다). -> 그래서 플랫폼 기업들이 생긴다. 네이버가 고객 한명을 더 받는데 비용이 얼마나 들까? 거의 0 일것이다.\n테슬라가 노리는 것? 전기차 -> OS게임. 안드로이드/IOS 처럼… 네비게이션을 사면 자동차를 주는 세상?\n삼천리자전거. 여태까지는 B2C에 최적화되었음. 하지만 디지털 경제의 공유사업을 통해 공유자전거가 생기고 나서 B2B가 되었음. Customer가 아닌 플랫폼 기업Business에 팔아야 하는 것임.\n디지털 위에서는 패러다임이 바뀐다. 그래서 기술경영이 중요. 기술(제조)과 경영.\n\n혁신도 하나의 이론. Innovation Theories.\n\n세계 탑 저널은 혁신이론을 기반으로 좋은 페이퍼가 나오고 있음.\n사회과학은 누적 지식을 바탕으로 원전(original text)을 깔 수있음. 아담스미스의 국부론은 지금 시대에 맞지 않아! 라는 얘기 가능.\n혁신의 아버지 조지프 슘페터. 슘페터로 부터 시작. 미국 대공황. 그 시대는 사회주의 - 자본주의가 격동할 때며, 누가 정답인지 모를 때. 소련의 사회주의, 미국의 자본주의 둘 다 잘 살았고 두 체제의 우월성이 확연하지 않았음. 미국의 fordism을 다른 국가들이 보고 자본주의 체제를 따라가게 됨.\n케인즈 - 수요, 슘페터 - 공급. 케인즈 수요정책, 통화정책. 슘페터는 기업의 혁신. 케인즈 소득주도, 슘페터 혁신 성장론.\n혁신성장은 구조적으로 기업의 체질을 바꿔야 하기 때문에 효과를 보기에 오래 걸린다. 정권 초기에는 혁신 얘기를 하고, 정권 말기에는 숫자로 나타나야 하니까 수요/통화정책을 다룬다. 이념 얘기는 아니다.\n국가의 통화량을 얘기할 때 MV=PY. 통화량 법칙. 통화량을 높이면 GDP가 성장하는 것처럼 보인다. 국가적으로 볼 때 fake 쓰기 좋다.\n\nM: 통화량, V: 화폐유통속도(거의 상수), P:가격(물가상승률, 이자율), Y:생산량(GDP)\n\n시장은 한번도 완전한 적이 없다. 개입에 관해 결정하는 것이 정책. 정책은 어떻게 균형상태를 이끌어 갈 것인가. 시장이 실패하기 쉬운 곳에 정책이 많이 들어간다. 20년 전만 해도 자유주의 팽배하였으나, 지금은 변화함.\nHistory of Innovation Studies 강의 슬라이드\n\n\nScience & Technology\nIn Social Science (Innovation study)\n\n혁신이론의 순서. 혁신연구는 과학에서 기술로 가고, 기술에서 사업화로 가야함. 과학을 통해 기술을 만들고, 기술을 통해 사업화를 이루어내야 함.\n과학의 주체는 어디? 대학. 과학의 측정? 논문으로. 과학을 기술로 바꾸는 역할? 기업 혹은 기업부설연구소. 기술의 측정? 특허. 출연연구소의 역할은 과학을-> 기술로.\nClarivate(Web of Science) KAIST 학생 사용 가능!! 제일 좋은 특허 사이트.\n\n다음주 숙제\n\n슘페터 가설이란?\n슘페터 가설과 관련된 논문 1편 요약 1p 이상"
  },
  {
    "objectID": "ITM501.html#homework",
    "href": "ITM501.html#homework",
    "title": "ITM501 - Innovation Management",
    "section": "2.2 Homework",
    "text": "2.2 Homework\n\nIntroduction\n\n(The author’s name(s) and the title of the article) Determinants of innovation in energy intensive industry and implications for energy policy, 2015년 Energy Policy에 발간된 논문(Song and Oh 2015)\n(The author’s main point) This study aims to emphasize the importance of innovation and analyze the impact of Research and Development (R&D) on both product innovation and process innovation in energy intensive industry(EII) and compare it to less EII.\n(A thesis statement that previews your analysis) It was Schumpeter who brought determinants of innovation into the context of mainstream economics. Schumpeter(Schumpeter 1934) argued that large firms with monopolistic power in a concentrated market are more likely to promote technological innovation. This is the famous Schumpeterian hypothesis, which presents firm size and market structure as the main determinants of innovation.\n\nSummary\n\n(The main points of the article) Use data from the 2008 Korea Innovation Survey to estimate the impact of key economic variables (e.g. firm size and market structure) that are proposed in the Schumpeterian hypothesis, along with other economic variables on innovation in EII.\nBased on the empirical results, we uncover the implications that energy policies have on facilitating innovation in EII.\nR&D will surely facilitate innovation in EII.\n\nCritique\n\n(Limitations) Followed by the OECD Oslo manual, the KIS2008 measures technological innovation using a dummy variable to check whether a firm performs product or process innovations. The outputs of innovations are very heterogeneous in terms of quality.\nThe insignificant effect of market concentration on product and process innovations in EII may be caused by the relatively short sample period.\nTo extend the results of the study, further studies can examine this through different analytical methods, such as a panel regression or a Logit model.\n\nConclusion\n\nR&D personnel ratio has a positive and statistically significant effect on both product and process innovations. While only R&D intensity appears to have a significant effect on process innovation in EII. This means that for improving process innovation, increasing both manpower and monetary expenditures in R&D programs is necessary.\nproduct innovation can be enhanced by adding R&D manpower with relatively less financial investment. It is clear that process innovation is more costly and time consuming than product innovation.\nAs for the relationship between firm size and innovation, interestingly, firm size has no impact on product innovation, but a positive effect on process innovation.\n\nThe Probit model is described as the relationship between the explanatory variables and the dependent variable that has a value of 0 or 1\n\n\nDeterminants of innovation in energy intensive industry and implications for energy policy\n20235575 안지석\n\n들어가며\n\n본 논문 분석은 슘페터 가설과 관련된 논문을 요약하는 2023년 가을학기 이노베이션 경영 1주차 과제이며, 분석 대상 논문은 에너지집약산업에서 연구개발의 영향과 혁신의 중요성을 분석한 논문으로 2015년 Energy Policy에 개제되었고 인용수는 ’23년 9월 현재 89회를 기록하고 있다.\n\n주요 내용요약\n\n본 연구는 연구개발(Research & Development, R&D)이 에너지집약산업(Energy Intensive Industry, EII)에서의 제품 혁신(process innovation)과 공정 혁신(process innovation)에 미치는 영향을 분석하고, 혁신의 중요성을 강조한다. 그리고 이 영향을 비 에너지집약산업과 비교한다.\n혁신의 결정요인을 주류 경제학으로 끌고 온 사람은 슘페터이다. 슘페터는 집중된 시장에서 독점력을 가진 대기업이 기술 혁신을 촉진할 가능성성이 높다고 주장하였다. 이는 유명한 슘페터 가설로, 기업의 규모와 시장 구조를 혁신의 주요 결정요인으로 제시하였다.\n2008년 한국의 기술혁신조사의 데이터를 사용하여 에너지집악산업의 혁신에 대한 경제 변수와 함께 슘페터 가설에서 제안된 주요 경제 변수(예: 기업 규모 및 시장 구조)가 혁신에 미치는 영향을 추정한다.\n실증적 결과를 바탕으로 우리는 에너지 정책이 에너지집약산업의 혁신 촉진에 미치는 영향을 밝히며, R&D는 에너지집약산업의 혁신을 촉진할 것이라는 점을 밝힌다.\n\n결과\n\n에너지집약산업과 비에너지집약산업에 회귀분석을 수행하였고, 분석에 사용된 변수는 firm size, squared firm size, market concentration, export ratio, capital ratio, advertising intensity, R&D intensity, R&D personnel ratio로 구분된다. 분석 결과는 다음과 같이 4가지로 구분하여 제시한다.\nR&D 집약도와 제품 혁신, R&D 인력과 제품 혁신, R&D 집약도와 공정 혁신, R&D 인력과 공정 혁신\nR&D 인력 비율은 제품 및 공정 혁신 모두에 통계적으로 유의미한 영향을 미치나, R&D 집약도는 에너지집약산업의 공정 혁신에만 중요한 영향을 미치는 것으로 보인다. 이는 공정 혁신을 개선하기 위해서는 R&D 프로그램에서 인력과 금전적 지출을 모두 늘려야 함을 의미한다. 반면, 제품 혁신은 상대적으로 적은 재정 투자로 R&D 인력을 추가함으로써 강화할 수 있다. 공정 혁신이 제품 혁신보다 비용과 시간이 더 많이 소요되는 것이다.\n기업 규모와 혁신의 관계를 살펴보면, 흥미롭게도 기업 규모는 제품 혁신에는 영향을 미치지 않지만 공정 혁신에는 긍정적인 영향을 미친다. 큰 규모의 재정적 투자가 필요한 R&D는 중소기업들에게는 쉽지 않을 것이기 때문에, 공정 혁신을 달성하기는 보다는 제품 혁신을 달성해야 할 것이다.\n\n결론\n\nOECD 오슬로 매뉴얼에 따라 KIS2008에서는 기업이 제품 혁신을 수행하는지, 공정 혁신을 수행하는지 확인하기 위해 더미변수를 사용하여 기술 혁신을 측정하였다. 이렇게 측정된 혁신의 결과는 신뢰도 측면에서 매우 이질적일 수 있다. 또한, 에너지집약산업의 혁신에 대한 시장 집중도의 미미한 영향은 상대적으로 짧은 표본 기간으로 인해 발생한 것일 수도 있다.\nHermansen (2011)에 따르면 소기업과 대기업은 중기업보다 혁신을 추진하기 쉬우며, 기업의 규모에 따라 혁신활동에 미치는 영향이 다를 것이다. 향후 연구에서는 기업의 규모와 산업에 따른 혁신의 결정요인들을 분석해볼 수 있을 것이다. 또한, 패널 회귀 또는 로짓 모델과 같은 다양한 분석 방법을 통해 이를 조사를 통해 연구 결과를 확장할 수 있을 것이다."
  },
  {
    "objectID": "ITM812.html",
    "href": "ITM812.html",
    "title": "머신러닝과 사업기회 - ITM812",
    "section": "",
    "text": "30분만에 첫 강의 끝"
  },
  {
    "objectID": "ITM812.html#과정-소개",
    "href": "ITM812.html#과정-소개",
    "title": "머신러닝과 사업기회 - ITM812",
    "section": "2.1 과정 소개",
    "text": "2.1 과정 소개\n\n데이터가 많이 쌓이고, 선제적 의사결정 지원, Data-Driven Decision 중요\n질 좋은, 많은 데이터 활용 중요\nData Exploration, Machine Learning 개념 및 실습, Internal/External Insight 발견, 프로세스의 효율적 개선과 경쟁력 향상을 위한 역량 향상 목표\nPython을 활용한 머신러닝 기반 데이터 분석 실습하고 실제 적용을 위한 모델링 수행\nPython 3를 기반으로 실습"
  },
  {
    "objectID": "ITM812.html#평가기준",
    "href": "ITM812.html#평가기준",
    "title": "머신러닝과 사업기회 - ITM812",
    "section": "2.2 평가기준",
    "text": "2.2 평가기준\n\nTerm projects : 50%, Peer-review\nMid Term : 30%,\n출석,참여도: 20%"
  },
  {
    "objectID": "ITM812.html#강의-구성",
    "href": "ITM812.html#강의-구성",
    "title": "머신러닝과 사업기회 - ITM812",
    "section": "2.3 강의 구성",
    "text": "2.3 강의 구성\n\nRegression, Classification, etc."
  },
  {
    "objectID": "ITM812.html#data-analytic-maching-learning-value",
    "href": "ITM812.html#data-analytic-maching-learning-value",
    "title": "머신러닝과 사업기회 - ITM812",
    "section": "2.4 Data Analytic (Maching Learning) Value",
    "text": "2.4 Data Analytic (Maching Learning) Value\n\n정형/비정형 데이터\n정형 - 틀이 정해진. 나이, 키, 성별 등의 정보를 담는 데이터\n비정형 - 이미지, 언어, 소리, 컨설팅"
  },
  {
    "objectID": "ITM812.html#data---insight---value",
    "href": "ITM812.html#data---insight---value",
    "title": "머신러닝과 사업기회 - ITM812",
    "section": "2.5 Data -> Insight -> Value",
    "text": "2.5 Data -> Insight -> Value\n\nAnalysis : 데이터마이닝, 머신러닝, 딥러닝\nAction: 의사결정자, 엔지니어, 마케터, 투자자, 인사관리자 등\nGPU vs CPU. 다윗과 골리앗. 무거운 걸 들을때 다윗 10명 << 찐 골리앗 1명. 우표를 붙이는 작업을 할 때 다윗 10명 >> 골리앗 1명.\n\nGPU는 반복작업을 위해 만들어진 것\n\n딥러닝은 머신러닝의 세부 단위?. 머신러닝을 잘 알면 딥러닝이 언제 필요한 지 알 수 있을 것"
  },
  {
    "objectID": "ITM812.html#ai-machine-learning-deep-learning",
    "href": "ITM812.html#ai-machine-learning-deep-learning",
    "title": "머신러닝과 사업기회 - ITM812",
    "section": "2.6 AI > Machine Learning > Deep Learning",
    "text": "2.6 AI > Machine Learning > Deep Learning\n\nAI: 가장 큰 범주. 큰 범주 안에 머신러닝이 속하고, 머신러닝의 일부분이 딥러닝\n사람과 유사한 판단을 컴퓨터가 할 수 있게 끔 만드는 것이 인공지능\n기존의 데이터를 이용해 앞으로 일을 예측하는 머신러닝\n머신러닝 안에 여러 알고리즘 중 하나가 딥러닝\n인공지능이 먹을 수 있는 모든 음식이라면, 머신러닝은 그 중 영양가가 많은 고기 음식, 딥러닝은 그 중에서도 최고급 스테이크 요리"
  },
  {
    "objectID": "ITM812.html#machine-learning-vs.-programming",
    "href": "ITM812.html#machine-learning-vs.-programming",
    "title": "머신러닝과 사업기회 - ITM812",
    "section": "2.7 Machine Learning vs. Programming",
    "text": "2.7 Machine Learning vs. Programming\n\n전통적인 SW(프로그래밍)은 한번만 만들면 된다.\nML(Machine Learning)은 전통적인 SW 개발과 달리, 데이터 학습을 통해 더 좋은 규칙을 계속 만들어 냄.\n하드웨어 성능이 좋을 수록, 계속 반복할 수록, 데이터가 많아질 수록 성능도 함께 발전.\n값을 주면 결과를 제시하는 게 프로그래밍, 데이터와 답을 주면 규칙을 내놓는 게 머신러닝."
  },
  {
    "objectID": "ITM812.html#머신라닝의-세-가지-타입",
    "href": "ITM812.html#머신라닝의-세-가지-타입",
    "title": "머신러닝과 사업기회 - ITM812",
    "section": "2.8 머신라닝의 세 가지 타입",
    "text": "2.8 머신라닝의 세 가지 타입\n\nSupervised Learning 지도 학습\n\n제일 많이 쓰임. 지도하다. 가이드하다. 명확하게 답이 있는 것. 정답(맞았다, 틀렸다 가능// TRUE OR FALSE). 라벨링.\n딥러닝을 잘하라면 라벨링을 잘해야함. 딥러닝은 Classification 방법 중 하나. 방법론은 물론, 데이터 라벨링이 중요.\n\nUnsupervised Learning 비지도 학습\n\n지도하지 않는다. 라벨링하지 않는다. 정답이 없다. 정답이 없으니 비슷한 것들끼리 그룹을 지어준다. Grouping-> Clustering. Similarity. Distance 기반.\nClustering. 비슷한 데에 모여있는 것들끼리. K-means, K-NN(Nearest Neighbor, 최근접 이웃)\n\nReinforcement 강화 학습\n\n정답은 없는데, 부족한 부분을 계속 학습하는 과정. 딥러닝으로 발전하는 것과 비슷함.\n\n딥러닝은 Supervised와 Reinforcement가 합쳐진 것이다."
  },
  {
    "objectID": "jiseok.html",
    "href": "jiseok.html",
    "title": "Quarto_jiseok",
    "section": "",
    "text": "library(tidyverse)"
  },
  {
    "objectID": "KAIST_IAM_GROUP.html",
    "href": "KAIST_IAM_GROUP.html",
    "title": "KAIST IAM Group",
    "section": "",
    "text": "Natural Gas Vehicles(NGV)\nLight-duty vehicles\n\n지역별로 LDV의 사이즈를 4개씩 구분하였고, 각각의 배터리 용량(kwh poer vehicle) 제시\nBEV battery pack costs는 2020 BNEF EV Outlook에서 가져옴\ncost mark-up??\nFuel cell stack costs는 NREL, 2020 Transportation Annual Technology Baseline에서 가져옴\n\n\n\n\n\n\nregion : South Korea\n\nsupplysector :\n\n항공 :\n\naviation_intl\n\n화물 :\n\nfreight\nfreight_road\n\n여객 :\n\npass_road\npass_road_LDV\npass_road_LDV_4W\n\n해운 :\n\nshipping_intl\n\n\nenergy_final_demand\n\n항공:\n\naviation_intl\n\n화물:\n\nfreight\n\n여객:\n\npass\n\n해운:\n\nshipping_intl\n\n\n\n\n\n\ngcam-7의 transportation_UCD_CORE.xml과 비교해보니, supplysector, energy-final-demand의 구성 name은 동일함\nThe most common failure to run GCAM when double clicking the run-gcam executable script typically relate to Java.\nmodel running 테스트 -> All model periods solved correctly.\n\n\n\n\n스크린샷\n\n\n\nModel run 결과는 exe - log - main_log.txt 에서 확인 가능\noutput - queries\nModelInterface - run-model-interface.bat\n\n ### transportation"
  },
  {
    "objectID": "KAIST_IAM_GROUP.html#representation-of-h2-in-gcam",
    "href": "KAIST_IAM_GROUP.html#representation-of-h2-in-gcam",
    "title": "KAIST IAM Group",
    "section": "4.1 Representation of H2 in GCAM",
    "text": "4.1 Representation of H2 in GCAM\n\nTried to make a flow chart of hydrogen in GCAM using mermaid\n\n\n\n\n\n\nflowchart TD\n  Electricity --> H2centralproduction\n  Electricity --> H2wholesaledispensing\n  Electricity --> H2industrial\n  Wind --> H2centralproduction\n  Solar --> H2centralproduction\n  Coal --> H2centralproduction\n  Nuclear --> H2centralproduction\n  Gas --> H2centralproduction\n  Gas --> H2wholesaledispensing\n  H2centralproduction --> H2deliverytrack\n  H2centralproduction --> H2pipeline\n  H2deliverytrack --> H2wholesaledelivery\n  H2deliverytrack --> H2wholesaledispensing\n  H2pipeline --> H2wholesaledelivery \n  H2pipeline --> H2wholesaledispensing\n  H2wholesaledelivery --> H2rentaldelivery\n  H2wholesaledelivery --> H2industrial\n  H2rentaldelivery --> CommercialBuildings\n  H2rentaldelivery --> ResidentialBuildings\n  H2industrial --> AmmoniaProduction\n  H2industrial --> IronandSteelProduction\n  H2industrial --> IndustrialEnergyUse\n  H2wholesaledispensing --> AgriculturalMachinery\n  H2wholesaledispensing --> ConstruntionEquipment\n  H2wholesaledispensing --> MiningEquipment\n  H2wholesaledispensing --> H2rentaldispensing\n  H2rentaldispensing --> HeavyDutyTrucks\n  H2rentaldispensing --> LightDutyVehicles\n  H2rentaldispensing --> AirTransport\n  H2rentaldispensing --> RailTransport\n  H2rentaldispensing --> ShipTransport"
  },
  {
    "objectID": "KAIST_IAM_GROUP.html#to-do",
    "href": "KAIST_IAM_GROUP.html#to-do",
    "title": "KAIST IAM Group",
    "section": "5.1 To do",
    "text": "5.1 To do\n\nComparing dac_ssp2.xml and dac_ssp_x3.xml. Maybe _x3 means that the value has increased by 3 times??\ntransportation_UCD_CORE.xml is default in configuration file in GCAM7. transportation_UCD_CORE_CurPol_KOREA_Dawoon.xml, transportation_UCD_CORE_NetZero_KOREA_Dawoon.xml in GCAM-KAIST2.0. Run KOREA files in GCAM7."
  },
  {
    "objectID": "KAIST_IAM_GROUP.html#failure",
    "href": "KAIST_IAM_GROUP.html#failure",
    "title": "KAIST IAM Group",
    "section": "5.2 Failure",
    "text": "5.2 Failure\n\nRun a model with transportation_UCD_CORE.xml(default file in GCAM7.0) -> Of course, it worked!\nRun a model with transportation_UCD_CORE_CurPol_KOREA_Dawoon.xml from GCAM-KAIST2.0 -> ERROR! error messages are as follows.\n\n\n\n\n\n\n\nerorr message\n\n\n\n\nUnknown tag: loadFactor encountered while processing period\n\nMarket info object cannot be returned because market H2 enduse in XX does not exit\n\nCalled for price of non-existant market H2 enduse in region XX\n\n\n\n\n\nTried to find a way to see the differences in different XML files in R. But couldn’t. What is the best way to see the structure of XML? and easy way to do diff?\nTried to run library(rgcam) but… failed. need to explore more."
  },
  {
    "objectID": "KAIST_IAM_GROUP.html#success",
    "href": "KAIST_IAM_GROUP.html#success",
    "title": "KAIST IAM Group",
    "section": "5.3 Success",
    "text": "5.3 Success\n\nLet’s check the difference btw dac_ssp2.xml and dap_ssp2_x3.xml scenarios.\nBased on the scenarios described below, ran a model to see the different results caused by two dac scenarios. dac_ssp2 and dac_ssp2_x3\n\n\n\n\n\n\n\ncommon scenarios(.xml) applied for two dac_ssp2 & dap_ssp2_x3 comparison\n\n\n\n\n../input/Korea/1p5-incr-UC-kor-LUC-Kwangnam.xml \n../input/Korea/GHG_link_Kor.xml \n\n../input/Korea/FFI-const-row-deeper-decarb-2050-1p5-6000-Linear.xml \n\n../input/Korea/PCT_CO2_LUC_link_p10_row.xml\n\n\n\n\n\nDiscovery of library(gcamaextractor)\ngcamextractor is R package used to extract and process GCAM data and manipulate into standardized tables. gcamextractor converts GCAM outputs into commonly used units as well as aggregates across different classes and sectors for easy use in plots, maps and tables. See Details.\n\n\n\n\n\nline charts\n\n\n\nCode\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(gghighlight)\n\ndac_data<- read_excel('./KAIST_IAM_GROUP/results.xlsx')\n\ndata_tidy<- dac_data %>% \n    select(-scenario, -region) %>% \n    relocate(type, query) %>% \n    pivot_longer(-c('type', 'query', 'sector'), names_to =\"year\", values_to = \"MTC\") %>% \n    mutate( year = as.numeric(year))\n\ndata_tidy %>% \n    filter(query == \"sequestration\") %>% \n    ggplot(aes(x= year, y = MTC, group = sector, color = type))+\n    geom_line()+\n    facet_wrap(~sector)+\n    labs(title= \"Comparison between scenario dap_ssp2 and dap_ssp_x3\")\n\n\n\n\n\nCode\ndata_tidy %>% \n    filter(query == \"emission\") %>% \n    ggplot(aes(x= year, y = MTC, group = sector, color = type))+\n    geom_line()+\n    facet_wrap(~type)+\n    labs(title= \"Comparison between scenario dap_ssp2 and dap_ssp_x3\")\n\n\n\n\n\n\n5.3.1 Results\n\nI used library(gcamextractor) to ease data manipulation burden and make the differences in result more visible.\nResults from two scenarios showed no big differences (the results are almost identical).\nFigures below show the results, and it seems like there is only one scenario not two.\n\n\n\n\nMake a chart using library(rchart)"
  },
  {
    "objectID": "KAIST_IAM_GROUP.html#needs-to-be-done",
    "href": "KAIST_IAM_GROUP.html#needs-to-be-done",
    "title": "KAIST IAM Group",
    "section": "5.4 Needs to be done",
    "text": "5.4 Needs to be done\n\nHow to see the structure of XML files in R? Using XML, XML2 libraries or rgacm library works?\nWhat are the parameters we need to focus on? in dac scenario analysis I just selected the emissCO2BySector and emissCO2BySectorNoBio. What else should I consider? ->NoBio is in which we are intersted.\nHow to immigrate transportation scenario from GCAM-KAIST2.0 to GCAM7?"
  },
  {
    "objectID": "paper_critique_slide.html#writing-an-article-critique",
    "href": "paper_critique_slide.html#writing-an-article-critique",
    "title": "Paper critique",
    "section": "Writing an Article Critique",
    "text": "Writing an Article Critique\n\n\n\nIntroduction\n\n\nThe author’s name(s) and the title of the article\nThe author’s main point\nA thesis statement that previews your analysis\n\n\n\nSummary\n\n\nThe main points of the article\nThe arguments presented in the article\nThe findings of the article\n\n\n\nCritique\n\n\nDiscuss the strengths and weaknesses of the article that you noted while critically reading the article.\nState your informed opinions about the clarity, relevancy, and accuracy of the article, using specific examples from the article to support your statements."
  },
  {
    "objectID": "paper_critique_slide.html#writing-an-article-critique2",
    "href": "paper_critique_slide.html#writing-an-article-critique2",
    "title": "Paper critique",
    "section": "Writing an Article Critique2",
    "text": "Writing an Article Critique2\n\n\n\n\nIntroduction\n\n\nThe author’s name(s) and the title of the article\nThe author’s main point\nA thesis statement that previews your analysis\n\n\n\n\n\nSummary\n\n\nThe main points of the article\nThe arguments presented in the article\nThe findings of the article\n\n\n\n\n\nCritique\n\n\nDiscuss the strengths and weaknesses of the article that you noted while critically reading the article.\nState your informed opinions about the clarity, relevancy, and accuracy of the article, using specific examples from the article to support your statements."
  },
  {
    "objectID": "paper_critique_slide.html#summarize-the-key-findings-of-the-paper",
    "href": "paper_critique_slide.html#summarize-the-key-findings-of-the-paper",
    "title": "Paper critique",
    "section": "Summarize the key findings of the paper",
    "text": "Summarize the key findings of the paper\n\n\nA summary of a research article requires you to share the key points of the article so your reader can get a clear picture of what the article is about.\nA critique may include a brief summary, but the main focus should be on your evaluation and analysis of the research itself."
  },
  {
    "objectID": "paper_critique_slide.html#discuss-3-strengths-of-the-paper",
    "href": "paper_critique_slide.html#discuss-3-strengths-of-the-paper",
    "title": "Paper critique",
    "section": "Discuss 3 strengths of the paper",
    "text": "Discuss 3 strengths of the paper\n\n\nOver 20 syntax highlighting themes available\nDefault theme optimized for accessibility\n\n\n\nLearn more: Syntax Highlighting"
  },
  {
    "objectID": "paper_critique_slide.html#critique-3-weakness-of-the-paper",
    "href": "paper_critique_slide.html#critique-3-weakness-of-the-paper",
    "title": "Paper critique",
    "section": "Critique 3 weakness of the paper",
    "text": "Critique 3 weakness of the paper\n\n\nOver 20 syntax highlighting themes available\nDefault theme optimized for accessibility"
  },
  {
    "objectID": "paper_critique_slide.html#how-to-improve-for-my-own-research",
    "href": "paper_critique_slide.html#how-to-improve-for-my-own-research",
    "title": "Paper critique",
    "section": "How to improve for my own research",
    "text": "How to improve for my own research"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "research_slide.html#motivation-my-works",
    "href": "research_slide.html#motivation-my-works",
    "title": "Research Topic with GCAM",
    "section": "Motivation & My Works",
    "text": "Motivation & My Works\n\nElectricity infrastructure is prone to cost overrun issues almost independently of technology or location\n\nhydroelectric dams and nuclear reactors have the greatest amount and frequency of cost overruns, solar and wind projects seem to present the least construction risk(Sovacool, Nugent, and Gilbert 2014).\nProjections of nuclear plant costs have repeatedly failed to predict the cost overruns observed(Sovacool, Gilbert, and Nugent 2014).\nBottom-up cost modeling mechanisms to identify the rise in nuclear construction costs over the past five decades have also been studied(Eash-Gates et al. 2020)."
  },
  {
    "objectID": "research_slide.html#introduction",
    "href": "research_slide.html#introduction",
    "title": "Research Topic with GCAM",
    "section": "Introduction",
    "text": "Introduction\n\n\nResearch questions\n\nHow does coal decline affect local county and city govermnent financials?\nHow large are fiscal spillover effects of coal decline?"
  },
  {
    "objectID": "research_slide.html#introduction-1",
    "href": "research_slide.html#introduction-1",
    "title": "Research Topic with GCAM",
    "section": "Introduction",
    "text": "Introduction\n\n\nResearch questions\n\nHow does coal decline affect local county and city govermnent financials?\nHow large are fiscal spillover effects of coal decline?"
  },
  {
    "objectID": "research_slide.html#summary-of-findings",
    "href": "research_slide.html#summary-of-findings",
    "title": "Research Topic with GCAM",
    "section": "Summary of Findings",
    "text": "Summary of Findings\n\n\nA decline in coal production is associated with"
  },
  {
    "objectID": "research_slide.html#contribution",
    "href": "research_slide.html#contribution",
    "title": "Research Topic with GCAM",
    "section": "Contribution",
    "text": "Contribution\n\n\nFiscal Impacts of industry collapse\nLocal fossil fuel econommy\nPublic finance redistribution - Fiscal federalism"
  },
  {
    "objectID": "research_slide.html#background-coal-industry-and-local-govt-revenue",
    "href": "research_slide.html#background-coal-industry-and-local-govt-revenue",
    "title": "Energy Transition: Delivering Energy Infrastructure Projects On Time and On Budget",
    "section": "Background: Coal industry and Local Gov’t Revenue",
    "text": "Background: Coal industry and Local Gov’t Revenue\n\nIG(Intergovermental) Transfer Channel\nDisbursements to localities\n\nSpecific-puropse aid: reimpbursements of local public services (e.g., highways, housing development)\nGeneral-purpose aid: local share of state-collected taxes(e.g., state severance, income tax)\n\nSpecific-purpose work // General-purpose work"
  },
  {
    "objectID": "research_slide.html#data",
    "href": "research_slide.html#data",
    "title": "Energy Transition: Delivering Energy Infrastructure Projects On Time and On Budget",
    "section": "Data",
    "text": "Data\n\nAnnual survey of coal production and preperation\nU.S. Census Bureau\n\nCensus of Governments :::"
  },
  {
    "objectID": "research_slide.html#identification-strategy",
    "href": "research_slide.html#identification-strategy",
    "title": "Energy Transition: Delivering Energy Infrastructure Projects On Time and On Budget",
    "section": "Identification Strategy",
    "text": "Identification Strategy\n\nTwo-way fixede ffects model:\nInstrumental variables(IV) approach\nThe product of temporal and cross-sectoral variables affecting coal production\n\ntemporal variation: exogenous coal demand shocks induced by innovations in natural gas fracking\nCross-sectional variation: predetermined geological determinant of coal mining"
  },
  {
    "objectID": "research_slide.html#temporal-variation",
    "href": "research_slide.html#temporal-variation",
    "title": "Energy Transition: Delivering Energy Infrastructure Projects On Time and On Budget",
    "section": "Temporal Variation",
    "text": "Temporal Variation\n\nDifferent initiation timings of shale gas plays (Bartik et al., 2019)"
  },
  {
    "objectID": "research_slide.html#cross-sectional-variation",
    "href": "research_slide.html#cross-sectional-variation",
    "title": "Energy Transition: Delivering Energy Infrastructure Projects On Time and On Budget",
    "section": "Cross-Sectional Variation",
    "text": "Cross-Sectional Variation\n\nCoal reservoir quality as an exogenous predictor for coal extraction"
  },
  {
    "objectID": "research_slide.html#results-local-labor-market",
    "href": "research_slide.html#results-local-labor-market",
    "title": "Energy Transition: Delivering Energy Infrastructure Projects On Time and On Budget",
    "section": "Results :Local Labor Market",
    "text": "Results :Local Labor Market\n\nlocal labor market impacts - income shocks\nOut-migration to other counties in the same state increased"
  },
  {
    "objectID": "research_slide.html#results-local-govnt-finance",
    "href": "research_slide.html#results-local-govnt-finance",
    "title": "Energy Transition: Delivering Energy Infrastructure Projects On Time and On Budget",
    "section": "Results :Local Govnt Finance",
    "text": "Results :Local Govnt Finance\n\nTotal revenue and expenditure go down"
  },
  {
    "objectID": "research_slide.html#alternative-strategy",
    "href": "research_slide.html#alternative-strategy",
    "title": "Energy Transition: Delivering Energy Infrastructure Projects On Time and On Budget",
    "section": "Alternative Strategy",
    "text": "Alternative Strategy\n\nDID with nonparametric preprocessing data\nIdea: compare coal counties with non-coal ones\n\n\n\n\n\n\n\n\nEash-Gates, Philip, Magdalena M. Klemun, Goksin Kavlak, James McNerney, Jacopo Buongiorno, and Jessika E. Trancik. 2020. “Sources of Cost Overrun in Nuclear Power Plant Construction Call for a New Approach to Engineering Design.” Joule 4 (11): 2348–73. https://doi.org/10.1016/j.joule.2020.10.001.\n\n\nSovacool, Benjamin K., Alex Gilbert, and Daniel Nugent. 2014. “Risk, Innovation, Electricity Infrastructure and Construction Cost Overruns: Testing Six Hypotheses.” Energy 74 (September): 906–17. https://doi.org/10.1016/j.energy.2014.07.070.\n\n\nSovacool, Benjamin K., Daniel Nugent, and Alex Gilbert. 2014. “Construction Cost Overruns and Electricity Infrastructure: An Unavoidable Risk?” The Electricity Journal 27 (4): 112–20. https://doi.org/10.1016/j.tej.2014.03.015."
  },
  {
    "objectID": "sector_coupling.html",
    "href": "sector_coupling.html",
    "title": "GGS621",
    "section": "",
    "text": "References\n\nBrown, T., D. Schlachtberger, A. Kies, S. Schramm, and M. Greiner. 2018. “Synergies of Sector Coupling and Transmission Reinforcement in a Cost-Optimised, Highly Renewable European Energy System.” Energy 160 (October): 720–39. https://doi.org/10.1016/j.energy.2018.06.222.\n\n\nInce, Alper Can, C. Ozgur Colpan, Anke Hagen, and Mustafa Fazıl Serincan. 2021. “Modeling and Simulation of Power-to-x Systems: A Review.” Fuel 304 (November): 121354. https://doi.org/10.1016/j.fuel.2021.121354.\n\n\nVictoria, Marta, Kun Zhu, Tom Brown, Gorm B. Andresen, and Martin Greiner. 2019. “The Role of Storage Technologies Throughout the Decarbonisation of the Sector-Coupled European Energy System.” Energy Conversion and Management 201 (December): 111977. https://doi.org/10.1016/j.enconman.2019.111977.\n\n\nZerrahn, Alexander, Wolf-Peter Schill, and Claudia Kemfert. 2018. “On the Economics of Electrical Storage for Variable Renewable Energy Sources.” European Economic Review 108 (September): 259–79. https://doi.org/10.1016/j.euroecorev.2018.07.004.\n\n\n김도원정연제. n.d. “E-mobility 성장에 따른 석유·전력·신재생에너지 산업 대응 전략 연구(전력).”"
  },
  {
    "objectID": "GGS621.html#e",
    "href": "GGS621.html#e",
    "title": "GGS621",
    "section": "5.1 e",
    "text": "5.1 e"
  },
  {
    "objectID": "GGS621.html#carbon-brief-explainers-how-iams-are-used-to-study-climate-change",
    "href": "GGS621.html#carbon-brief-explainers-how-iams-are-used-to-study-climate-change",
    "title": "GGS621",
    "section": "1.1 Carbon Brief Explainers : How IAMs are used to study climate change",
    "text": "1.1 Carbon Brief Explainers : How IAMs are used to study climate change\n\nCarbon Brief Explainers, Q&A: How ‘integrated assessment models’ are used to study climate change, 2018.\nThere is a group of simple IAMs that can compare the costs and benefits of avoiding different levels of warming. Typically, these are run in a spreadsheet using highly simplified equations. They do not model the detailed processes and relationships of the economy, energy and Earth systems.These simple IAMs – such as “DICE”, “FUND” and “PAGE” – are often used to calculate the “social cost of carbon”, a measure of the quantifiable costs and benefits of emitting one additional tonne of CO2 in monetary terms.\nSecond, there is a group of more complex IAMs, which are the focus of this article. These look at the energy technologies, energy use choices, land-use changes and societal trends that cause – or prevent – greenhouse gas emissions. They do this using linked modules representing the global economy, as well as its energy, land and climate systems.\nIAMs are used to answer “what if” questions about the relationships between society and the natural world, including its changing climate.\nEdmonds, J. and J. Reilly. 1983. A Long-Term, Global, Energy-Economic Model of Carbon Dioxide Release From Fossil Fuel Use, Energy Economics, 5(2):74-88.\n\nAs simple a modelling system as possible has been used to develop a long-term global base case for CO2 emissions.\nDeveloped a concept called the ‘doubling window’. it is defined by looking at the date at which the CO2 concentration reaches 600 ppm under a given scenario assuming that f goes no higher than 0.7 nor lower than 0.4.\n\nVolker Krey. Global energy‐climate scenarios and models: a review. WIREs Energy Environ 2014, 3:363–383.\n\n\n\n\n\n\n\nAssignment 1\n\n\n\nSetup GCAM on a computer (laptop, cluster, or cloud), and run a GCAM reference scenario"
  },
  {
    "objectID": "sector_coupling.html#modeling-and-simulation-of-power-to-x-systems-a-reviewince2021",
    "href": "sector_coupling.html#modeling-and-simulation-of-power-to-x-systems-a-reviewince2021",
    "title": "GGS621",
    "section": "2.1 Modeling and simulation of Power-to-X systems: A review(Ince et al. 2021)",
    "text": "2.1 Modeling and simulation of Power-to-X systems: A review(Ince et al. 2021)\n\n2.1.1 Introduction\n\nToday, the share of electricity generation from renewable energy resources is approximately 25% in the world, and it is forecasted that it will have a share of two-thirds of the electricity generation by 2040\nAn important limitation of the use of RES is the fluctuating nature, which may lead to temporary surplus or lack of electricity and instability in the electric grid network. In this regard, energy storage technologies play an important role in providing a balance between supply and demand in the grid network at different time scales (seconds and subseconds, hourly, daily, and seasonally)\nThe capacity of energy storage (e.g, hydrogen, batteries, pumped hydro and compressed air) has increased.\nHydrogen (H2) is a promising energy carrier, which is versatile, transportable, clean (if produced from renewables), and has the highest gravimetric energy density among other alternatives, but its energy content per volume is comparatively low.\nHydrogen was produced from natural gas through steam reforming (48%), from the petroleum fraction through steam reforming and partial oxidation (30%), and from coal through gasification (18%), while only about 4% were produced from water through electrolysis in the early 2010 s.\n\n\n\n2.1.2 Conclusions\n\nThis study(reviews) aims to investigate, compare, and discuss the recent studies (between 2015 and 2020) on thermodynamic, techno-economic, and life cycle assessments of different P-t-X systems.\nThermodynamic analysis\n\nThe SOE technology in steam and co-electrolysis modes is the most preferable electrolyzer technology in the P-t-X systems, among the other electrolyzer types. SOE technology is preferred mainly due to its thermal integration compatibility with other subcomponents.\nThe energy requirement for the CO2 capture process. The membrane-integrated P-t-X system gives better system performance thermodynamically among the other CO2 capture techniques\nThe methane production system through the P-t-X concept is the most preferable technology due to fact the synthetic methane can be utilized in many industrial applications (heating, transport, and power production), and can be integrated into available gas transportation infrastructures.\n\nTechno-economic assessment\n\nWind and solar energy sources are the most preferable energy sources. It is also reported that methanol, DME, and gasoline are the favourable products in terms of economics, while the cost of renewable ammonia production is relatively higher.\n\nLife cycle assessment\n\n18 different indicators are commonly used in the literature, and the most preferred indexes have been GWI (Global Warming Impact) and GHG (Greenhouse Gas Emission).\nThe physical adsorption techniques in the CO2 capture process are superior to the chemical absorption techniques in terms of life cycle assessment indicators.\nSome P-t-Methane systems are more harmful in terms of some factors such as metal depletion, water depletion, and terrestrial, marine, and human toxicity compared to the conventional natural gas production system."
  },
  {
    "objectID": "sector_coupling.html#synergies-of-sector-coupling-and-transmission-reinforcement-in-a-cost-optimised-highly-renewable-european-energy-systembrown2018",
    "href": "sector_coupling.html#synergies-of-sector-coupling-and-transmission-reinforcement-in-a-cost-optimised-highly-renewable-european-energy-systembrown2018",
    "title": "GGS621",
    "section": "2.2 Synergies of sector coupling and transmission reinforcement in a cost-optimised, highly renewable European energy system(Brown et al. 2018)",
    "text": "2.2 Synergies of sector coupling and transmission reinforcement in a cost-optimised, highly renewable European energy system(Brown et al. 2018)\n\n2.2.1 Introduction\n\nMany studies have been conducted for European electricity sector. But, focusing on the electricity sector means not only neglecting the significant greenhouse gas emissions from other energy demand sectors, such as heating and transport, but also ignoring important sources of flexibility in these sectors.\nModelling all energy sectors in high spatial and temporal detail is computationally demanding. Studies of a few sectors have either considered just electricity - heat, electricity - transport, or electricity - gas.\nStudies that include multiple sectors, often encompassing all energy usage, but that sacrifice spatial resolution have typically either considered single countries or considered the whole continent of Europe without any spatial differentiation.\nIn this paper both sector coupling and international grid integration are considered in the model PyPSA-Eur-Sec-30, the first open, hourly, country-resolved, sector-coupled investment model of the European energy system.\n\n\n\n2.2.2 Conclusions\n\nThe coupling of the heating and transport sectors to electricity in a European context enables both the consideration of a higher share (75%) of the total final energy usage in the model.\nThe cost-optimal use of battery electric vehicles, synthetic electrofuels such as hydrogen and methane, heat pumps, district heating and long-term thermal energy storage removes the economic case for almost all stationary electricity storage and can reduce total system costs by up to 28%.\nPolicy Conclusions\n\nin cost-optimal energy systems with low emissions, wind and solar dominate primary energy generation, while heat pumps dominate heat provision.\nelectrification of transport is more cost-effective than using synthetic fuels in transport because of efficiency losses when producing the fuels.\nThe algorithms for managing battery electric vehicle charging should be exposed to dynamic electricity market prices.\nDistrict heating in high-density, urban areas with long-term thermal energy storage can significantly reduce costs\nFor heating systems with multiple technologies (heat pumps, resistive heating, solar thermal collectors and backup gas boilers for cold periods) can be efficient.\nConverting power to hydrogen and methane is advantageous in highly renewable systems, and the technologies for methanation and carbon dioxide capture should be developed further in view of this\nFinally, there are a variety of different possible paths to a highly renewable energy system, and no significant technical or economic barriers could be identified."
  },
  {
    "objectID": "GGS621.html#recommended-diverse-carbon-dioxide-removal-approaches-could-reduce-impacts-on-the-energy-water-land-system",
    "href": "GGS621.html#recommended-diverse-carbon-dioxide-removal-approaches-could-reduce-impacts-on-the-energy-water-land-system",
    "title": "GGS621",
    "section": "2.1 (Recommended) Diverse carbon dioxide removal approaches could reduce impacts on the energy-water-land system",
    "text": "2.1 (Recommended) Diverse carbon dioxide removal approaches could reduce impacts on the energy-water-land system\n\nArticle here (Fuhrman et al. 2023)\n\n\nSummarize the key findings\n\nUse an integrated assessment model to assess a complete suite of carbon dioxide removal(CDR) approaches including bioenergy with carbon capture and storage, afforestation, direct air capture with carbon storage, enhanced weathering, biochar and direct ocean capture with carbon storage."
  },
  {
    "objectID": "GGS621.html#section",
    "href": "GGS621.html#section",
    "title": "GGS621",
    "section": "2.2 ",
    "text": "2.2"
  },
  {
    "objectID": "GGS621.html#recommended-technology-interactions-among-low-carbon-energy-technologies-what-can-we-learn-from-a-large-number-of-scenarios",
    "href": "GGS621.html#recommended-technology-interactions-among-low-carbon-energy-technologies-what-can-we-learn-from-a-large-number-of-scenarios",
    "title": "GGS621",
    "section": "2.3 (Recommended) Technology interactions among low-carbon energy technologies: What can we learn from a large number of scenarios?",
    "text": "2.3 (Recommended) Technology interactions among low-carbon energy technologies: What can we learn from a large number of scenarios?\n\nArticle here(McJeon et al. 2011)"
  },
  {
    "objectID": "sector_coupling.html#modeling-of-power-to-x-systems",
    "href": "sector_coupling.html#modeling-of-power-to-x-systems",
    "title": "GGS621",
    "section": "2.1 Modeling of Power-to-X systems",
    "text": "2.1 Modeling of Power-to-X systems\n\nArticle : Modeling and simulation of Power-to-X systems: A review(Ince et al. 2021)\n\n\nIntroduction\n\nToday, the share of electricity generation from renewable energy resources is approximately 25% in the world, and it is forecasted that it will have a share of two-thirds of the electricity generation by 2040\nAn important limitation of the use of RES is the fluctuating nature, which may lead to temporary surplus or lack of electricity and instability in the electric grid network. In this regard, energy storage technologies play an important role in providing a balance between supply and demand in the grid network at different time scales (seconds and subseconds, hourly, daily, and seasonally)\nThe capacity of energy storage (e.g, hydrogen, batteries, pumped hydro and compressed air) has increased.\nHydrogen (H2) is a promising energy carrier, which is versatile, transportable, clean (if produced from renewables), and has the highest gravimetric energy density among other alternatives, but its energy content per volume is comparatively low.\nHydrogen was produced from natural gas through steam reforming (48%), from the petroleum fraction through steam reforming and partial oxidation (30%), and from coal through gasification (18%), while only about 4% were produced from water through electrolysis in the early 2010 s.\n\n\n\nConclusions\n\nThis study(reviews) aims to investigate, compare, and discuss the recent studies (between 2015 and 2020) on thermodynamic, techno-economic, and life cycle assessments of different P-t-X systems.\nThermodynamic analysis\n\nThe SOE technology in steam and co-electrolysis modes is the most preferable electrolyzer technology in the P-t-X systems, among the other electrolyzer types. SOE technology is preferred mainly due to its thermal integration compatibility with other subcomponents.\nThe energy requirement for the CO2 capture process. The membrane-integrated P-t-X system gives better system performance thermodynamically among the other CO2 capture techniques\nThe methane production system through the P-t-X concept is the most preferable technology due to fact the synthetic methane can be utilized in many industrial applications (heating, transport, and power production), and can be integrated into available gas transportation infrastructures.\n\nTechno-economic assessment\n\nWind and solar energy sources are the most preferable energy sources. It is also reported that methanol, DME, and gasoline are the favourable products in terms of economics, while the cost of renewable ammonia production is relatively higher.\n\nLife cycle assessment\n\n18 different indicators are commonly used in the literature, and the most preferred indexes have been GWI (Global Warming Impact) and GHG (Greenhouse Gas Emission).\nThe physical adsorption techniques in the CO2 capture process are superior to the chemical absorption techniques in terms of life cycle assessment indicators.\nSome P-t-Methane systems are more harmful in terms of some factors such as metal depletion, water depletion, and terrestrial, marine, and human toxicity compared to the conventional natural gas production system."
  },
  {
    "objectID": "sector_coupling.html#synergies-of-sector-coupling-and-transmission-reinforcement",
    "href": "sector_coupling.html#synergies-of-sector-coupling-and-transmission-reinforcement",
    "title": "GGS621",
    "section": "2.2 Synergies of sector coupling and transmission reinforcement",
    "text": "2.2 Synergies of sector coupling and transmission reinforcement\n\nArticle: Synergies of sector coupling and transmission reinforcement in a cost-optimised, highly renewable European energy system(Brown et al. 2018)\n\n\nIntroduction\n\nMany studies have been conducted for European electricity sector. But, focusing on the electricity sector means not only neglecting the significant greenhouse gas emissions from other energy demand sectors, such as heating and transport, but also ignoring important sources of flexibility in these sectors.\nModelling all energy sectors in high spatial and temporal detail is computationally demanding. Studies of a few sectors have either considered just electricity - heat, electricity - transport, or electricity - gas.\nStudies that include multiple sectors, often encompassing all energy usage, but that sacrifice spatial resolution have typically either considered single countries or considered the whole continent of Europe without any spatial differentiation.\nIn this paper both sector coupling and international grid integration are considered in the model PyPSA-Eur-Sec-30, the first open, hourly, country-resolved, sector-coupled investment model of the European energy system.\n\n\n\nConclusions\n\nThe coupling of the heating and transport sectors to electricity in a European context enables both the consideration of a higher share (75%) of the total final energy usage in the model.\nThe cost-optimal use of battery electric vehicles, synthetic electrofuels such as hydrogen and methane, heat pumps, district heating and long-term thermal energy storage removes the economic case for almost all stationary electricity storage and can reduce total system costs by up to 28%.\nPolicy Conclusions\n\nin cost-optimal energy systems with low emissions, wind and solar dominate primary energy generation, while heat pumps dominate heat provision.\nelectrification of transport is more cost-effective than using synthetic fuels in transport because of efficiency losses when producing the fuels.\nThe algorithms for managing battery electric vehicle charging should be exposed to dynamic electricity market prices.\nDistrict heating in high-density, urban areas with long-term thermal energy storage can significantly reduce costs\nFor heating systems with multiple technologies (heat pumps, resistive heating, solar thermal collectors and backup gas boilers for cold periods) can be efficient.\nConverting power to hydrogen and methane is advantageous in highly renewable systems, and the technologies for methanation and carbon dioxide capture should be developed further in view of this\nFinally, there are a variety of different possible paths to a highly renewable energy system, and no significant technical or economic barriers could be identified."
  },
  {
    "objectID": "KAIST_IAM_GROUP.html#dac_ssp2랑-dac_ssp_x3-modelinterface에서-확인하기",
    "href": "KAIST_IAM_GROUP.html#dac_ssp2랑-dac_ssp_x3-modelinterface에서-확인하기",
    "title": "KAIST IAM Group",
    "section": "7.1 dac_ssp2랑 dac_ssp_x3 ModelInterface에서 확인하기",
    "text": "7.1 dac_ssp2랑 dac_ssp_x3 ModelInterface에서 확인하기\n\n\n\n\n\n\nImportant\n\n\n\n\nCheck dac_ssp2 & dac_ssp2_x3 scenarios results of USA using Modelinterface in GCAM\nSelected parameter : CO2 emission by tech and CO2 sequestration by tech\nDrag and drop the resuls in excels\nSee the differences using R\nDifferences can be seen in airCO2 , CO2 removal, process heat dac, -> dac scenario works in USA\n/KAIST_IAM_GROUP dac_scenario_analysis.R 에서 확인 가능\n\n\n\n\nRegion : USA, Query: CO2 emissions by sector\n\n\n\nRegion : USA, Query: CO2 sequestration by sector"
  },
  {
    "objectID": "ITM501.html#why-do-firms-differ",
    "href": "ITM501.html#why-do-firms-differ",
    "title": "ITM501 - Innovation Management",
    "section": "2.1 Why Do Firms Differ",
    "text": "2.1 Why Do Firms Differ\n\n혁신 역량의 차이\n\n디지털 경제는 기존 경제와 완전히 다르다. 디지털 기술의 속성을 모르면 디지털 경제을 알 수 없다. 디지털은 한계체증을 한다(marginal utility, 한계체감이 제로로간다). -> 그래서 플랫폼 기업들이 생긴다. 네이버가 고객 한명을 더 받는데 비용이 얼마나 들까? 거의 0 일것이다.\n테슬라가 노리는 것? 전기차 -> OS게임. 안드로이드/IOS 처럼… 네비게이션을 사면 자동차를 주는 세상?\n삼천리자전거. 여태까지는 B2C에 최적화되었음. 하지만 디지털 경제의 공유사업을 통해 공유자전거가 생기고 나서 B2B가 되었음. Customer가 아닌 플랫폼 기업Business에 팔아야 하는 것임.\n디지털 위에서는 패러다임이 바뀐다. 그래서 기술경영이 중요. 기술(제조)과 경영.\n\n혁신도 하나의 이론. Innovation Theories.\n\n세계 탑 저널은 혁신이론을 기반으로 좋은 페이퍼가 나오고 있음.\n사회과학은 누적 지식을 바탕으로 원전(original text)을 깔 수있음. 아담스미스의 국부론은 지금 시대에 맞지 않아! 라는 얘기 가능.\n혁신의 아버지 조지프 슘페터. 슘페터로 부터 시작. 미국 대공황. 그 시대는 사회주의 - 자본주의가 격동할 때며, 누가 정답인지 모를 때. 소련의 사회주의, 미국의 자본주의 둘 다 잘 살았고 두 체제의 우월성이 확연하지 않았음. 미국의 fordism을 다른 국가들이 보고 자본주의 체제를 따라가게 됨.\n케인즈 - 수요, 슘페터 - 공급. 케인즈 수요정책, 통화정책. 슘페터는 기업의 혁신. 케인즈 소득주도, 슘페터 혁신 성장론.\n혁신성장은 구조적으로 기업의 체질을 바꿔야 하기 때문에 효과를 보기에 오래 걸린다. 정권 초기에는 혁신 얘기를 하고, 정권 말기에는 숫자로 나타나야 하니까 수요/통화정책을 다룬다. 이념 얘기는 아니다.\n국가의 통화량을 얘기할 때 MV=PY. 통화량 법칙. 통화량을 높이면 GDP가 성장하는 것처럼 보인다. 국가적으로 볼 때 fake 쓰기 좋다.\n\nM: 통화량, V: 화폐유통속도(거의 상수), P:가격(물가상승률, 이자율), Y:생산량(GDP)\n\n시장은 한번도 완전한 적이 없다. 개입에 관해 결정하는 것이 정책. 정책은 어떻게 균형상태를 이끌어 갈 것인가. 시장이 실패하기 쉬운 곳에 정책이 많이 들어간다. 20년 전만 해도 자유주의 팽배하였으나, 지금은 변화함.\nHistory of Innovation Studies 강의 슬라이드\n\n\nScience & Technology\nIn Social Science (Innovation study)\n\n혁신이론의 순서. 혁신연구는 과학에서 기술로 가고, 기술에서 사업화로 가야함. 과학을 통해 기술을 만들고, 기술을 통해 사업화를 이루어내야 함.\n과학의 주체는 어디? 대학. 과학의 측정? 논문으로. 과학을 기술로 바꾸는 역할? 기업 혹은 기업부설연구소. 기술의 측정? 특허. 출연연구소의 역할은 과학을-> 기술로.\nClarivate(Web of Science) KAIST 학생 사용 가능!! 제일 좋은 특허 사이트.\n\n다음주 숙제\n\n슘페터 가설이란?\n슘페터 가설과 관련된 논문 1편 요약 1p 이상"
  },
  {
    "objectID": "ITM501.html#classics-of-innovation-studies",
    "href": "ITM501.html#classics-of-innovation-studies",
    "title": "Innovation Management lecture",
    "section": "2.1 Classics of Innovation Studies",
    "text": "2.1 Classics of Innovation Studies\n\n슘페터 가설은 정답이 없다. 그래서 여전히 논문주제로 나온다. 왜 이런 물음들이 던져진걸까..?\n아담 스미스 전의 경제는 학문이 아니고, 신학과 철학, 정치 수준에 불과하였음. 수요 = 공급, 공식화 될 수 있는 걸 얘기한 사람이 아담스미스. 아담스미스가 경제학을만든 것임. 하나의 시장, 커뮤니티를 살펴보니 그 안에서 개인은 이기적인 마음으로 자유롭게 거래를 하는 것을 살펴봄. 모아놓고 보니 그 안에서 균형을 잡고, 전체의 후생이 증가한다는 걸 발견. 기본은 완전경쟁시장.\n칼 막스는 아담 스미스의 주장에 돌을 던짐.\n대공황(1929, Great Depression)이 오는 원인? 수요와 공급의 사이클이 느리거나 멈췄을 때. 어디를 자극해서? 어디를 관심? 케인즈는 수요쪽. 슘페터는 공급(기업).\n케인즈는 유효수요론 주장하면서 정부의 개입을 이야기함. - 소득주도. 왜냐하면 시장은 한번도 완전한 적이 없었기 때문. 케인즈는 기업 쪽에 관심이 없음. 수요쪽에만 관심을 가짐.\n슘페터는 기업에서 혁신을 통해 수요 공급 사이클을 돌릴 수 있다고 말함. - 혁신성장론.\n케인즈, 슘페터 모두 시장의 불완전성에는 동의"
  },
  {
    "objectID": "ITM501.html#schumpeterian-hypothesis",
    "href": "ITM501.html#schumpeterian-hypothesis",
    "title": "Innovation Management lecture",
    "section": "2.2 Schumpeterian hypothesis",
    "text": "2.2 Schumpeterian hypothesis\n\nPart II Can Capitalism Survive? Chapter 8 Monopolistic Practices\n\n\n\n\n\n\n\n중간고사 시험 대비\n\n\n\n\nThe Schumpeterian tradeoff revisited(Nelson and Winter 1982), 시험에 나옴!!! Table의 결과 정리해두면 좋을 것\n\n기본적으로 슘페터의 가설 및 이론을 지지함.\n많은 경쟁자들이 있는 시장보다는 적은 경쟁자들이 있는 시장에서 혁신적인 활동이 일어나기 쉽다는 것을 보여줌.\n투자가 제한된 상황에서는 경쟁 시장이 혁신을 촉진하는 것 보다는 독점적 시장이 혁신을 촉진하는 것이 더 효과적임.\nTable 1~6 주요 내용 정리 필요\n\n\n\n\n\n제품혁신, 공정혁신. cost reduction인지, 매출 상승인지에 따라 제품 or 공정 혁신이 달라질 수 있다.\nfirst-mover와 follower의 R&D 비용 차이는 어마어마함."
  },
  {
    "objectID": "ITM501.html#paradigm",
    "href": "ITM501.html#paradigm",
    "title": "Innovation Management lecture",
    "section": "2.3 Paradigm",
    "text": "2.3 Paradigm\n\nThomas Kuhn, 패러다임의 변화. 믿음, 세계관을 포함하는 넓은 의미의 형이상학적 변화 정상과학(교과서의 변경)\nThough the world does not change with a change of paradigm, the scientist afterwards works in a different world. “The Structure of Scientific Revolutions, p. 121.”\n\n\n\n\n\nflowchart LR \n  A(Anomalies) --> B(Crisis) \n  B(Crisis) --> C(Revolution)\n  C(Revolution) --> D(Normal Science)\n  D(Normal Science) --> A(Anomalies)"
  },
  {
    "objectID": "ITM501.html#section",
    "href": "ITM501.html#section",
    "title": "ITM501 - Innovation Management",
    "section": "2.4 ",
    "text": "2.4"
  },
  {
    "objectID": "ITM501.html#general-purpose-technology",
    "href": "ITM501.html#general-purpose-technology",
    "title": "Innovation Management lecture",
    "section": "2.4 General Purpose Technology",
    "text": "2.4 General Purpose Technology\n\n범용기술. 새로운 세대로의 기술적, 산업적 변화를 불러오는 혁신기술들의 최상단에 위치하는 기술로, 한 세대 산업혁명 안에서 많은 기술들의 뿌리로 인식될 수 있는 기술\n산업혁명은 일반적으로 범용기술의 출현과 함께 시작되어 당시 산업구조의 혁신적 변화를 일으킴\nEnabling Technology라고도 부르며, 산업간 확산성(Pervasiveness), 개선 가능성(Improvement), 혁신 촉진성(Innovation Spawning)"
  },
  {
    "objectID": "ITM501.html#assignment",
    "href": "ITM501.html#assignment",
    "title": "Innovation Management lecture",
    "section": "2.5 Assignment",
    "text": "2.5 Assignment\n\n혁신의 원천 관련 논문( 공급, 수요, 사용자, 기술 디지털 등) 1편 요약\n혁신의 trigger, driving force? 기술인가 수요인가. source of innovation\nSource of Innovation [Where Don innovations Come From?] 에 관련하여 과제를 제출하시면 됩니다.\n\n\nSources of innovation and innovation type: firm-level evidence from the United States\n\nSummary\n\nthis study provides one of the first tests to identify how important sources of new information (suppliers, customers, other business people in the industry, workers, and university) are associated with types of innovations (product, process, and marketing).\nThis article uses the first approach and treats innovation as the dependent variable while analyzing how different sources associated with overall firm innovation activity and four types of innovations.\nThe results show that innovation ideas emanating from customers, workers, and universities are positively associated with all types of innovations, suggesting that these sources are critical for developing different types of innovation.\n\n차이점\n\nour dependent variable is type of innovation and overall innovations, while most other studies focus on the amount of R&D expenditures, patent applications, location, and geographic proximity of knowledge sources\ndata for major studies focusing on sources of innovation are mostly from the 1990s and are not current.\nthese many studies cover only the largest R&D-performing firms, so they generally exclude smaller firms and non R&D firms, which is also true for the US Business R&D and the Innovation Survey of 2008. Our study, on the other hand, is not restricted to high-performing firms.\nmost of the studies on innovation focus on Europe, but the focus of this study is on the United States.\n\nData and Methods\n\nData come from the National Survey of Business Competitiveness (NSBC), which was conducted by the Social & Economic Sciences Research Center (SESRC) at Washington State University for the United States Department of Agriculture’s Economic Research Service.\nFour different dependent variables are used in this article. (i) product(including goods and service innovations), (ii) process, (iii) marketing innovations, (iv) firm innovation activity.\n\nResults\n\nthe results of the unstandardized SUR models. Ideas emanating from customers, workers, and universities are positively associated with all types of innovations.\nideas emanating from customers are positively associated with namely product (B = 0.06, P < 0.001), process (B = 0.05, P < 0.001), and marketing innovations (B = 0.05, P < 0.001), holding other variables constant.\nIdeas emanating from suppliers are moderately and positively associated with only product innovation (B = 0.02, P < 0.05). suppliers do not have any statistical relationship to other types of innovations.\nIdeas from other business people in the firm’s industry are positively associated with process and marketing innovations (P < 0.05), holding other variables constant, although the magnitude of the effect is small.\nIdeas emanating from workers are positively associated with all types of innovations (product, process, and marketing), holding other variables constant (B ¼ 0.04, 0.05, and 0.05, respectively; P < 0.001).\nThe universities as sources of innovation are also positively and statistically associated with both types of innovations (P < 0.001). In fact, the university as a source of innovation has higher coefficients (B ¼ 0.07, 0.08, and 0.1, respectively), suggesting that universities are important sources of innovation.\nRegarding the control variables, on average, the younger firms, firms with more employees, and firms located in urban areas exhibit a greater degree of innovation activity for all types of innovations—product, process, and marketing innovations.\n\nConclusion\n\nAnalyzing how five important sources of innovation (i.e. suppliers, customers, others in industry, workers, and university) are associated with types of innovation (i.e. product, process, and marketing), we have found that universities, workers, and customers are important sources of innovation positively associated with all of innovation types.\nUsing United States National Survey of Business Competitiveness (NSBC) conducted in 2014, this study has contributed to the innovation literature by analyzing how the following knowledge sources are associated with product, process, and marketing innovation as well as overall innovation activity by the following important innovation sources: (i) suppliers, (ii) customers, (iii) other people in industry, (iv) workers, and (v) university.\nUniversities, customers, and workers are crucial knowledge sources, positively associated with product, process, marketing, and overall innovation activity. Suppliers and others in industry have either little or no statistical effect for innovation activities.\nLike most of survey research, this article is based on a cross-sectional data. Therefore, lack of panel data, or quasi-experimental design does not permit scholars to make causal claims about findings. In other words, the study is correlational, not causal. In addition, due to data limitations, we could not measure which particular innovations are more important and effective than others. For example, data does not provide information about the quality of innovations such as which innovations lead huge savings, sales, or patents.\nWe recommend future studies to focus on the link of sources and types of innovation in other context with qualitative research."
  },
  {
    "objectID": "KAIST_IAM_GROUP.html#automation-of-graph-reporting-with-librarygcamextractor",
    "href": "KAIST_IAM_GROUP.html#automation-of-graph-reporting-with-librarygcamextractor",
    "title": "KAIST IAM Group",
    "section": "7.2 Automation of graph reporting with library(gcamextractor)",
    "text": "7.2 Automation of graph reporting with library(gcamextractor)\n\nUsing library(gcamextractor), query data can be imported as dataset in R and saved in excel files. -> Details can be found in Template.R"
  },
  {
    "objectID": "KAIST_IAM_GROUP.html#automation-of-graph-reporting-with-r-librarygcamextractor",
    "href": "KAIST_IAM_GROUP.html#automation-of-graph-reporting-with-r-librarygcamextractor",
    "title": "KAIST IAM Group",
    "section": "7.3 Automation of graph reporting with R library(gcamextractor)",
    "text": "7.3 Automation of graph reporting with R library(gcamextractor)\n\nUsing library(gcamextractor), query data can be imported as dataset in R and saved in excel files. -> Details can be found in Template.R\nUser guide for gcamextractor here."
  },
  {
    "objectID": "KAIST_IAM_GROUP.html#회의",
    "href": "KAIST_IAM_GROUP.html#회의",
    "title": "KAIST IAM Group",
    "section": "7.4 2023-09-12 회의",
    "text": "7.4 2023-09-12 회의\n\n\n\n\n\n\nWhat to do\n\n\n\n\n에너지통계연보 수치를, R 템플릿 결과에 추가해서 볼 수 있게\ndac 시나리오 나라 바꿔서 돌려보기 e.g., Korea <-> USA"
  },
  {
    "objectID": "ITM812.html#data-analytic-machine-learning-value",
    "href": "ITM812.html#data-analytic-machine-learning-value",
    "title": "머신러닝과 사업기회 - ITM812",
    "section": "2.4 Data Analytic (Machine Learning) Value",
    "text": "2.4 Data Analytic (Machine Learning) Value\n\n정형/비정형 데이터\n정형 - 틀이 정해진. 나이, 키, 성별 등의 정보를 담는 데이터\n비정형 - 이미지, 언어, 소리, 컨설팅\n인간과 유사한 판단을 할 수 있도록 컴퓨터에게 학습을 시킨다. 적당한 반복을 할 수 있을 때, 이를 대체할 수 도록"
  },
  {
    "objectID": "ITM812.html#통계-vs.-machine-learning",
    "href": "ITM812.html#통계-vs.-machine-learning",
    "title": "머신러닝과 사업기회 - ITM812",
    "section": "3.1 통계 vs. Machine Learning",
    "text": "3.1 통계 vs. Machine Learning\n\n통계는 과거를 통해 대표값을 얻는 것. e.g., 평균. 머신라닝은 과거를 통해 미래를 예측하고 인사이트를 찾는 것.\n표본: 다수에서 소수를 뽑고 대표값을 구하는 것"
  },
  {
    "objectID": "ITM812.html#deep-learning",
    "href": "ITM812.html#deep-learning",
    "title": "머신러닝과 사업기회 - ITM812",
    "section": "3.2 Deep Learning",
    "text": "3.2 Deep Learning\n\n다른 머신러닝 기법들과 차이점. Nonlinear function을 풀기 휘애 NONLINEAR FUNCTION을 linear function의 결합으로. -> 엄청 복잡한 함수(인공지능)를 만들 수 있다.\n다층 레이어(Multiple layer)\nHidden layer가 2개 이상인 NN(Neural Network)을 Deep Learning이라고 부른다."
  },
  {
    "objectID": "ITM812.html#linear-function-vs.-nonlinear-function",
    "href": "ITM812.html#linear-function-vs.-nonlinear-function",
    "title": "머신러닝과 사업기회 - ITM812",
    "section": "3.3 Linear function vs. Nonlinear function",
    "text": "3.3 Linear function vs. Nonlinear function\n\n선형회귀 모델은 ‘회귀계수(regression coefficient)를 선형 결합으로 표현할 수 있는 모델’\n계수들과 변수의 곱셈과 그들의 덧셈과 뺄셈으ㅗㄹ만 결합되어 있는 것을 의미한다.\n독립변수가 일차식인지, 이차식인지, 로그함수인지가 중요한 것이 아니라 추정할 대사인 파라미터가 어떻게 생겼느냐의 문제.\ny = a0 + a1x1, y = a0+a1x1+a2x2 등은 선형회귀식.\n비선형회귀 모델은 데이터를 어떻게 변형하더라도 파라미터를 선형결합식으로 표현할 수 없는 모델.\n선형회귀모델은 파라미터 계수에 대한 해석이 단순하지만, 비선형 모델은 형태가 복잡할 경우 해석하기 어렵기 때문에 통계 모델에서는 비선형회귀 모델을 잘 사용하지 않는다."
  },
  {
    "objectID": "ITM812.html#machine-learning은-문제-해결을-위한-함수-f를-찾는-것이다.",
    "href": "ITM812.html#machine-learning은-문제-해결을-위한-함수-f를-찾는-것이다.",
    "title": "머신러닝과 사업기회 - ITM812",
    "section": "3.4 Machine Learning은 문제 해결을 위한 함수 f()를 찾는 것이다.",
    "text": "3.4 Machine Learning은 문제 해결을 위한 함수 f()를 찾는 것이다.\n\n종속변수(y)를 독립변수(x)들의 함수 f(x)로 적합. Y =f(x)\n\n독립변수!!! x1, x2, x3, x4, … 독립볍수가 아니라면 전처리를 해야 함\n\nf(함수)가 무엇일까? 문제를 풀 방법? 분석방법을 의미한다.\n예제 1) Sales prediction : 특정 고객 -> 마케핑 캠패인에 반응할 확률\n\nx : 고객 과거 data, 캠페인 요소들\ny : 반응할 확률 -> 캠패인을 확인한 사람에게 세일즈를 하는 것이 좋다.\n\n예제 2) 휴대폰 고객이 향후 6개월 이내에 이탈할 확률\n\nx : 휴대폰 고객, y : 이탈할 확률\n\n예제 3) y: 다음 주 주가상승 여부 =f(x :최근 주가 추이, 환경분석)"
  },
  {
    "objectID": "ITM812.html#머신러닝-모델링-프로세스",
    "href": "ITM812.html#머신러닝-모델링-프로세스",
    "title": "머신러닝과 사업기회 - ITM812",
    "section": "3.5 머신러닝 모델링 프로세스",
    "text": "3.5 머신러닝 모델링 프로세스\n\n문제 정의 및 명확한 목표 설정\n데이터 수집 (including Random Sampling)\n데이터 탐색, Cleaning, Pre-processing\n데이터 분류 및 데이터 세닝\n\nTraining set, Validation set, Testing set 으로 구분\n\n데이터 방법론 선택\n\nRegression, Classification, Clustering, Recommender 등 선택\nregression은 원인과 결과를 찾는 데 쓸 수 있는데, 상관관계는 correlation. 다르다.\n\n구체적인 기술 및 평가 방법 선택\n\nLinear Regression, Logistic Regression, Decision tree, Random Forest, KNN, K-means, Matrix Factorization\n\n테스트 및 튜닝\n결과 및 모델 비교\n모델 선택 및 적용"
  },
  {
    "objectID": "GGS621.html#lecture",
    "href": "GGS621.html#lecture",
    "title": "GGS621",
    "section": "3.1 Lecture",
    "text": "3.1 Lecture"
  },
  {
    "objectID": "ITM501.html#source-of-innovation",
    "href": "ITM501.html#source-of-innovation",
    "title": "Innovation Management lecture",
    "section": "3.1 Source of Innovation",
    "text": "3.1 Source of Innovation\n\nTechnology Push - 시험 준비!!!\n\nTechnology Push Model\nBasic Science -> Applied Science and Engineering -> Manufacturing -> Marketing\n\n\n\nDemand Pull - 시험 준비!!!\n\n\nSeven sources of innovation\n\nUnexpected, Incongruity, Market Structure, Necessity, Demographics, Changing Perception, New Knowledge\nDescriptive, 통계 등을 활용해 증명을 해야 한다.\nPrescriptive 처방적. 피터 드러커의 연구는 증명할 거리가 아니다. 처방적인 것은 측정하기가 너무 어렵다. 기껏해야 설문 조사. 처방적 얘기를 가지고 연구주제로 가기는 쉽지 않다. 마케팅이나 MBA에서 가능\nGibbons Mode 1 to Mode 2 (중요!!!!!!!!!!!)\n\n\n\n과학-기술 연계성 분석\n\n특허의 NPL(Non-Patent Literature) 특허에 있는 참고문헌 중 비특허문헌\n\n\n\nAssignment\n\n\n\n\n\n\nImportant\n\n\n\nTypes, Architecture and Taxonomy 관련 논문\n(아티텍쳐 혁신, 모듈러 혁신, Pavitt 분류, 혁신의 분류)\n\n\n\n\nPatterns of technological innovation and evolution in the energy sector: A patent-based approach\n\n요약\n\nGiven the ever-increasing pace and complexity of technological innovation in the energy sector, monitoring technological changes has become of strategic importance.\nPatent analysis is one of the most prevalent methods used to analyze technological innovation. This study aims to explore patterns of innovation and of evolution in energy technologies, particularly focusing on similarities and differences across technologies.\nwe first defined the relevant energy technologies and extracted the associated patent data from the United States Patents and Trademark Office (USPTO) and then adopted six patent indices and developed six patent maps to analyze their innovation characteristics\n\n인트로\n\nPatents are a good proxy measure for innovation activities—they have been used to assess the current state of firms’ innovation efforts and to shape their future direction and support their R&D decision-making.\nThe importance of patent data is being increasingly recognized, and is growing daily across all industry sectors. Given the globally competitive and cooperative landscape of energy technology development, patents are considered as the core means of protecting innovation in the energy sector, as in other sectors.\nThis paper, therefore, aims to investigate the energy sector’s patterns of technological innovation and evolution, and to categorize the essential innovation aspects of energy technologies in more detail according to their similarities and differences..\n\nPatterns of innovation\n\nPrevious research has broadly classified types of innovation into ‘innovation subject’, ‘innovation degree’, and ‘innovation activity’.\nAs well as these classifications using single criteria, there have been various studies that try to classify technological innovation according to two criteria.\nAs well as these classifications using single criteria, there have been various studies that try to classify technological innovation according to two criteria.\n\nMethodology\n\nwe collected energy-technology related patents filed at the USPTO during the period 1991–2010.\nwe designed a composite index to analyze innovations’ characteristics, composed of six indices and classified according to input, process and output points of view.\nthe six individual indices we developed measured: (1) developer intensity (actor-input); (2) developer openness (actor-process); (3) market potential (actor-output); (4) technology intensity (technology-input); (5) technology openness (technology-process); and (6) technology potential (technology-output).\nUsing these indices, we analyzed the innovation characteristics of 21 new energy technologies to identify patterns of technology innovation and evolution.\n\nPatterns of innovation in the energy sector\n\nStatic portfolio analyis result\n\nDiscussions and policy implications\n\ndifferent energy technologies have different patterns of innovation. Six types of innovation patterns were identified—but those types were not determined by the sources of energies.\nthe innovation patterns of energy technologies change as they evolve.\nenergy technologies have a tendency to decrease in developer openness, against the trend towards open innovation.\nboth the application and technology potential of energy technologies tended to increase, indicating that their importance in the marketplace has increased and their development has accelerated.\n\nConclusions\n\nFirst, in methodological terms, it is one of the first attempts to use patent analysis to categorize the general characteristics of energy technologies.\non a practical level, our results provide an important overview of the current situation, which shows that, since innovation and evolution patterns differ according to energy technology.\neach new and renewable technology also have representative characteristics, but the general trends show that both developer intensity and technology intensity are generally low.\n\nLimitations\n\nMore advanced patent analysis methods such as patent citation and text-mining could also be applied.\nwe adopted a technology classification system from a previous study and identified 20 technologies from it.\nlimited to the analysis of global patterns of innovation, but the analysis of national or regional innovation patterns could reveal implications that might be more meaningful at those levels.\nit should be noted that knowledge about patterns of innovation and evolution based on patent analysis may be limited, as patent documents provide only little information about markets."
  },
  {
    "objectID": "GGS621.html#assignment---due-to-920",
    "href": "GGS621.html#assignment---due-to-920",
    "title": "GGS621",
    "section": "3.1 Assignment - due to 9/20",
    "text": "3.1 Assignment - due to 9/20\n\nRun a GCAM low-emission scenario\n\n\nQuery a sector of your choice\n\n\nPlot the outcome\n\n\nWhere do CO2 emission reductions come from?\n\n\nHow is energy supply decarbonized?\n\n\nHow are the end-use sectors decarbonized?\n\n\nHow does total energy consumption in each sector change?\n\n\nHow does fuel structure in each sector change?"
  },
  {
    "objectID": "KAIST_IAM_GROUP.html#section-3",
    "href": "KAIST_IAM_GROUP.html#section-3",
    "title": "KAIST IAM Group",
    "section": "7.2 ",
    "text": "7.2"
  },
  {
    "objectID": "KAIST_IAM_GROUP.html#section-4",
    "href": "KAIST_IAM_GROUP.html#section-4",
    "title": "KAIST IAM Group",
    "section": "7.5 ",
    "text": "7.5"
  },
  {
    "objectID": "KAIST_IAM_GROUP.html#after-ggs621-class-meeting-with-proj-mcjeon",
    "href": "KAIST_IAM_GROUP.html#after-ggs621-class-meeting-with-proj-mcjeon",
    "title": "KAIST IAM Group",
    "section": "7.6 2023-09-13 After GGS621 class, meeting with proj McJeon",
    "text": "7.6 2023-09-13 After GGS621 class, meeting with proj McJeon\n\n\n\n\n\n\nImportant\n\n\n\n\n(DAC) DAC 기술인 high DAC? 이런 애들은 다른 나라에서는 나타남, Japan, Thailand에서도. 한국에서만 안나타나는거 같음. 결과가 잘 나타나는 국가를 하나 택해서 이름만 바꿔보기. 예를 들어. Japan <-> Korea\nJapan을 대상으로 테스트.\n\n\n\n\nModified NZ scenario to verify in case of Japan\n\n1p5-incr-UC-kor-LUC-Japan.xml\n\nregion name = replaced South Korea with Japan\nmarket = Japan\nconstraint year 2025 = 1100, 2030 = 900, 2035 = 700, 2040 = 500, 2045 = 300, 2050 = 56, 2055= 5, 2066 = 5 ..\n\nGHG-link-Japan.xml\n\nregion name = replaced all South Korea with Japan\n\nFFI-const-row-deeper-decarb-2050-1p5-6000-Linear-Japan.xml\n\nReplaced Japan with South Korea\n\nPCT_CO2_LUC_Link_p10_row-Japan.xml\n\nReplaced Japan with South Korea\n\ndac_ssp2.xml\n\n\n\n7.6.1 \n\n\n7.6.2 2023-09-19\n\n\n\n\n\n\nImportant\n\n\n\n\nThe reason why there is no dac in South Korea is due to high price!!\ngeneral -> market and prices -> cost by tech\n\nsector : carbon storage, subsector: onshore carbon storage, technology: onshore carbon storage => cost is 8,938 90us$/tC\nfrom 2020 to 2050, 8938.28 19663.0 30745.3 41899.0 53067.0 64237.9 75409.4\n\nit’s not KAIST member who set this high values, but JGCRI member.(pf Haewon told)\nand what’s next??"
  },
  {
    "objectID": "ITM812.html#regression회귀",
    "href": "ITM812.html#regression회귀",
    "title": "머신러닝과 사업기회 - ITM812",
    "section": "4.1 Regression(회귀)",
    "text": "4.1 Regression(회귀)\n\nRegression -> 원인과 결과를 분석하는데 쓰임. y= ax1+bx2+cx3+d 와 같은 회귀식을 도출한다.\n똘똘한 기울기와 절편을 구하는 것. 기울기 = 가중치, 절편 = 편향.\n\n\n선형회귀\n\n가장 훌륭한 선 긋기 -> 머신러닝은 미래의 방향을 설정하는 것에서 부터 시작. y = ax+b로 표현될 수 있으며, x값이 변함에 따라 y 값도 변한다. Simple linear regression. 예, 독립변수 x가 공부한 시간, 성적 y를 예측할 경우, x가 한 개 이므로, simple linear regression.\n가장 정확한 기울기 a와 절편 b를 찾으면 된다.\n여러가지 선을 그을 수 있고, 여러가지 선 중, 반복되는 선 긋기를 통해 가장 훌륭한 선을 찾는다.\n선형회귀는 임의의 직선을 그어 이에 대한 평균제곱오차를 구하고, 이 값을 가장 작게 만들어주는 a와 b를 찾아가는 작업.\n어떻게 훌륭한 선을 찾을까? 오차(예측값 - 실제값) 줄이기\n예측모델 성능 평가\n\n가장 많이 쓰는 방법: 평균 제곱 오차(MSE: Mean Square Error)\n평균 제곱근 오차(RMSE: Root Mean Square Error) : MSE 값은 오류의 제곱을 구하므로, 실제 오류의 푱균보다 값이 더 커질 수 있어, MSE 에 루트를 씌운 경우\n평균절대오차(MAE: Mean Absolute Error)\n\n모든 것을 정확히 고려하면 overfitting이 될 수 있다.\n선형은 직선을 의미하는 것이 아니라 계수들의 곱과 합을로 이루어진 것을 말함.\n예재로 배우기 : 집값 예측\n\n최근 주변 부동산 시세를 살펴본다. 보통 얼마에 거래되었을까?\n방법: 여러가지 특징 세트가 있을 때 특정 변화에 따라 output(y)의 변화를 살펴본다.\n\n(로지스틱 회귀) 전달받은 정보를 놓고 참과 거짓 중 하나를 Output으로 선택하는 방법론. 참/거짓 판단장치라고 하며, 이진 분류에 많이 사용함. 예제) 합격자 발표에서 점수화 상관없이 ‘합격’ 불합격’만 존재합니다."
  },
  {
    "objectID": "ITM812/Fall2023_Regression_BostonHousingPrediction_Lecture_upload/Fall2023_Regression_BostonHousingPrediction_Lecture.html",
    "href": "ITM812/Fall2023_Regression_BostonHousingPrediction_Lecture_upload/Fall2023_Regression_BostonHousingPrediction_Lecture.html",
    "title": "Jiseok AHN",
    "section": "",
    "text": "Regression 실습 : 집값 예측하기\n\n미국 매사추세츠주의 주택 가격 데이터(Boston Housing 1970)를 활용해 지역의 평균 주택 가격을 예측하는 선형 회귀 모델을 학습한다.\n이를 기초로 하여 주택 가격의 영향 요소 파악 및 주택 가격 예측을 진행할 수 있다.\n\nLibrary & Data Import\n데이터 파악 (EDA: 탐색적 데이터 분석) 2-1. 데이터셋 기본 정보 파악 2-2. 종속 변수(목표 변수) 탐색 2-3. 설명 변수 탐색 2-4. 설명변수와 종속변수 간의 관계 탐색\n주택 가격 예측 모델링: 회귀 분석 3-1. 데이터 전처리 3-2. 회귀 모델링 3-3. 모델 해석 3-4. 모델 예측 결과 및 성능 평가\n\n\n\n\n1. Import Libraries & Data Set\n\nimport pandas as pd ### 데이터 분석을 하기 위한 파이썬 라이브러리 such as a table\nimport numpy as np ### 수치해석 라이브러리\nimport matplotlib.pyplot as plt ### 그래프 그리는 라이브러리\nimport seaborn as sns ### 그래프 그리는 라이브러리\n%matplotlib inline \n### 출력을 jupyter lab으로\n\n#import warnings\n#warnings.filterwarnings(\"ignore\", category=FutureWarning)\n#warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n\n\nImport Boston Housing Dataset\n\n분석에 사용될 데이터셋은 Boston Housing 데이터의 일부 변수를 추출한 데이터입니다.\n미국 매사추세츠주 92개 도시(TOWN)의 506개 지역의 주택 가격 및 기타 지역 특성 데이터가 포함되어 있습니다. (Dataset Introduction)\n\n\ndf=pd.read_csv(\"./BostonHouse/BostonHousing2.csv\")\n\n\n\n\n2. EDA (Exploratory Data Analysis)\n\n#데이터셋을 불러와서 첫 세 줄을 출력하여 데이터의 구성을 한 번 살펴볼게요.\ndf.tail(10)\n#df.tail(3)\n\n\n\n\n\n  \n    \n      \n      TOWN\n      LON\n      LAT\n      CMEDV\n      CRIM\n      ZN\n      INDUS\n      CHAS\n      NOX\n      RM\n      AGE\n      DIS\n      RAD\n      TAX\n      PTRATIO\n      B\n      LSTAT\n    \n  \n  \n    \n      496\n      Revere\n      -71.0010\n      42.2525\n      19.7\n      0.28960\n      0.0\n      9.69\n      0\n      0.585\n      5.390\n      72.9\n      2.7986\n      6\n      391\n      19.2\n      396.90\n      21.14\n    \n    \n      497\n      Revere\n      -70.9947\n      42.2496\n      18.3\n      0.26838\n      0.0\n      9.69\n      0\n      0.585\n      5.794\n      70.6\n      2.8927\n      6\n      391\n      19.2\n      396.90\n      14.10\n    \n    \n      498\n      Revere\n      -71.0050\n      42.2455\n      21.2\n      0.23912\n      0.0\n      9.69\n      0\n      0.585\n      6.019\n      65.3\n      2.4091\n      6\n      391\n      19.2\n      396.90\n      12.92\n    \n    \n      499\n      Revere\n      -70.9985\n      42.2430\n      17.5\n      0.17783\n      0.0\n      9.69\n      0\n      0.585\n      5.569\n      73.5\n      2.3999\n      6\n      391\n      19.2\n      395.77\n      15.10\n    \n    \n      500\n      Revere\n      -70.9920\n      42.2380\n      16.8\n      0.22438\n      0.0\n      9.69\n      0\n      0.585\n      6.027\n      79.7\n      2.4982\n      6\n      391\n      19.2\n      396.90\n      14.33\n    \n    \n      501\n      Winthrop\n      -70.9860\n      42.2312\n      22.4\n      0.06263\n      0.0\n      11.93\n      0\n      0.573\n      6.593\n      69.1\n      2.4786\n      1\n      273\n      21.0\n      391.99\n      9.67\n    \n    \n      502\n      Winthrop\n      -70.9910\n      42.2275\n      20.6\n      0.04527\n      0.0\n      11.93\n      0\n      0.573\n      6.120\n      76.7\n      2.2875\n      1\n      273\n      21.0\n      396.90\n      9.08\n    \n    \n      503\n      Winthrop\n      -70.9948\n      42.2260\n      23.9\n      0.06076\n      0.0\n      11.93\n      0\n      0.573\n      6.976\n      91.0\n      2.1675\n      1\n      273\n      21.0\n      396.90\n      5.64\n    \n    \n      504\n      Winthrop\n      -70.9875\n      42.2240\n      22.0\n      0.10959\n      0.0\n      11.93\n      0\n      0.573\n      6.794\n      89.3\n      2.3889\n      1\n      273\n      21.0\n      393.45\n      6.48\n    \n    \n      505\n      Winthrop\n      -70.9825\n      42.2210\n      19.0\n      0.04741\n      0.0\n      11.93\n      0\n      0.573\n      6.030\n      80.8\n      2.5050\n      1\n      273\n      21.0\n      396.90\n      7.88\n    \n  \n\n\n\n\n\n2-1 : 데이터 셋 기본 정보 파악\n\nFeatures Descriptions\n\nTOWN: 소속 도시 이름\nLON, LAT: 해당 지역의 경도(Longitudes) 위도(Latitudes) 정보\nCMEDV: 해당 지역의 주택 가격 (중앙값) (corrected median values of housing)(단위 USD 1000)\nCRIM: 지역 범죄율 (per capita crime)\nZN: 소속 도시에 25,000 제곱 피트(sq.ft) 이상의 주택지 비율\nINDUS: 소속 도시에 상업적 비즈니스에 활용되지 않는 농지 면적\nCHAS: 해당 지역이 Charles 강과 접하고 있는지 여부 (dummy variable)\nNOX: 소속 도시의 산화질소 농도\nRM: 해당 지역의 자택당 평균 방 갯수\nAGE: 해당 지역에 1940년 이전에 건설된 주택의 비율\nDIS: 5개의 보스턴 고용 센터와의 거리에 따른 가중치 부여\nRAD: 소속 도시가 Radial 고속도로와의 접근성 지수\nTAX: 소속 도시의 10000달러당 재산세\nPTRATIO: 소속 도시의 학생-교사 비율\nB: 해당 지역의 흑인 지수 (1000(Bk - 0.63)^2), Bk는 흑인의 비\nLSTAT: 해당 지역의 빈곤층 비율\n\n데이터의 각 변수(features, attributes, columns, x들)의 기본 정보 및 각 변수의 특성을 살펴보겠습니다.\n\n\n### 그래프의 배경 설정 : sns --> seaborn 라이브러리 이용\nsns.set_style('darkgrid')\n\n\n데이터 셋의 기본 정보 파악하기\n\n\n### 데이터 셋 구조 보기\ndf.shape \n### 해석: 행이 506, 집이 506개, 열이 17, 집값을 예측하는데 사용한 변수가 17개라고 생각하시면 됩니다. \n\n(506, 17)\n\n\n\n### 결측치 : 비어있는 데이터를 찾습니다. \n### 여기서는 어떤 컬럼(변수, 특성, x)에 결측치가 많은지 봅니다.\ndf.isnull().sum()\n\nTOWN       0\nLON        0\nLAT        0\nCMEDV      0\nCRIM       0\nZN         0\nINDUS      0\nCHAS       0\nNOX        0\nRM         0\nAGE        0\nDIS        0\nRAD        0\nTAX        0\nPTRATIO    0\nB          0\nLSTAT      0\ndtype: int64\n\n\n\n#### 해석) 다행입니다. 506개의 관측치(observations, 집들)에서, 17개의 변수(variable, features, columns, x)에 결측치가 없습니다.\n### 만약 결측치가 있다면, 결측치를 처리해야 됩니다. 이것을 전처리라고 합니다.\n\n\n### data type을 확인합니다.\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 506 entries, 0 to 505\nData columns (total 17 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   TOWN     506 non-null    object \n 1   LON      506 non-null    float64\n 2   LAT      506 non-null    float64\n 3   CMEDV    506 non-null    float64\n 4   CRIM     506 non-null    float64\n 5   ZN       506 non-null    float64\n 6   INDUS    506 non-null    float64\n 7   CHAS     506 non-null    int64  \n 8   NOX      506 non-null    float64\n 9   RM       506 non-null    float64\n 10  AGE      506 non-null    float64\n 11  DIS      506 non-null    float64\n 12  RAD      506 non-null    int64  \n 13  TAX      506 non-null    int64  \n 14  PTRATIO  506 non-null    float64\n 15  B        506 non-null    float64\n 16  LSTAT    506 non-null    float64\ndtypes: float64(13), int64(3), object(1)\nmemory usage: 67.3+ KB\n\n\n\n### 해셕)Town(소속도시이름)만 문자형 변수(범주형 변수)이고, 이를 제외한 모든 변수는 숫자형입니다.\n\n\n### 슷지형 변수만 뽑아서 기본 통계를 해 봅니다.\ndf.describe()\n\n\n\n\n\n  \n    \n      \n      LON\n      LAT\n      CMEDV\n      CRIM\n      ZN\n      INDUS\n      CHAS\n      NOX\n      RM\n      AGE\n      DIS\n      RAD\n      TAX\n      PTRATIO\n      B\n      LSTAT\n    \n  \n  \n    \n      count\n      506.000000\n      506.000000\n      506.000000\n      506.000000\n      506.000000\n      506.000000\n      506.000000\n      506.000000\n      506.000000\n      506.000000\n      506.000000\n      506.000000\n      506.000000\n      506.000000\n      506.000000\n      506.000000\n    \n    \n      mean\n      -71.056389\n      42.216440\n      22.528854\n      3.613524\n      11.363636\n      11.136779\n      0.069170\n      0.554695\n      6.284634\n      68.574901\n      3.795043\n      9.549407\n      408.237154\n      18.455534\n      356.674032\n      12.653063\n    \n    \n      std\n      0.075405\n      0.061777\n      9.182176\n      8.601545\n      23.322453\n      6.860353\n      0.253994\n      0.115878\n      0.702617\n      28.148861\n      2.105710\n      8.707259\n      168.537116\n      2.164946\n      91.294864\n      7.141062\n    \n    \n      min\n      -71.289500\n      42.030000\n      5.000000\n      0.006320\n      0.000000\n      0.460000\n      0.000000\n      0.385000\n      3.561000\n      2.900000\n      1.129600\n      1.000000\n      187.000000\n      12.600000\n      0.320000\n      1.730000\n    \n    \n      25%\n      -71.093225\n      42.180775\n      17.025000\n      0.082045\n      0.000000\n      5.190000\n      0.000000\n      0.449000\n      5.885500\n      45.025000\n      2.100175\n      4.000000\n      279.000000\n      17.400000\n      375.377500\n      6.950000\n    \n    \n      50%\n      -71.052900\n      42.218100\n      21.200000\n      0.256510\n      0.000000\n      9.690000\n      0.000000\n      0.538000\n      6.208500\n      77.500000\n      3.207450\n      5.000000\n      330.000000\n      19.050000\n      391.440000\n      11.360000\n    \n    \n      75%\n      -71.019625\n      42.252250\n      25.000000\n      3.677083\n      12.500000\n      18.100000\n      0.000000\n      0.624000\n      6.623500\n      94.075000\n      5.188425\n      24.000000\n      666.000000\n      20.200000\n      396.225000\n      16.955000\n    \n    \n      max\n      -70.810000\n      42.381000\n      50.000000\n      88.976200\n      100.000000\n      27.740000\n      1.000000\n      0.871000\n      8.780000\n      100.000000\n      12.126500\n      24.000000\n      711.000000\n      22.000000\n      396.900000\n      37.970000\n    \n  \n\n\n\n\n\n### 범주형 변수(Categorical Variables): 문자열 변수\n### 타운의 수, 소속도시의 수를 구합니다. \n### .unique()는 중복을 제거 하는 것입니다. \n### 2명이 노원구에 산다고, 노원구가 2개 되면 안되겠죠?:-) 노원구는 1개니까요. :-)\n\nnum_town=df['TOWN'].unique()\nprint(len(num_town))\nnum_town\n\n92\n\n\narray(['Nahant', 'Swampscott', 'Marblehead', 'Salem', 'Lynn', 'Sargus',\n       'Lynnfield', 'Peabody', 'Danvers', 'Middleton', 'Topsfield',\n       'Hamilton', 'Wenham', 'Beverly', 'Manchester', 'North Reading',\n       'Wilmington', 'Burlington', 'Woburn', 'Reading', 'Wakefield',\n       'Melrose', 'Stoneham', 'Winchester', 'Medford', 'Malden',\n       'Everett', 'Somerville', 'Cambridge', 'Arlington', 'Belmont',\n       'Lexington', 'Bedford', 'Lincoln', 'Concord', 'Sudbury', 'Wayland',\n       'Weston', 'Waltham', 'Watertown', 'Newton', 'Natick', 'Framingham',\n       'Ashland', 'Sherborn', 'Brookline', 'Dedham', 'Needham',\n       'Wellesley', 'Dover', 'Medfield', 'Millis', 'Norfolk', 'Walpole',\n       'Westwood', 'Norwood', 'Sharon', 'Canton', 'Milton', 'Quincy',\n       'Braintree', 'Randolph', 'Holbrook', 'Weymouth', 'Cohasset',\n       'Hull', 'Hingham', 'Rockland', 'Hanover', 'Norwell', 'Scituate',\n       'Marshfield', 'Duxbury', 'Pembroke', 'Boston Allston-Brighton',\n       'Boston Back Bay', 'Boston Beacon Hill', 'Boston North End',\n       'Boston Charlestown', 'Boston East Boston', 'Boston South Boston',\n       'Boston Downtown', 'Boston Roxbury', 'Boston Savin Hill',\n       'Boston Dorchester', 'Boston Mattapan', 'Boston Forest Hills',\n       'Boston West Roxbury', 'Boston Hyde Park', 'Chelsea', 'Revere',\n       'Winthrop'], dtype=object)\n\n\n\n\n2-2 종속변수(타겟변수, y)의 탐색\n- CMEDV : 주택가격이 타겟변수가 됩니다. 왜냐면, 저희는 주택의 가격을 맞추기 위하이기 때문입니다. \n- matplot 참고 사이트:\n    - https://codetorial.net/matplotlib/basic_plot.html\n    - https://matplotlib.org/stable/gallery/index.html\n\n### 기초 통계량을 다시 살펴봅니다.\ndf['CMEDV'].describe()\n\ncount    506.000000\nmean      22.528854\nstd        9.182176\nmin        5.000000\n25%       17.025000\n50%       21.200000\n75%       25.000000\nmax       50.000000\nName: CMEDV, dtype: float64\n\n\n\n### 시각화를 해서 살펴봅니다. \n### 데이터의 분포를 파악할때, 시각화각 가장 좋은 방법 중 한개 입니다.\n### .hist(): 히스토그램을 의미합니다. bins=50:주머니가 50개 이다. x가 50개로 나누어 진다라고 이해하셔도 좋습니다. \n### y축은 frequency 빈도수입니다. x 축은 실제 주택 값입니다. \n## $50,000 근처의 집도 16~17개는 된다??\n\ndf['CMEDV'].hist()\n\n<Axes: >\n\n\n\n\n\n\n### 분포를 보는 또 다른 시각화 방법이 있습니다. 여러가지로 찍어 보겠습니다.\n### 첫번째 :boxplot 입니다.\n\nplt.boxplot(df['CMEDV'])\nplt.show()\n\n\n\n\n\n### 해석) $17,000~$25000 사이에 분포되어 있고, $40,000 이상인 고가 주택도 존재합니다.\n\n\n### numerical features(except \"LON\" & \"LAT\") :  LON, LAT: 해당 지역의 경도(Longitudes) 위도(Latitudes) 정보\nnumerical_columns=['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n\n### figsize()는 plot()의 기본 크기를 지정합니다. \nfig = plt.figure(figsize = (16, 20))\nax = fig.gca()  # Axes 생성\n\n### gca()\n### gca()로 현재의 Axes를 할당한다.\n### ax=plt.gica(): 축의 위치를 호출하여 ax로 설정(축 위치 변경을 위해 필요한 과정)\n\ndf[numerical_columns].hist(ax=ax)\nplt.show()\n\nUserWarning: To output multiple subplots, the figure containing the passed axes is being cleared\n  df[numerical_columns].hist(ax=ax)\n\n\n\n\n\n\n\n2-3 설명변수(독립변수, features, attributes, x) 살펴보기\n\n## AttributeError: 'SubplotSpec' object has no attribute 'is_first_col'\n\n\n## ! pip install --upgrade matplotlib\n\n\n### numerical features(except \"LON\" & \"LAT\") :  LON, LAT: 해당 지역의 경도(Longitudes) 위도(Latitudes) 정보\nnumerical_columns=['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n\n### figsize()는 plot()의 기본 크기를 지정합니다. \nfig = plt.figure(figsize = (16, 20))\nax = fig.gca()  # Axes 생성\n\n### gca()\n### gca()로 현재의 Axes를 할당한다.\n### ax=plt.gica(): 축의 위치를 호출하여 ax로 설정(축 위치 변경을 위해 필요한 과정)\n\ndf[numerical_columns].hist(ax=ax)\nplt.show()\n\nUserWarning: To output multiple subplots, the figure containing the passed axes is being cleared\n  df[numerical_columns].hist(ax=ax)\n\n\n\n\n\n\n\n2-4. 설명변수(x) 와 종속변수(y) 간의 관계 탐색\n\n변수간의 상관관계 파악해 봅니다.\n\n\n### Person 상관계수 : 대표적으로 상관관계 분석시 사용하는 지표입니다.\n### -1 에서 1 사이의 값을 가진다는 특징이 있습니다.\n### 1일 때는 완전 양의 상관(perfect positive correlation), -1일 때는 완전 음의 상관관계(perfect negative correlation)관계를 보입니다.\n### https://m.blog.naver.com/istech7/50153047118\n\ncols = ['CMEDV', 'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n\ncorr = df[cols].corr(method = 'pearson')\ncorr\n\n\n\n\n\n  \n    \n      \n      CMEDV\n      CRIM\n      ZN\n      INDUS\n      CHAS\n      NOX\n      RM\n      AGE\n      DIS\n      RAD\n      TAX\n      PTRATIO\n      B\n      LSTAT\n    \n  \n  \n    \n      CMEDV\n      1.000000\n      -0.389582\n      0.360386\n      -0.484754\n      0.175663\n      -0.429300\n      0.696304\n      -0.377999\n      0.249315\n      -0.384766\n      -0.471979\n      -0.505655\n      0.334861\n      -0.740836\n    \n    \n      CRIM\n      -0.389582\n      1.000000\n      -0.200469\n      0.406583\n      -0.055892\n      0.420972\n      -0.219247\n      0.352734\n      -0.379670\n      0.625505\n      0.582764\n      0.289946\n      -0.385064\n      0.455621\n    \n    \n      ZN\n      0.360386\n      -0.200469\n      1.000000\n      -0.533828\n      -0.042697\n      -0.516604\n      0.311991\n      -0.569537\n      0.664408\n      -0.311948\n      -0.314563\n      -0.391679\n      0.175520\n      -0.412995\n    \n    \n      INDUS\n      -0.484754\n      0.406583\n      -0.533828\n      1.000000\n      0.062938\n      0.763651\n      -0.391676\n      0.644779\n      -0.708027\n      0.595129\n      0.720760\n      0.383248\n      -0.356977\n      0.603800\n    \n    \n      CHAS\n      0.175663\n      -0.055892\n      -0.042697\n      0.062938\n      1.000000\n      0.091203\n      0.091251\n      0.086518\n      -0.099176\n      -0.007368\n      -0.035587\n      -0.121515\n      0.048788\n      -0.053929\n    \n    \n      NOX\n      -0.429300\n      0.420972\n      -0.516604\n      0.763651\n      0.091203\n      1.000000\n      -0.302188\n      0.731470\n      -0.769230\n      0.611441\n      0.668023\n      0.188933\n      -0.380051\n      0.590879\n    \n    \n      RM\n      0.696304\n      -0.219247\n      0.311991\n      -0.391676\n      0.091251\n      -0.302188\n      1.000000\n      -0.240265\n      0.205246\n      -0.209847\n      -0.292048\n      -0.355501\n      0.128069\n      -0.613808\n    \n    \n      AGE\n      -0.377999\n      0.352734\n      -0.569537\n      0.644779\n      0.086518\n      0.731470\n      -0.240265\n      1.000000\n      -0.747881\n      0.456022\n      0.506456\n      0.261515\n      -0.273534\n      0.602339\n    \n    \n      DIS\n      0.249315\n      -0.379670\n      0.664408\n      -0.708027\n      -0.099176\n      -0.769230\n      0.205246\n      -0.747881\n      1.000000\n      -0.494588\n      -0.534432\n      -0.232471\n      0.291512\n      -0.496996\n    \n    \n      RAD\n      -0.384766\n      0.625505\n      -0.311948\n      0.595129\n      -0.007368\n      0.611441\n      -0.209847\n      0.456022\n      -0.494588\n      1.000000\n      0.910228\n      0.464741\n      -0.444413\n      0.488676\n    \n    \n      TAX\n      -0.471979\n      0.582764\n      -0.314563\n      0.720760\n      -0.035587\n      0.668023\n      -0.292048\n      0.506456\n      -0.534432\n      0.910228\n      1.000000\n      0.460853\n      -0.441808\n      0.543993\n    \n    \n      PTRATIO\n      -0.505655\n      0.289946\n      -0.391679\n      0.383248\n      -0.121515\n      0.188933\n      -0.355501\n      0.261515\n      -0.232471\n      0.464741\n      0.460853\n      1.000000\n      -0.177383\n      0.374044\n    \n    \n      B\n      0.334861\n      -0.385064\n      0.175520\n      -0.356977\n      0.048788\n      -0.380051\n      0.128069\n      -0.273534\n      0.291512\n      -0.444413\n      -0.441808\n      -0.177383\n      1.000000\n      -0.366087\n    \n    \n      LSTAT\n      -0.740836\n      0.455621\n      -0.412995\n      0.603800\n      -0.053929\n      0.590879\n      -0.613808\n      0.602339\n      -0.496996\n      0.488676\n      0.543993\n      0.374044\n      -0.366087\n      1.000000\n    \n  \n\n\n\n\n\n### 상관관계를 직관적으로 살펴보기 위해 Heatmap 으로 돌려봅니다.\n### heatmap (seaborn): 여기서는 seaborn 시각화 라이브러리를 사용해서 표현합니다. \n### 시각화의 대표적인 라이브러리가 matplot(https://matplotlib.org/)과 seaborn(https://seaborn.pydata.org/)이 있습니다.\n\nfig = plt.figure(figsize = (16, 12))\nax = fig.gca()\n\n# https://seaborn.pydata.org/generated/seaborn.heatmap.html\nsns.set(font_scale = 1.5)  # heatmap 안의 font-size 설정 \nheatmap = sns.heatmap(corr.values, annot = True, fmt='.2f', annot_kws={'size':15},\n                      yticklabels = cols, xticklabels = cols, ax=ax, cmap = \"RdYlBu\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n### 해설) \n### 우리의 관심사인 target variable **“CMEDV - 주택 가격”**과 다른 변수간의 상관관계를 살펴보면,\n### “CMEDV - 주택 가격”\n### “RM - 자택당 평균 방 갯수”(0.7) (양의 상관관계)\n### “LSTAT - 빈곤층의 비율”(-0.74) (음의 상관관게)\n## 과 강한 상관관계를 보이고 있다는 것을 알 수 있습니다.\n\n\n2.4.1 CMEDV - 주택 가격~ 방 갯수(“RM”)의 관계\n\n### scatter plot 산점도, https://seaborn.pydata.org/generated/seaborn.scatterplot.html\nsns.scatterplot(data=df, x='RM', y='CMEDV', markers='o', color='blue', alpha=0.6)\nplt.title('Scatter Plot')\nplt.show()\n\n\n\n\n\n### 해설) \n### 주택 가격이 방 갯수와 양의 상관관계(positive correlation)를 갖고 있습니다. \n### 즉, 방 갯수가 많은 주택들이 상대적으로 더 높은 가격을 갖고 있습니다.\n\n\n\n2.4.2 CMEDV - 빈곤층의 비율(“LSTAT”)의 관계\n\n# scatter plot\nsns.scatterplot(data=df, x='LSTAT', y='CMEDV', markers='o', color='blue', alpha=0.6)\nplt.title('Scatter Plot')\nplt.show()\n\n\n\n\n\n### 해설)\n### 주택 가격이 빈곤층의 비율과 음의 상관관계(negative correlation)를 갖고 있습니다. \n### 즉, 빈곤층의 비율이 높은 지역의 주택 가격이 상대적으로 낮은 경향이 있습니다.\n\n\n\n2.4.3 도시별 차이 탐색\n\n데이터를 살펴보면 여러 지역이 같은 도시에 속한 경우가 있습니다.\n변수 중에서도 도시 단위로 측정되는 변수가 많고요. 따라서 우리는 자연스럽게 도시 간의 차이를 궁금하게 됩니다.\n다시한번 변수의 설명을 열거해 봅니다.\n\n\nTOWN: 소속 도시 이름\nLON, LAT: 해당 지역의 경도(Longitudes) 위도(Latitudes) 정보\nCMEDV: 해당 지역의 주택 가격 (중앙값) (corrected median values of housing)(단위: USD 1000)\nCRIM: 지역 범죄율 (per capita crime)\nZN: 소속 도시에 25,000 제곱 피트(sq.ft) 이상의 주택지 비율\nINDUS: 소속 도시에 상업적 비즈니스에 활용되지 않는 농지 면적\nCHAS: 해당 지역이 Charles 강과 접하고 있는지 여부 (dummy variable)\nNOX: 소속 도시의 산화질소 농도\nRM: 해당 지역의 자택당 평균 방 갯수\nAGE: 해당 지역에 1940년 이전에 건설된 주택의 비율\nDIS: 5개의 보스턴 고용 센터와의 거리에 따른 가중치 부여\nRAD: 소속 도시가 Radial 고속도로와의 접근성 지수\nTAX: 소속 도시의 10000달러당 재산세\nPTRATIO: 소속 도시의 학생-교사 비율\nB: 해당 지역의 흑인 지수 (1000(Bk - 0.63)^2), Bk는 흑인의 비\nLSTAT: 해당 지역의 빈곤층 비율\n\n\n### 각 도시 데이터 갯수 살펴보기 \ndf['TOWN'].value_counts()\n\nCambridge            30\nBoston Savin Hill    23\nLynn                 22\nBoston Roxbury       19\nNewton               18\n                     ..\nMedfield              1\nDover                 1\nLincoln               1\nSherborn              1\nNahant                1\nName: TOWN, Length: 92, dtype: int64\n\n\n\n### 해설) 도시 갯수가 92개 입니다.\n\n\n### 각 도시 데이터 갯수 살펴보기 (bar plot)을 활용해서 시각화를 합니다.\n### 위의 각 도시 데이터 갯수를 x 축으로, 합니다. \n### x 축을 50개로 나누었습니다.\n\ndf['TOWN'].value_counts().hist(bins=50)\n\n<Axes: >\n\n\n\n\n\n\n### 도시별 주택 가격 특징 (boxplot 이용)\nfig = plt.figure(figsize = (12, 20))\nsns.boxplot(x='CMEDV', y='TOWN', data=df)\n\n<Axes: xlabel='CMEDV', ylabel='TOWN'>\n\n\n\n### 해설) Boston 지역(Boston으로 시작하는 도시)의 주택 가격이 전반적으로 다른 지역보다 낮다는 것을 알 수 있습니다.\n\n\n### 도시별 범죄율을 확인해 보겠습니다.\n### 도시별 범죄율 특징\nfig = plt.figure(figsize = (12, 20))\nsns.boxplot(x='CRIM', y='TOWN', data=df)\n\n\n### 해설) Boston 지역의 범죄율이 유독 높다는 것을 확인할 수 있고, 따라서 범죄율이 높은 지역의 주택 가격이 상대적으로 낮다는 것을 추측해볼 수 있겠습니다.\n\n1. Library & Data Import\n2. 데이터 파악 (EDA: 탐색적 데이터 분석)\n    2-1. 데이터셋 기본 정보 파악\n    2-2. 종속 변수(목표 변수) 탐색\n    2-3. 설명 변수 탐색\n    2-4. 설명변수와 종속변수 간의 관계 탐색\n3. 주택 가격 예측 모델링: 회귀 분석\n    3-1. 데이터 전처리\n    3-2. 회귀 모델링\n    3-3. 모델 해석\n    3-4. 모델 예측 결과 및 성능 평가\n\n\n\n\n3. 주택가격 예측 모델링: 회귀분석\n\n이제 변수들을 활용하여 매사추세츠주 각 지역의 주택 가격을 예측하는 회귀 모델을 만들어 보겠습니다.\n\n\n3-1 데이터 전처리\n\n먼저 Feature 들의 scale 차이를 없애기 위해 수치형 Feature에 대해서 표준화를 진행해야 합니다.\n\n\ndf.head()\n\n\n데이터를 살펴보면 여러 지역이 같은 도시에 속한 경우가 있습니다.\n변수 중에서도 도시 단위로 측정되는 변수가 많고요. 따라서 우리는 자연스럽게 도시 간의 차이를 궁금하게 됩니다.\n다시한번 변수의 설명을 열거해 봅니다.\n\nTOWN: 소속 도시 이름\nLON, LAT: 해당 지역의 경도(Longitudes) 위도(Latitudes) 정보\nCMEDV: 해당 지역의 주택 가격 (중앙값) (corrected median values of housing)(단위: USD 1000)\nCRIM: 지역 범죄율 (per capita crime)\nZN: 소속 도시에 25,000 제곱 피트(sq.ft) 이상의 주택지 비율\nINDUS: 소속 도시에 상업적 비즈니스에 활용되지 않는 농지 면적\nCHAS: 해당 지역이 Charles 강과 접하고 있는지 여부 (dummy variable)\nNOX: 소속 도시의 산화질소 농도\nRM: 해당 지역의 자택당 평균 방 갯수\nAGE: 해당 지역에 1940년 이전에 건설된 주택의 비율\nDIS: 5개의 보스턴 고용 센터와의 거리에 따른 가중치 부여\nRAD: 소속 도시가 Radial 고속도로와의 접근성 지수\nTAX: 소속 도시의 10000달러당 재산세\nPTRATIO: 소속 도시의 학생-교사 비율\nB: 해당 지역의 흑인 지수 (1000(Bk - 0.63)^2), Bk는 흑인의 비\nLSTAT: 해당 지역의 빈곤층 비율\n\n\n\ndf.info()\n\n\n### 문자형 변수인 \"TOWN\"와 범주형 변수인 “CHAS” (Dummy variable), 위도(LON), 경도(LAT)를 제외하고 모든 수치형 변수에 대해서 표준화를 진행합니다.\n### 참고) CHAS: 해당 지역이 Charles 강과 접하고 있는지 여부 (dummy variable)\n### 사이킷런은 파이썬에서 머신러닝 분석을 할 때 유용하게 사용할 수 있는 라이브러리입니다. 여러가지 머신러닝 모듈로 구성되어있습니다.\n\nfrom sklearn.preprocessing import StandardScaler\n\n# feature standardization  (numerical_columns except dummy var.-\"CHAS\")\n\nscaler = StandardScaler()  # 평균 0, 표준편차 1\n## numerical_columns=['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']\nscale_columns = ['CRIM', 'ZN', 'INDUS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']\ndf[scale_columns] = scaler.fit_transform(df[scale_columns])\n\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      TOWN\n      LON\n      LAT\n      CMEDV\n      CRIM\n      ZN\n      INDUS\n      CHAS\n      NOX\n      RM\n      AGE\n      DIS\n      RAD\n      TAX\n      PTRATIO\n      B\n      LSTAT\n    \n  \n  \n    \n      0\n      Nahant\n      -70.955\n      42.2550\n      24.0\n      -0.419782\n      0.284830\n      -1.287909\n      0\n      -0.144217\n      0.413672\n      -0.120013\n      0.140214\n      -0.982843\n      -0.666608\n      -1.459000\n      0.441052\n      -1.075562\n    \n    \n      1\n      Swampscott\n      -70.950\n      42.2875\n      21.6\n      -0.417339\n      -0.487722\n      -0.593381\n      0\n      -0.740262\n      0.194274\n      0.367166\n      0.557160\n      -0.867883\n      -0.987329\n      -0.303094\n      0.441052\n      -0.492439\n    \n    \n      2\n      Swampscott\n      -70.936\n      42.2830\n      34.7\n      -0.417342\n      -0.487722\n      -0.593381\n      0\n      -0.740262\n      1.282714\n      -0.265812\n      0.557160\n      -0.867883\n      -0.987329\n      -0.303094\n      0.396427\n      -1.208727\n    \n    \n      3\n      Marblehead\n      -70.928\n      42.2930\n      33.4\n      -0.416750\n      -0.487722\n      -1.306878\n      0\n      -0.835284\n      1.016303\n      -0.809889\n      1.077737\n      -0.752922\n      -1.106115\n      0.113032\n      0.416163\n      -1.361517\n    \n    \n      4\n      Marblehead\n      -70.922\n      42.2980\n      36.2\n      -0.412482\n      -0.487722\n      -1.306878\n      0\n      -0.835284\n      1.228577\n      -0.511180\n      1.077737\n      -0.752922\n      -1.106115\n      0.113032\n      0.441052\n      -1.026501\n    \n  \n\n\n\n\n\ndf[scale_columns].head()\n\n\n\n\n\n  \n    \n      \n      CRIM\n      ZN\n      INDUS\n      NOX\n      RM\n      AGE\n      DIS\n      RAD\n      TAX\n      PTRATIO\n      B\n      LSTAT\n    \n  \n  \n    \n      0\n      -0.419782\n      0.284830\n      -1.287909\n      -0.144217\n      0.413672\n      -0.120013\n      0.140214\n      -0.982843\n      -0.666608\n      -1.459000\n      0.441052\n      -1.075562\n    \n    \n      1\n      -0.417339\n      -0.487722\n      -0.593381\n      -0.740262\n      0.194274\n      0.367166\n      0.557160\n      -0.867883\n      -0.987329\n      -0.303094\n      0.441052\n      -0.492439\n    \n    \n      2\n      -0.417342\n      -0.487722\n      -0.593381\n      -0.740262\n      1.282714\n      -0.265812\n      0.557160\n      -0.867883\n      -0.987329\n      -0.303094\n      0.396427\n      -1.208727\n    \n    \n      3\n      -0.416750\n      -0.487722\n      -1.306878\n      -0.835284\n      1.016303\n      -0.809889\n      1.077737\n      -0.752922\n      -1.106115\n      0.113032\n      0.416163\n      -1.361517\n    \n    \n      4\n      -0.412482\n      -0.487722\n      -1.306878\n      -0.835284\n      1.228577\n      -0.511180\n      1.077737\n      -0.752922\n      -1.106115\n      0.113032\n      0.441052\n      -1.026501\n    \n  \n\n\n\n\n\ntraining/test set 나누기\n\n나중에 도출될 예측 모델의 예측 성능을 평가하기 위해, 먼저 전체 데이터셋을 “Training set”과 “Test set”으로 나누겠습니다.\nTraining set에서 모델을 학습하고 Test set에서 모델의 예측 성능을 검증할 겁니다.\n\n\n\n### features for linear regression model\n### 참고) CHAS: 해당 지역이 Charles 강과 접하고 있는지 여부 (dummy variable)\n### numerical features(except \"LON\" & \"LAT\") :  LON, LAT: 해당 지역의 경도(Longitudes) 위도(Latitudes) 정보\nnumerical_columns=['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n\ndf[numerical_columns].head()\n\n\n\n\n\n  \n    \n      \n      CRIM\n      ZN\n      INDUS\n      CHAS\n      NOX\n      RM\n      AGE\n      DIS\n      RAD\n      TAX\n      PTRATIO\n      B\n      LSTAT\n    \n  \n  \n    \n      0\n      -0.419782\n      0.284830\n      -1.287909\n      0\n      -0.144217\n      0.413672\n      -0.120013\n      0.140214\n      -0.982843\n      -0.666608\n      -1.459000\n      0.441052\n      -1.075562\n    \n    \n      1\n      -0.417339\n      -0.487722\n      -0.593381\n      0\n      -0.740262\n      0.194274\n      0.367166\n      0.557160\n      -0.867883\n      -0.987329\n      -0.303094\n      0.441052\n      -0.492439\n    \n    \n      2\n      -0.417342\n      -0.487722\n      -0.593381\n      0\n      -0.740262\n      1.282714\n      -0.265812\n      0.557160\n      -0.867883\n      -0.987329\n      -0.303094\n      0.396427\n      -1.208727\n    \n    \n      3\n      -0.416750\n      -0.487722\n      -1.306878\n      0\n      -0.835284\n      1.016303\n      -0.809889\n      1.077737\n      -0.752922\n      -1.106115\n      0.113032\n      0.416163\n      -1.361517\n    \n    \n      4\n      -0.412482\n      -0.487722\n      -1.306878\n      0\n      -0.835284\n      1.228577\n      -0.511180\n      1.077737\n      -0.752922\n      -1.106115\n      0.113032\n      0.441052\n      -1.026501\n    \n  \n\n\n\n\n\nfrom sklearn.model_selection import train_test_split\n\n# split dataset into training & test\nX = df[numerical_columns]\ny = df['CMEDV']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n\n\nX_train.shape, y_train.shape\n\n((404, 13), (404,))\n\n\n\nX_test.shape, y_test.shape\n\n((102, 13), (102,))\n\n\n\ny_train\n### CMEDV 값을 의미합니다.\n\n42     25.3\n58     23.3\n385     7.2\n78     21.2\n424    11.7\n       ... \n255    20.9\n72     22.8\n396    12.5\n235    24.0\n37     21.0\nName: CMEDV, Length: 404, dtype: float64\n\n\n\nX_train\n\n\n\n\n\n  \n    \n      \n      CRIM\n      ZN\n      INDUS\n      CHAS\n      NOX\n      RM\n      AGE\n      DIS\n      RAD\n      TAX\n      PTRATIO\n      B\n      LSTAT\n    \n  \n  \n    \n      42\n      -0.404051\n      -0.487722\n      -0.616727\n      0\n      -0.921667\n      -0.164740\n      -2.203863\n      0.915493\n      -0.752922\n      -1.040783\n      -0.256858\n      0.292704\n      -0.959218\n    \n    \n      58\n      -0.402544\n      0.585267\n      -0.876445\n      0\n      -0.878475\n      -0.198931\n      -1.400194\n      1.910869\n      -0.178120\n      -0.737880\n      0.575395\n      0.372854\n      -0.812036\n    \n    \n      385\n      1.535926\n      -0.487722\n      1.015999\n      0\n      1.255192\n      -1.435535\n      1.049929\n      -1.126122\n      1.661245\n      1.530926\n      0.806576\n      0.441052\n      2.545127\n    \n    \n      78\n      -0.413947\n      -0.487722\n      0.247057\n      0\n      -1.016689\n      -0.074986\n      -0.528960\n      0.579502\n      -0.523001\n      -0.060801\n      0.113032\n      0.325926\n      -0.043883\n    \n    \n      424\n      0.602650\n      -0.487722\n      1.015999\n      0\n      0.253146\n      -1.025233\n      0.072014\n      -0.823122\n      1.661245\n      1.530926\n      0.806576\n      -3.870682\n      0.631754\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      255\n      -0.416388\n      2.945843\n      -1.093850\n      0\n      -1.405414\n      -0.582165\n      -1.759356\n      2.579000\n      -0.982843\n      -0.553762\n      -0.950402\n      0.422193\n      -0.477020\n    \n    \n      72\n      -0.409853\n      -0.487722\n      -0.047680\n      0\n      -1.224009\n      -0.312904\n      -2.161190\n      0.709373\n      -0.637962\n      -0.613155\n      0.344213\n      0.375375\n      -0.999868\n    \n    \n      396\n      0.262832\n      -0.487722\n      1.015999\n      0\n      1.194724\n      0.171480\n      0.975252\n      -1.006947\n      1.661245\n      1.530926\n      0.806576\n      0.441052\n      0.941538\n    \n    \n      235\n      -0.382062\n      -0.487722\n      -0.720322\n      0\n      -0.412006\n      -0.282986\n      -0.251588\n      -0.068046\n      -0.178120\n      -0.601276\n      -0.488039\n      0.220120\n      -0.248537\n    \n    \n      37\n      -0.411191\n      -0.487722\n      -0.755340\n      0\n      -0.481112\n      -0.619206\n      -0.962799\n      0.066151\n      -0.523001\n      -0.767576\n      0.344213\n      0.441052\n      -0.544304\n    \n  \n\n404 rows × 13 columns\n\n\n\n\n다중공산성\n\n회귀 분석에서 하나의 feature(예측 변수)가 다른 feature와의 상관 관계가 높으면 (즉, 다중공선성이 존재하면), 회귀 분석 시 부정적인 영향을 미칠 수 있기 때문에, 모델링 하기 전에 먼저 다중공선성의 존재 여부를 확인해야합니다.\n보통 다중공선성을 판단할 때 VIF값을 확인합니다. 일반적으로, VIF > 10인 feature들은 다른 변수와의 상관관계가 높아, 다중공선성이 존재하는 것으로 판단합니다.\n즉, VIF > 10인 feature들은 설명변수에서 제거하는 것이 좋을 수도 있습니다.\n\n\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\nvif['features'] = X_train.columns\nvif[\"VIF Factor\"] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])] ## X_train.shape[1]=13\nvif.round(1) \n### 소수점 첫째자리까지 표시합니다. 즉, 소수점 둘째짜리에서 반올림 합니다.\n\n\n\n\n\n  \n    \n      \n      features\n      VIF Factor\n    \n  \n  \n    \n      0\n      CRIM\n      1.7\n    \n    \n      1\n      ZN\n      2.5\n    \n    \n      2\n      INDUS\n      3.8\n    \n    \n      3\n      CHAS\n      1.1\n    \n    \n      4\n      NOX\n      4.4\n    \n    \n      5\n      RM\n      1.9\n    \n    \n      6\n      AGE\n      3.2\n    \n    \n      7\n      DIS\n      4.2\n    \n    \n      8\n      RAD\n      8.1\n    \n    \n      9\n      TAX\n      9.8\n    \n    \n      10\n      PTRATIO\n      1.9\n    \n    \n      11\n      B\n      1.4\n    \n    \n      12\n      LSTAT\n      3.0\n    \n  \n\n\n\n\n\n### 해설) VIF값을 확인해보면, 모든 변수의 VIF값이 다 10 이하입니다. 따라서 다중공선성 문제가 존재하지 않아 모든 feature을 활용하여 회귀 모델링을 진행하면 됩니다.\n\n\n\n3-2 회귀모델링\n\n먼저 Training set에서 선형 회귀 예측 모델을 학습합니다.\n그 다음 도출된 모델을 Test set에 적용해 주택 가격(“CMEDV”)을 예측합니다.\n이 결과는 실제 “CMEDV” 값과 비교하여 모델의 예측 성능을 평가하는 데 활용하게 됩니다.\n\n\nfrom sklearn import linear_model\n\n# fit regression model in training set\nlr = linear_model.LinearRegression()\nmodel = lr.fit(X_train, y_train)\n\n# predict in test set\npred_test = lr.predict(X_test)\n\n\n\n3-3 모델해석하기\n\n각 feature, 설명변수, x, 컬럼에 대한 회귀계수를 확인해 보겠습니다.\n\n\n### print coef \n### 계수를 출력합니다.\nprint(lr.coef_)\n\n[-0.9479409   1.39796831  0.14786968  2.13469673 -2.25995614  2.15879342\n  0.12103297 -3.23121173  2.63662665 -1.95959865 -2.05639351  0.65670428\n -3.93702535]\n\n\n\n### \"feature - coefficients\" DataFrame 만들기\n### zip 키워드는 리스트 2개를 하나로 묶습니다.(결과는 튜플로 됩니다.)\n\ncoefs = pd.DataFrame(zip(df[numerical_columns].columns, lr.coef_), columns = ['feature', 'coefficients'])\ncoefs\n\n### from sklearn.model_selection import train_test_split\n\n### split dataset into training & test\n# X = df[numerical_columns]\n# y = df['CMEDV']\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n\n\n\n\n\n  \n    \n      \n      feature\n      coefficients\n    \n  \n  \n    \n      0\n      CRIM\n      -0.947941\n    \n    \n      1\n      ZN\n      1.397968\n    \n    \n      2\n      INDUS\n      0.147870\n    \n    \n      3\n      CHAS\n      2.134697\n    \n    \n      4\n      NOX\n      -2.259956\n    \n    \n      5\n      RM\n      2.158793\n    \n    \n      6\n      AGE\n      0.121033\n    \n    \n      7\n      DIS\n      -3.231212\n    \n    \n      8\n      RAD\n      2.636627\n    \n    \n      9\n      TAX\n      -1.959599\n    \n    \n      10\n      PTRATIO\n      -2.056394\n    \n    \n      11\n      B\n      0.656704\n    \n    \n      12\n      LSTAT\n      -3.937025\n    \n  \n\n\n\n\n\n### 크기 순서대로 나열합니다. \n### 크기 순서로 나열 : 내림차순으로 합니다. 다만 양, 음을 가리지 않습니다. 절대값을 기준으로 합니다. \n### 절대값 기준 함수: coefficients.abs().sort_values \n\ncoefs_new = coefs.reindex(coefs.coefficients.abs().sort_values(ascending=False).index)\ncoefs_new\n\n\n\n\n\n  \n    \n      \n      feature\n      coefficients\n    \n  \n  \n    \n      12\n      LSTAT\n      -3.937025\n    \n    \n      7\n      DIS\n      -3.231212\n    \n    \n      8\n      RAD\n      2.636627\n    \n    \n      4\n      NOX\n      -2.259956\n    \n    \n      5\n      RM\n      2.158793\n    \n    \n      3\n      CHAS\n      2.134697\n    \n    \n      10\n      PTRATIO\n      -2.056394\n    \n    \n      9\n      TAX\n      -1.959599\n    \n    \n      1\n      ZN\n      1.397968\n    \n    \n      0\n      CRIM\n      -0.947941\n    \n    \n      11\n      B\n      0.656704\n    \n    \n      2\n      INDUS\n      0.147870\n    \n    \n      6\n      AGE\n      0.121033\n    \n  \n\n\n\n\n1. TOWN: 소속 도시 이름\n2. LON, LAT: 해당 지역의 경도(Longitudes) 위도(Latitudes) 정보\n3. CMEDV: 해당 지역의 주택 가격 (중앙값) (corrected median values of housing)(단위: USD 1000)\n4. CRIM: 지역 범죄율 (per capita crime)\n5. ZN: 소속 도시에 25,000 제곱 피트(sq.ft) 이상의 주택지 비율\n6. INDUS: 소속 도시에 상업적 비즈니스에 활용되지 않는 농지 면적\n7. CHAS: 해당 지역이 Charles 강과 접하고 있는지 여부 (dummy variable)\n8. NOX: 소속 도시의 산화질소 농도\n9. RM: 해당 지역의 자택당 평균 방 갯수\n10. AGE: 해당 지역에 1940년 이전에 건설된 주택의 비율\n11. DIS: 5개의 보스턴 고용 센터와의 거리에 따른 가중치 부여\n12. RAD: 소속 도시가 Radial 고속도로와의 접근성 지수\n13. TAX: 소속 도시의 10000달러당 재산세\n14. PTRATIO: 소속 도시의 학생-교사 비율\n15. B: 해당 지역의 흑인 지수 (1000(Bk - 0.63)^2), Bk는 흑인의 비\n16. LSTAT: 해당 지역의 빈곤층 비율\n\n### coefficients 를 시각화 합니다. \n\n### figure size\nplt.figure(figsize = (8, 8))\n\n### bar plot : matplotlib.pyplot 모듈의 barh() 함수를 사용해서 수평 막대 그래프를 그릴 수 있습니다. \nplt.barh(coefs_new['feature'], coefs_new['coefficients'])\nplt.title('\"feature - coefficient\" Graph')\nplt.xlabel('coefficients')\nplt.ylabel('features')\nplt.show()\n\n\n\n\n\n유의성 검정을 합니다 (통계에서 변수가 유의하다는 의미입니다).\nX=sm.add_constant(X)\nmodeling - model = sm.OLS(y,X) - result = model.fit() - print(results.summary())\n\n\n## https://stackoverflow.com/questions/68463220/pandas-importing-error-importerror-cannot-import-name-dtypearg-from-pandas\n\n\nimport statsmodels.api as sm\n\nX_train2 = sm.add_constant(X_train)\n### 회귀분석모형 수식을 간단하게 만들기 위해 다음과 같이 상수항을 독립변수 데이터에 추가하는 것을 상수항 결합(bias augmentation)작업이라고 합니다.\n\n### ordinary least square 의 약자로, 거리의 최소값을 기준으로 구하는 함수입니다. \n### 상수항이 추간된 독립변수와 그에 대한 y 값으로 학습을 합니다. \nmodel2 = sm.OLS(y_train, X_train2).fit()\nmodel2.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:          CMEDV        R-squared:             0.734 \n\n\n  Model:                   OLS         Adj. R-squared:        0.725 \n\n\n  Method:             Least Squares    F-statistic:           82.86 \n\n\n  Date:             Tue, 19 Sep 2023   Prob (F-statistic): 1.72e-103\n\n\n  Time:                 21:09:19       Log-Likelihood:      -1191.9 \n\n\n  No. Observations:         404        AIC:                   2412. \n\n\n  Df Residuals:             390        BIC:                   2468. \n\n\n  Df Model:                  13                                     \n\n\n  Covariance Type:      nonrobust                                   \n\n\n\n\n             coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  const      22.4313     0.245    91.399  0.000    21.949    22.914\n\n\n  CRIM       -0.9479     0.290    -3.263  0.001    -1.519    -0.377\n\n\n  ZN          1.3980     0.372     3.758  0.000     0.667     2.129\n\n\n  INDUS       0.1479     0.458     0.323  0.747    -0.753     1.049\n\n\n  CHAS        2.1347     0.899     2.375  0.018     0.367     3.902\n\n\n  NOX        -2.2600     0.490    -4.617  0.000    -3.222    -1.298\n\n\n  RM          2.1588     0.332     6.495  0.000     1.505     2.812\n\n\n  AGE         0.1210     0.415     0.292  0.771    -0.695     0.937\n\n\n  DIS        -3.2312     0.477    -6.774  0.000    -4.169    -2.293\n\n\n  RAD         2.6366     0.671     3.931  0.000     1.318     3.955\n\n\n  TAX        -1.9596     0.731    -2.679  0.008    -3.398    -0.522\n\n\n  PTRATIO    -2.0564     0.319    -6.446  0.000    -2.684    -1.429\n\n\n  B           0.6567     0.272     2.414  0.016     0.122     1.191\n\n\n  LSTAT      -3.9370     0.405    -9.723  0.000    -4.733    -3.141\n\n\n\n\n  Omnibus:       169.952   Durbin-Watson:         1.935 \n\n\n  Prob(Omnibus):  0.000    Jarque-Bera (JB):    859.012 \n\n\n  Skew:           1.762    Prob(JB):           2.94e-187\n\n\n  Kurtosis:       9.213    Cond. No.               10.7 \n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n##!pip install pandas==1.3.0\n##!pip install --force-reinstall pandas\n\n\n## https://stackoverflow.com/questions/71106940/cannot-import-name-centered-from-scipy-signal-signaltools\n\n\n##!pip install statsmodels --upgrade\n\n\nimport statsmodels.api as sm\n## import scipy.signal.signaltools as sm\n\nX_train2 = sm.add_constant(X_train)\n### 회귀분석모형 수식을 간단하게 만들기 위해 다음과 같이 상수항을 독립변수 데이터에 추가하는 것을 상수항 결합(bias augmentation)작업이라고 합니다.\n\n### ordinary least square 의 약자로, 거리의 최소값을 기준으로 구하는 함수입니다. \n### 상수항이 추간된 독립변수와 그에 대한 y 값으로 학습을 합니다. \nmodel2 = sm.OLS(y_train, X_train2).fit()\nmodel2.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:          CMEDV        R-squared:             0.734 \n\n\n  Model:                   OLS         Adj. R-squared:        0.725 \n\n\n  Method:             Least Squares    F-statistic:           82.86 \n\n\n  Date:             Tue, 19 Sep 2023   Prob (F-statistic): 1.72e-103\n\n\n  Time:                 21:14:40       Log-Likelihood:      -1191.9 \n\n\n  No. Observations:         404        AIC:                   2412. \n\n\n  Df Residuals:             390        BIC:                   2468. \n\n\n  Df Model:                  13                                     \n\n\n  Covariance Type:      nonrobust                                   \n\n\n\n\n             coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  const      22.4313     0.245    91.399  0.000    21.949    22.914\n\n\n  CRIM       -0.9479     0.290    -3.263  0.001    -1.519    -0.377\n\n\n  ZN          1.3980     0.372     3.758  0.000     0.667     2.129\n\n\n  INDUS       0.1479     0.458     0.323  0.747    -0.753     1.049\n\n\n  CHAS        2.1347     0.899     2.375  0.018     0.367     3.902\n\n\n  NOX        -2.2600     0.490    -4.617  0.000    -3.222    -1.298\n\n\n  RM          2.1588     0.332     6.495  0.000     1.505     2.812\n\n\n  AGE         0.1210     0.415     0.292  0.771    -0.695     0.937\n\n\n  DIS        -3.2312     0.477    -6.774  0.000    -4.169    -2.293\n\n\n  RAD         2.6366     0.671     3.931  0.000     1.318     3.955\n\n\n  TAX        -1.9596     0.731    -2.679  0.008    -3.398    -0.522\n\n\n  PTRATIO    -2.0564     0.319    -6.446  0.000    -2.684    -1.429\n\n\n  B           0.6567     0.272     2.414  0.016     0.122     1.191\n\n\n  LSTAT      -3.9370     0.405    -9.723  0.000    -4.733    -3.141\n\n\n\n\n  Omnibus:       169.952   Durbin-Watson:         1.935 \n\n\n  Prob(Omnibus):  0.000    Jarque-Bera (JB):    859.012 \n\n\n  Skew:           1.762    Prob(JB):           2.94e-187\n\n\n  Kurtosis:       9.213    Cond. No.               10.7 \n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n### 해설) coeff를 봐야 합니다.\n### P>|t|를 봐야 합니다. p-value를 의미한다. 0.05 보다 작아야 유의하다!! \n### R-squre 를 봐야 합니다. 설명력을 의미합니다.\n\n\n|기억하기|\n\n결정 계수 (coefficient of determination)는 추정한 선형 모형이 주어진 자료에 적합한 정도를 재는 척도이다.\n\n종속 변수의 변동량 중에서 적용한 모형으로 설명 가능한 부분의 비율을 가리킨다.\n결정계수의 통상적인 기호는 R²이다.\n일반적으로 모형의 설명력으로 해석되지만 모형에 설명 변수가 들어갈수록 증가하기 때문에 해석에 주의해야 한다.\n결정계수의 값은 0에서 1 사이에 있으며, 종속 변인과 독립변인 사이에 상관관계가 높을수록 1에 가까워진다.\n즉, 결정계수가 0에 가까운 값을 가지는 회귀모형은 유용성이 낮은 반면, 결정계수의 값이 클수록 회귀모형의 유용성이 높다고 할 수 있다.\n\n\n\n\n| 해석하기 |\n\n“INDUS”(상업적 비즈니스에 활용되지 않는 농지 면적: p-value: 0.747)과 “AGE”(1940년 이전에 건설된 비율: p-value:0.771)은 유의하지 않습니다. (p value > 0.05)\n주택 가격에 Positive(coefficient > 0)한 영향을 미칩니다.\n\n\n“ZN”(25,000 제곱 피트(sq.ft) 이상의 주택지 비율),\n“CHAS”(Charles 강과 접하고 있는지 여부),\n“RM”(자택당 평균 방 갯수),\n“RAD”(소속 도시가 Radial 고속도로와의 접근성 지수),\n\n“B”(흑인 지수)는\n\n다른 변수의 값이 고정했을 때, 해당 변수의 값이 클수록 주택의 가격이 높을 것입니다.\n\n\n\n\n주택 가격에 Negative(coefficient < 0)한 영향을 미칩니다.\n\n\n“CRIM”(지역 범죄율),\n“NOX”(산화질소 농도),\n“DIS”(보스턴 고용 센터와의 거리),\n“TAX”(재산세),\n“PTRATIO”(학생-교사 비율),\n“LSTAT”(빈곤층 비율)은 :다른 변수의 값이 고정했을 때, 해당 변수의 값이 작을수록 주택의 가격이 높을 것입니다.\n\n\n\n\n3-4 모델 에측 결과 및 성능 평가\n\n예측 결과를 가시화 합니다\n\n학습한 모델을 Test set에 적용하여 y값(“CMEDV”)을 예측합니다.\n예측 결과를 확인하기 위해 실제값과 예측값을 한 plot에 출력해 시각화해보겠습니다.\n\nfrom sklearn import linear_model\n\n# fit regression model in training set\nlr = linear_model.LinearRegression()\nmodel = lr.fit(X_train, y_train)\n\n# predict in test set\npred_test = lr.predict(X_test)\n\n\n### 예측 결과 시각화 (test set)\ndf = pd.DataFrame({'actual': y_test, 'prediction': pred_test})\ndf = df.sort_values(by='actual').reset_index(drop=True)\ndf.head()\n\n### reset_index() : 아무래도 데이터프레임의 다양한 전처리 과정을 거치게 되면 인덱스가 뒤죽박죽인 경우가 많다. 이럴때 인덱스를 다시 처음부터 재배열 해주는 유용한 함수다.\n### drop=True옵션을 주면 기존 인덱스를 버리고 재배열해준다.\n### https://yganalyst.github.io/data_handling/Pd_2/\n\n\n\n\n\n  \n    \n      \n      actual\n      prediction\n    \n  \n  \n    \n      0\n      6.3\n      10.769593\n    \n    \n      1\n      8.1\n      4.473377\n    \n    \n      2\n      8.3\n      10.669609\n    \n    \n      3\n      8.4\n      14.509258\n    \n    \n      4\n      8.5\n      16.522647\n    \n  \n\n\n\n\n\ndf.tail(20)\n\n\n\n\n\n  \n    \n      \n      actual\n      prediction\n    \n  \n  \n    \n      82\n      27.9\n      32.642201\n    \n    \n      83\n      27.9\n      20.686190\n    \n    \n      84\n      28.2\n      32.792043\n    \n    \n      85\n      28.7\n      25.088598\n    \n    \n      86\n      29.8\n      25.100201\n    \n    \n      87\n      30.1\n      29.576537\n    \n    \n      88\n      31.2\n      28.746416\n    \n    \n      89\n      34.9\n      34.560048\n    \n    \n      90\n      36.2\n      27.223864\n    \n    \n      91\n      37.2\n      32.469474\n    \n    \n      92\n      37.3\n      34.878258\n    \n    \n      93\n      41.7\n      36.717567\n    \n    \n      94\n      43.1\n      36.455461\n    \n    \n      95\n      44.8\n      37.130158\n    \n    \n      96\n      48.3\n      35.874772\n    \n    \n      97\n      50.0\n      33.903719\n    \n    \n      98\n      50.0\n      41.788581\n    \n    \n      99\n      50.0\n      39.182687\n    \n    \n      100\n      50.0\n      43.229771\n    \n    \n      101\n      50.0\n      38.246342\n    \n  \n\n\n\n\n\nplt.figure(figsize=(12, 9))\nplt.scatter(df.index, df['prediction'], marker='x', color='r')\nplt.scatter(df.index, df['actual'], alpha=0.3, marker='o', color='black')\nplt.title(\"Prediction Result in Test Set\", fontsize=20)\nplt.legend(['prediction', 'actual'], fontsize=12)\nplt.show()\n### x축은 집 index, y축이 집값\n\n\n\n\n\n모델 성능 평가 (R squre 와 RMSE)\n\n\n### R square\nprint(model.score(X_train, y_train))  # training set\nprint(model.score(X_test, y_test))  # test set\n\n0.7341832055169144\n0.7639579157366424\n\n\n\n# RMSE\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\n# training set\npred_train = lr.predict(X_train)\nprint(sqrt(mean_squared_error(y_train, pred_train)))\n\n# test set\n\nprint(sqrt(mean_squared_error(y_test, pred_test)))\n\n\n### 해설) Test set에서 해당 예측 모델의 R square가 0.76이고, RMSE가 4.83입니다."
  },
  {
    "objectID": "ITM812/Fall2023_Regression_BostonHousingPrediction_Lecture_upload/과제.html",
    "href": "ITM812/Fall2023_Regression_BostonHousingPrediction_Lecture_upload/과제.html",
    "title": "Jiseok AHN",
    "section": "",
    "text": "Regression 실습 : 집값 예측하기\n\n미국 매사추세츠주의 주택 가격 데이터(Boston Housing 1970)를 활용해 지역의 평균 주택 가격을 예측하는 선형 회귀 모델을 학습한다.\n이를 기초로 하여 주택 가격의 영향 요소 파악 및 주택 가격 예측을 진행할 수 있다.\n\nLibrary & Data Import\n데이터 파악 (EDA: 탐색적 데이터 분석) 2-1. 데이터셋 기본 정보 파악 2-2. 종속 변수(목표 변수) 탐색 2-3. 설명 변수 탐색 2-4. 설명변수와 종속변수 간의 관계 탐색\n주택 가격 예측 모델링: 회귀 분석 3-1. 데이터 전처리 3-2. 회귀 모델링 3-3. 모델 해석 3-4. 모델 예측 결과 및 성능 평가\n\n\n\n\n1. Import Libraries & Data Set\n\nimport pandas as pd ### 데이터 분석을 하기 위한 파이썬 라이브러리 such as a table\nimport numpy as np ### 수치해석 라이브러리\nimport matplotlib.pyplot as plt ### 그래프 그리는 라이브러리\nimport seaborn as sns ### 그래프 그리는 라이브러리\n%matplotlib inline \n### 출력을 jupyter lab으로\n\n#import warnings\n#warnings.filterwarnings(\"ignore\", category=FutureWarning)\n#warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n\n\nImport Boston Housing Dataset\n\n분석에 사용될 데이터셋은 Boston Housing 데이터의 일부 변수를 추출한 데이터입니다.\n미국 매사추세츠주 92개 도시(TOWN)의 506개 지역의 주택 가격 및 기타 지역 특성 데이터가 포함되어 있습니다. (Dataset Introduction)\n\n\ndf=pd.read_csv(\"./BostonHouse/demand_trade.csv\",  encoding='cp949')\n\n\n\n\n2. EDA (Exploratory Data Analysis)\n\n#데이터셋을 불러와서 첫 세 줄을 출력하여 데이터의 구성을 한 번 살펴볼게요.\ndf.tail(10)\n#df.tail(3)\n\n\n\n\n\n  \n    \n      \n      전력수요\n      원자력\n      유연탄\n      무연탄\n      바이오가스\n      부생가스\n      소수력\n      수력\n      양수\n      매립가스\n      ...\n      LPG\n      연료전지\n      경유\n      해양에너지\n      가스압\n      바이오매스\n      IGCC\n      바이오중유\n      RPS\n      기타\n    \n  \n  \n    \n      8750\n      73706\n      21611.57852\n      23902.89676\n      0.0\n      43.465699\n      63.942418\n      50.236036\n      216.997832\n      0.297540\n      26.007552\n      ...\n      13.340096\n      603.776762\n      0.0\n      190.618848\n      0\n      635.391697\n      263.12608\n      185.066112\n      0\n      66.313799\n    \n    \n      8751\n      74401\n      21607.37082\n      23869.77610\n      0.0\n      43.566059\n      63.357549\n      48.957665\n      268.057674\n      393.669769\n      25.762960\n      ...\n      12.023872\n      602.820888\n      0.0\n      160.209168\n      0\n      640.263001\n      264.09824\n      185.504312\n      0\n      65.993555\n    \n    \n      8752\n      75972\n      21606.57250\n      23912.40938\n      0.0\n      43.838738\n      66.471086\n      52.666449\n      266.936949\n      1758.277786\n      25.666768\n      ...\n      14.753088\n      603.022961\n      0.0\n      101.889480\n      0\n      641.781027\n      265.06256\n      185.983280\n      0\n      66.507547\n    \n    \n      8753\n      75798\n      21603.31334\n      24071.73105\n      0.0\n      44.281396\n      59.308129\n      51.982697\n      241.257582\n      2345.287308\n      25.679774\n      ...\n      26.767664\n      602.711322\n      0.0\n      29.854776\n      0\n      641.400021\n      264.11112\n      205.018016\n      0\n      65.943663\n    \n    \n      8754\n      74578\n      21602.67549\n      23977.67268\n      0.0\n      44.465464\n      58.252277\n      51.665045\n      229.640478\n      2201.006920\n      25.648395\n      ...\n      34.054832\n      603.045246\n      0.0\n      0.000000\n      0\n      644.101206\n      266.16968\n      205.968056\n      0\n      66.079962\n    \n    \n      8755\n      72976\n      21602.35903\n      23922.15419\n      0.0\n      44.445979\n      58.889583\n      53.217794\n      221.865026\n      2125.206968\n      25.530062\n      ...\n      33.991328\n      603.529542\n      0.0\n      0.000000\n      0\n      642.872043\n      266.58520\n      205.989280\n      0\n      65.303873\n    \n    \n      8756\n      71602\n      21602.06826\n      23721.98757\n      0.0\n      44.515754\n      59.077641\n      54.150084\n      182.704335\n      2024.454339\n      25.365024\n      ...\n      33.786480\n      604.219977\n      0.0\n      0.000000\n      0\n      637.673190\n      266.57064\n      205.992640\n      0\n      65.740697\n    \n    \n      8757\n      69383\n      21600.93928\n      23474.14164\n      0.0\n      44.339380\n      59.033801\n      54.169285\n      84.330694\n      1814.126382\n      25.371944\n      ...\n      30.851184\n      604.657741\n      0.0\n      0.000000\n      0\n      633.697648\n      266.16688\n      205.894024\n      0\n      66.100753\n    \n    \n      8758\n      68874\n      21600.69253\n      23662.44691\n      0.0\n      44.449957\n      59.422150\n      53.064628\n      47.868268\n      1762.303100\n      25.363832\n      ...\n      28.246848\n      604.925284\n      0.0\n      0.000000\n      0\n      634.645627\n      267.70800\n      205.963520\n      0\n      65.475027\n    \n    \n      8759\n      70123\n      21601.90874\n      24206.65930\n      0.0\n      44.298274\n      60.707217\n      49.997528\n      39.134795\n      2131.843248\n      25.346184\n      ...\n      26.412176\n      605.367336\n      0.0\n      0.000000\n      0\n      635.806799\n      267.70912\n      205.779896\n      0\n      65.634424\n    \n  \n\n10 rows × 25 columns\n\n\n\n\n2-1 : 데이터 셋 기본 정보 파악\n\nFeatures Descriptions\n\nTOWN: 소속 도시 이름\nLON, LAT: 해당 지역의 경도(Longitudes) 위도(Latitudes) 정보\nCMEDV: 해당 지역의 주택 가격 (중앙값) (corrected median values of housing)(단위 USD 1000)\nCRIM: 지역 범죄율 (per capita crime)\nZN: 소속 도시에 25,000 제곱 피트(sq.ft) 이상의 주택지 비율\nINDUS: 소속 도시에 상업적 비즈니스에 활용되지 않는 농지 면적\nCHAS: 해당 지역이 Charles 강과 접하고 있는지 여부 (dummy variable)\nNOX: 소속 도시의 산화질소 농도\nRM: 해당 지역의 자택당 평균 방 갯수\nAGE: 해당 지역에 1940년 이전에 건설된 주택의 비율\nDIS: 5개의 보스턴 고용 센터와의 거리에 따른 가중치 부여\nRAD: 소속 도시가 Radial 고속도로와의 접근성 지수\nTAX: 소속 도시의 10000달러당 재산세\nPTRATIO: 소속 도시의 학생-교사 비율\nB: 해당 지역의 흑인 지수 (1000(Bk - 0.63)^2), Bk는 흑인의 비\nLSTAT: 해당 지역의 빈곤층 비율\n\n데이터의 각 변수(features, attributes, columns, x들)의 기본 정보 및 각 변수의 특성을 살펴보겠습니다.\n\n\n### 그래프의 배경 설정 : sns --> seaborn 라이브러리 이용\nsns.set_style('darkgrid')\n\n\n데이터 셋의 기본 정보 파악하기\n\n\n### 데이터 셋 구조 보기\ndf.shape \n### 해석: 행이 506, 집이 506개, 열이 17, 집값을 예측하는데 사용한 변수가 17개라고 생각하시면 됩니다. \n\n(8760, 25)\n\n\n\n### 결측치 : 비어있는 데이터를 찾습니다. \n### 여기서는 어떤 컬럼(변수, 특성, x)에 결측치가 많은지 봅니다.\ndf.isnull().sum()\n\n전력수요     0\n원자력      0\n유연탄      0\n무연탄      0\n바이오가스    0\n부생가스     0\n소수력      0\n수력       0\n양수       0\n매립가스     0\n중유       0\n태양광      0\n폐기물      0\n풍력       0\nLNG      0\nLPG      0\n연료전지     0\n경유       0\n해양에너지    0\n가스압      0\n바이오매스    0\nIGCC     0\n바이오중유    0\nRPS      0\n기타       0\ndtype: int64\n\n\n\n#### 해석) 다행입니다. 506개의 관측치(observations, 집들)에서, 17개의 변수(variable, features, columns, x)에 결측치가 없습니다.\n### 만약 결측치가 있다면, 결측치를 처리해야 됩니다. 이것을 전처리라고 합니다.\n\n\n### data type을 확인합니다.\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 8760 entries, 0 to 8759\nData columns (total 25 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   전력수요    8760 non-null   int64  \n 1   원자력     8760 non-null   float64\n 2   유연탄     8760 non-null   float64\n 3   무연탄     8760 non-null   float64\n 4   바이오가스   8760 non-null   float64\n 5   부생가스    8760 non-null   float64\n 6   소수력     8760 non-null   float64\n 7   수력      8760 non-null   float64\n 8   양수      8760 non-null   float64\n 9   매립가스    8760 non-null   float64\n 10  중유      8760 non-null   float64\n 11  태양광     8760 non-null   float64\n 12  폐기물     8760 non-null   float64\n 13  풍력      8760 non-null   float64\n 14  LNG     8760 non-null   float64\n 15  LPG     8760 non-null   float64\n 16  연료전지    8760 non-null   float64\n 17  경유      8760 non-null   float64\n 18  해양에너지   8760 non-null   float64\n 19  가스압     8760 non-null   int64  \n 20  바이오매스   8760 non-null   float64\n 21  IGCC    8760 non-null   float64\n 22  바이오중유   8760 non-null   float64\n 23  RPS     8760 non-null   int64  \n 24  기타      8760 non-null   float64\ndtypes: float64(22), int64(3)\nmemory usage: 1.7 MB\n\n\n\n### 해셕)Town(소속도시이름)만 문자형 변수(범주형 변수)이고, 이를 제외한 모든 변수는 숫자형입니다.\n\n\n### 슷지형 변수만 뽑아서 기본 통계를 해 봅니다.\ndf.describe().round(1)\n\n\n\n\n\n  \n    \n      \n      전력수요\n      원자력\n      유연탄\n      무연탄\n      바이오가스\n      부생가스\n      소수력\n      수력\n      양수\n      매립가스\n      ...\n      LPG\n      연료전지\n      경유\n      해양에너지\n      가스압\n      바이오매스\n      IGCC\n      바이오중유\n      RPS\n      기타\n    \n  \n  \n    \n      count\n      8760.0\n      8760.0\n      8760.0\n      8760.0\n      8760.0\n      8760.0\n      8760.0\n      8760.0\n      8760.0\n      8760.0\n      ...\n      8760.0\n      8760.0\n      8760.0\n      8760.0\n      8760.0\n      8760.0\n      8760.0\n      8760.0\n      8760.0\n      8760.0\n    \n    \n      mean\n      63737.3\n      17175.4\n      21369.6\n      189.2\n      40.7\n      124.4\n      65.8\n      276.1\n      419.4\n      21.0\n      ...\n      5.8\n      532.3\n      15.5\n      51.9\n      0.0\n      527.3\n      191.5\n      146.7\n      0.0\n      65.4\n    \n    \n      std\n      9156.9\n      1712.9\n      3838.2\n      112.0\n      10.9\n      127.3\n      16.2\n      166.2\n      529.0\n      8.6\n      ...\n      11.7\n      37.9\n      40.0\n      75.1\n      0.0\n      98.7\n      111.2\n      41.2\n      0.0\n      10.1\n    \n    \n      min\n      39720.0\n      14001.9\n      11269.4\n      0.0\n      7.8\n      12.8\n      26.3\n      19.0\n      0.0\n      1.1\n      ...\n      0.0\n      449.7\n      0.0\n      0.0\n      0.0\n      203.2\n      0.0\n      38.7\n      0.0\n      7.6\n    \n    \n      25%\n      56839.5\n      15944.7\n      18143.6\n      136.4\n      35.8\n      46.7\n      52.2\n      115.1\n      0.0\n      18.4\n      ...\n      0.0\n      500.1\n      0.0\n      0.0\n      0.0\n      450.3\n      167.5\n      109.5\n      0.0\n      64.9\n    \n    \n      50%\n      62861.0\n      16987.9\n      21215.4\n      164.2\n      45.7\n      59.0\n      65.7\n      284.3\n      122.8\n      22.9\n      ...\n      0.0\n      525.4\n      0.0\n      0.0\n      0.0\n      538.8\n      255.1\n      139.2\n      0.0\n      68.7\n    \n    \n      75%\n      69732.8\n      18087.4\n      24368.4\n      275.9\n      48.6\n      155.4\n      78.4\n      399.2\n      786.2\n      26.0\n      ...\n      9.4\n      563.1\n      0.0\n      103.7\n      0.0\n      609.2\n      265.8\n      185.0\n      0.0\n      71.6\n    \n    \n      max\n      91141.0\n      21640.2\n      30338.2\n      349.4\n      53.3\n      618.8\n      111.2\n      955.5\n      2945.1\n      36.2\n      ...\n      79.0\n      613.8\n      519.3\n      250.8\n      0.0\n      709.9\n      309.3\n      315.9\n      0.0\n      74.6\n    \n  \n\n8 rows × 25 columns\n\n\n\n\n### 범주형 변수(Categorical Variables): 문자열 변수\n### 타운의 수, 소속도시의 수를 구합니다. \n### .unique()는 중복을 제거 하는 것입니다. \n### 2명이 노원구에 산다고, 노원구가 2개 되면 안되겠죠?:-) 노원구는 1개니까요. :-)\n\nnum_town=df['TOWN'].unique()\nprint(len(num_town))\nnum_town\n\nKeyError: 'TOWN'\n\n\n\n\n2-2 종속변수(타겟변수, y)의 탐색\n- CMEDV : 주택가격이 타겟변수가 됩니다. 왜냐면, 저희는 주택의 가격을 맞추기 위하이기 때문입니다. \n- matplot 참고 사이트:\n    - https://codetorial.net/matplotlib/basic_plot.html\n    - https://matplotlib.org/stable/gallery/index.html\n\n### 기초 통계량을 다시 살펴봅니다.\ndf['전력수요'].describe().round(1)\n\ncount     8760.0\nmean     63737.3\nstd       9156.9\nmin      39720.0\n25%      56839.5\n50%      62861.0\n75%      69732.8\nmax      91141.0\nName: 전력수요, dtype: float64\n\n\n\n### 시각화를 해서 살펴봅니다. \n### 데이터의 분포를 파악할때, 시각화각 가장 좋은 방법 중 한개 입니다.\n### .hist(): 히스토그램을 의미합니다. bins=50:주머니가 50개 이다. x가 50개로 나누어 진다라고 이해하셔도 좋습니다. \n### y축은 frequency 빈도수입니다. x 축은 실제 주택 값입니다. \n## $50,000 근처의 집도 16~17개는 된다??\n\ndf['전력수요'].hist()\n\n<Axes: >\n\n\n\n\n\n\n### 분포를 보는 또 다른 시각화 방법이 있습니다. 여러가지로 찍어 보겠습니다.\n### 첫번째 :boxplot 입니다.\n\nplt.boxplot(df['전력수요'])\nplt.show()\n\n\n\n\n\n### 해석) $17,000~$25000 사이에 분포되어 있고, $40,000 이상인 고가 주택도 존재합니다.\n\nimport matplotlib.font_manager as fm\nf = [f.name for f in fm.fontManager.ttflist]\nprint(f)\n\n\n### 폰트 추가 하기\n# 확인 이후\nplt.rc('font', family='Malgun Gothic')\n\n['DejaVu Sans Mono', 'STIXNonUnicode', 'cmtt10', 'STIXGeneral', 'DejaVu Sans', 'DejaVu Sans', 'DejaVu Serif', 'STIXSizeOneSym', 'cmb10', 'STIXNonUnicode', 'STIXGeneral', 'STIXSizeFourSym', 'cmsy10', 'STIXGeneral', 'STIXSizeThreeSym', 'DejaVu Sans Display', 'DejaVu Sans', 'DejaVu Serif', 'STIXNonUnicode', 'DejaVu Sans Mono', 'DejaVu Sans', 'STIXNonUnicode', 'cmmi10', 'DejaVu Serif Display', 'STIXSizeFiveSym', 'cmex10', 'STIXSizeThreeSym', 'DejaVu Sans Mono', 'STIXSizeTwoSym', 'STIXGeneral', 'cmr10', 'STIXSizeOneSym', 'STIXSizeTwoSym', 'STIXSizeFourSym', 'DejaVu Sans Mono', 'cmss10', 'DejaVu Serif', 'DejaVu Serif', 'KBIZgo M', 'Gill Sans MT', 'Agency FB', 'Arial', 'Microsoft Sans Serif', 'Impact', 'Times New Roman', 'MaruBuriOTF', 'Franklin Gothic Demi Cond', 'Webdings', 'Calibri', 'Bernard MT Condensed', 'Harlow Solid Italic', 'MaruBuriOTF', 'Lucida Sans Typewriter', 'Georgia', 'Cooper Black', 'HYSinMyeongJo-Medium', 'High Tower Text', 'Perpetua', 'Stencil', 'NanumGothicOTF', 'Mistral', 'HoloLens MDL2 Assets', 'Bodoni MT', 'Sitka Small', 'Rage Italic', 'Bodoni MT', 'Berlin Sans FB', 'Kunstler Script', 'HCR Dotum Ext', 'NanumGothic', 'Book Antiqua', 'Fira Code', 'Niagara Engraved', 'Palatino Linotype', 'Cambria', 'Tempus Sans ITC', 'HCR Batang ExtB', 'Eras Light ITC', 'Bookman Old Style', 'Arial Rounded MT Bold', 'Franklin Gothic Medium', 'NanumMyeongjoOTF', 'Fira Code', 'Lucida Fax', 'Batang', 'Pyunji R', 'MJemokGothic', 'Colonna MT', 'MS Outlook', 'Copperplate Gothic Light', 'Rockwell', 'Informal Roman', 'Californian FB', 'Tw Cen MT', 'Candara', 'Constantia', 'Berlin Sans FB Demi', 'LG Smart UI', 'Sitka Small', 'Courier New', 'HYGothic-Medium', 'Arial', 'Corbel', 'Lucida Fax', 'MHunmin', 'Lucida Bright', 'Courier New', 'Segoe Print', 'Ink Free', 'MDotum', 'Candara', 'Poor Richard', 'Lucida Bright', 'Century Schoolbook', 'Gill Sans MT Ext Condensed Bold', 'Brush Script MT', 'Snap ITC', 'Magneto', 'Lucida Bright', 'Microsoft JhengHei', 'Lucida Sans', 'Yet R', 'Wingdings', 'Blackadder ITC', 'Yu Gothic', 'NanumGothicOTF', 'Segoe UI', 'Book Antiqua', 'Agency FB', 'Papyrus', 'Corbel', 'New Gulim', 'Berlin Sans FB', 'Verdana', 'Segoe UI Emoji', 'Calibri', 'Microsoft Uighur', 'Ami R', 'Century', 'MS Reference Specialty', 'Ebrima', 'Lucida Sans Unicode', 'Calisto MT', 'Bahnschrift', 'Candara', 'Headline R', 'Nirmala UI', 'Lucida Sans Typewriter', 'Palatino Linotype', 'Segoe UI', 'D2Coding', 'Microsoft YaHei', 'Book Antiqua', 'Sitka Small', 'Tw Cen MT Condensed', 'Century Gothic', 'Microsoft PhagsPa', 'Cambria', 'Constantia', 'KoPubDotum Light', 'Symbol', 'Footlight MT Light', 'Arial', 'MJemokBatang', 'Fira Code', 'Californian FB', 'Franklin Gothic Medium', 'Microsoft JhengHei', 'Rockwell', 'Bradley Hand ITC', 'MaruBuri', 'MGungHeulim', 'Engravers MT', 'Perpetua Titling MT', 'Bookshelf Symbol 7', 'Yu Gothic', 'Segoe Script', 'Trebuchet MS', 'Garamond', 'Kristen ITC', 'Javanese Text', 'KBIZmjo B', 'Bodoni MT', 'Algerian', 'Rockwell Condensed', 'Tw Cen MT', 'Lucida Sans', 'Segoe UI', 'Viner Hand ITC', 'Rockwell', 'Franklin Gothic Demi', 'Trebuchet MS', 'Lucida Fax', 'MaruBuri', 'Microsoft Tai Le', 'Showcard Gothic', 'Courier New', 'Consolas', 'Dubai', 'Microsoft Himalaya', 'Felix Titling', 'French Script MT', 'Century Gothic', 'MoeumT R', 'KoPubDotum Bold', 'Segoe UI', 'Matura MT Script Capitals', 'Monotype Corsiva', 'Candara', 'Malgun Gothic', 'MaruBuriOTF', 'Bell MT', 'Century Gothic', 'MV Boli', 'MaruBuriOTF', 'Leelawadee', 'Eras Bold ITC', 'Gill Sans Ultra Bold Condensed', 'Palatino Linotype', 'Consolas', 'Forte', 'NanumMyeongjo', 'NanumGothic', 'YiSunShin Dotum B', 'Bodoni MT', 'Arial', 'KBIZmjo M', 'Nirmala UI', 'Lucida Sans', 'Candara', 'Verdana', 'Arial', 'OCR A Extended', 'Sitka Small', 'YiSunShin Dotum L', 'Edwardian Script ITC', 'Bodoni MT', 'Cambria', 'Arial', 'HYPMokGak-Bold', 'Script MT Bold', 'Modern No. 20', 'Franklin Gothic Heavy', 'Parchment', 'Dubai', 'Onyx', 'Perpetua', 'Segoe UI', 'NanumMyeongjoOTF', 'Franklin Gothic Medium Cond', 'Arial', 'Comic Sans MS', 'Bell MT', 'Perpetua', 'Bodoni MT', 'Corbel', 'Niagara Solid', 'Calibri', 'Gigi', 'NewJumja', 'Garamond', 'MGungJeong', 'Lucida Handwriting', 'Broadway', 'Tw Cen MT', 'HYShortSamul-Medium', 'HYGraphic-Medium', 'HYMyeongJo-Extra', 'Californian FB', 'Bodoni MT', 'Microsoft PhagsPa', 'Myanmar Text', 'NanumSquare', 'Gadugi', 'HCR Batang Ext', 'Perpetua', 'Courier New', 'Centaur', 'Leelawadee UI', 'Juice ITC', 'KBIZmjo R', 'HYHeadLine-Medium', 'Franklin Gothic Book', 'Century Schoolbook', 'Constantia', 'HCR Batang', 'Bookman Old Style', 'NanumGothic', 'Bookman Old Style', 'Yu Gothic', 'LG Smart UI', 'Chiller', 'Segoe Print', 'NanumMyeongjo', 'Freestyle Script', 'Leelawadee UI', 'Microsoft New Tai Lue', 'HCR Dotum', 'V3Detect', 'KBIZgo R', 'Wingdings 3', 'Verdana', 'Corbel', 'MS Reference Sans Serif', 'Calibri', 'Dubai', 'Times New Roman', 'Gill Sans MT Condensed', 'Consolas', 'Pristina', 'NanumGothic', 'Eras Demi ITC', 'Segoe UI', 'Gadugi', 'Lucida Calligraphy', 'Georgia', 'Century Schoolbook', 'MaruBuriOTF', 'Vivaldi', 'NanumGothic', 'Tw Cen MT', 'Jokerman', 'Perpetua Titling MT', 'Goudy Stout', 'Leelawadee UI', 'Gill Sans Ultra Bold', 'Segoe UI Historic', 'Dubai', 'Bodoni MT', 'Imprint MT Shadow', 'Calisto MT', 'Comic Sans MS', 'Tahoma', 'Arial', 'Copperplate Gothic Bold', 'Corbel', 'Lucida Sans Typewriter', 'Verdana', 'Franklin Gothic Book', 'Book Antiqua', 'Myanmar Text', 'Cambria', 'Britannic Bold', 'Garamond', 'Century Gothic', 'MaruBuri', 'Microsoft Uighur', 'Franklin Gothic Heavy', 'Goudy Old Style', 'Segoe MDL2 Assets', 'HYPost-Light', 'Calisto MT', 'Lucida Fax', 'Segoe Script', 'Elephant', 'Microsoft Yi Baiti', 'Segoe UI', 'HCR Batang ExtB', 'MSugiJeong', 'Fira Code', 'Trebuchet MS', 'Franklin Gothic Demi', 'Microsoft Tai Le', 'Rockwell Extra Bold', 'Malgun Gothic', 'Georgia', 'Century Schoolbook', 'HYGothic-Extra', 'Palace Script MT', 'HCR Batang', 'HyhwpEQ', 'Magic R', 'Segoe UI', 'Arial', 'Palatino Linotype', 'Rockwell Condensed', 'Fira Code', 'Comic Sans MS', 'Baskerville Old Face', 'Goudy Old Style', 'Gulim', 'Sylfaen', 'Comic Sans MS', 'Gabriola', 'KoPubDotum Medium', 'Gill Sans MT', 'SimSun-ExtB', 'Castellar', 'Tahoma', 'MBatang', 'MaruBuri', 'Calibri', 'Consolas', 'Old English Text MT', 'SimSun', 'HYGungSo-Bold', 'Calisto MT', 'Marlett', 'NanumMyeongjoOTF', 'MaruBuri', 'Constantia', 'Microsoft YaHei', 'Vladimir Script', 'Bauhaus 93', 'LG Smart UI', 'MSugiHeulim', 'Leelawadee', 'KBIZgo B', 'Trebuchet MS', 'MS Gothic', 'Harrington', 'Haettenschweiler', 'Nirmala UI', 'Gloucester MT Extra Condensed', 'Goudy Old Style', 'Tw Cen MT Condensed Extra Bold', 'Lucida Console', 'MingLiU-ExtB', 'Rockwell', 'Lucida Sans', 'Corbel', 'Lucida Sans Typewriter', 'Microsoft New Tai Lue', 'Wide Latin', 'NanumSquare', 'ZWAdobeF', 'Ravie', 'Segoe UI', 'Bell MT', 'Eras Medium ITC', 'MT Extra', 'Playbill', 'Segoe UI', 'NanumGothicOTF', 'Maiandra GD', 'Bodoni MT', 'BareunDotum', 'HYPost-Medium', 'Times New Roman', 'Wingdings 2', 'Segoe UI Symbol', 'Bodoni MT', 'Microsoft YaHei', 'Lucida Bright', 'Georgia', 'Calibri', 'Gill Sans MT', 'Fira Code', 'Segoe UI', 'MaruBuriOTF', 'Times New Roman', 'Yu Gothic', 'Microsoft JhengHei', 'NanumSquare', 'NanumMyeongjo', 'Elephant', 'YiSunShin Dotum M', 'Bookman Old Style', 'HCR Dotum', 'NanumGothicOTF', 'High Tower Text', 'Tw Cen MT Condensed', 'Segoe UI', 'LG Smart UI', 'Ebrima', 'NanumSquare', 'Mongolian Baiti', 'Bodoni MT', 'Gill Sans MT', 'Candara', 'Malgun Gothic', 'Curlz MT']\n\n\n\n### numerical features(except \"LON\" & \"LAT\") :  LON, LAT: 해당 지역의 경도(Longitudes) 위도(Latitudes) 정보\n\n\n### figsize()는 plot()의 기본 크기를 지정합니다. \nfig = plt.figure(figsize = (16, 20))\nax = fig.gca()  # Axes 생성\n\n### gca()\n### gca()로 현재의 Axes를 할당한다.\n### ax=plt.gica(): 축의 위치를 호출하여 ax로 설정(축 위치 변경을 위해 필요한 과정)\n\ndf.hist(ax=ax)\nplt.show()\n\nC:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_35448\\2624497163.py:12: UserWarning: To output multiple subplots, the figure containing the passed axes is being cleared.\n  df.hist(ax=ax)\n\n\n\n\n\n\n\n2-3 설명변수(독립변수, features, attributes, x) 살펴보기\n\n## AttributeError: 'SubplotSpec' object has no attribute 'is_first_col'\n\n\n## ! pip install --upgrade matplotlib\n\n\n### numerical features(except \"LON\" & \"LAT\") :  LON, LAT: 해당 지역의 경도(Longitudes) 위도(Latitudes) 정보\nnumerical_columns=['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n\n### figsize()는 plot()의 기본 크기를 지정합니다. \nfig = plt.figure(figsize = (16, 20))\nax = fig.gca()  # Axes 생성\n\n### gca()\n### gca()로 현재의 Axes를 할당한다.\n### ax=plt.gica(): 축의 위치를 호출하여 ax로 설정(축 위치 변경을 위해 필요한 과정)\n\ndf[numerical_columns].hist(ax=ax)\nplt.show()\n\nUserWarning: To output multiple subplots, the figure containing the passed axes is being cleared\n  df[numerical_columns].hist(ax=ax)\n\n\n\n\n\n\n\n2-4. 설명변수(x) 와 종속변수(y) 간의 관계 탐색\n\n변수간의 상관관계 파악해 봅니다.\n\n\n### Person 상관계수 : 대표적으로 상관관계 분석시 사용하는 지표입니다.\n### -1 에서 1 사이의 값을 가진다는 특징이 있습니다.\n### 1일 때는 완전 양의 상관(perfect positive correlation), -1일 때는 완전 음의 상관관계(perfect negative correlation)관계를 보입니다.\n### https://m.blog.naver.com/istech7/50153047118\n\ncols = ['전력수요', '원자력', '유연탄', '무연탄', '바이오가스', '부생가스', '소수력', '수력', '양수', '매립가스', '중유', '태양광', '폐기물',\n        '풍력', 'LNG', 'LPG', '연료전지', '경유', '해양에너지', '가스압', '바이오매스', 'IGCC', '바이오중유', 'RPS', '기타' ]\n\ncorr = df[cols].corr(method = 'pearson')\ncorr\n\n\n\n\n\n  \n    \n      \n      전력수요\n      원자력\n      유연탄\n      무연탄\n      바이오가스\n      부생가스\n      소수력\n      수력\n      양수\n      매립가스\n      ...\n      LPG\n      연료전지\n      경유\n      해양에너지\n      가스압\n      바이오매스\n      IGCC\n      바이오중유\n      RPS\n      기타\n    \n  \n  \n    \n      전력수요\n      1.000000\n      0.174751\n      0.652963\n      0.189491\n      0.128547\n      0.436785\n      -0.048442\n      0.300876\n      0.473086\n      -0.063006\n      ...\n      0.357447\n      0.056724\n      0.165617\n      -0.023999\n      NaN\n      -0.028204\n      0.040418\n      0.302660\n      NaN\n      0.071249\n    \n    \n      원자력\n      0.174751\n      1.000000\n      -0.068908\n      -0.331350\n      0.014150\n      -0.041218\n      -0.558679\n      -0.379042\n      0.006063\n      0.173488\n      ...\n      0.464514\n      0.374126\n      -0.065265\n      -0.018044\n      NaN\n      0.065617\n      0.357635\n      0.057229\n      NaN\n      -0.049923\n    \n    \n      유연탄\n      0.652963\n      -0.068908\n      1.000000\n      0.321806\n      0.155439\n      0.112919\n      0.256629\n      0.188037\n      0.117159\n      -0.105392\n      ...\n      0.100317\n      0.200459\n      -0.023079\n      -0.024360\n      NaN\n      -0.001422\n      -0.182270\n      0.403813\n      NaN\n      0.189003\n    \n    \n      무연탄\n      0.189491\n      -0.331350\n      0.321806\n      1.000000\n      0.404113\n      0.167845\n      0.060150\n      0.159281\n      0.033684\n      0.086679\n      ...\n      0.003989\n      -0.384960\n      0.083641\n      -0.025557\n      NaN\n      -0.004082\n      -0.117133\n      -0.151878\n      NaN\n      -0.066913\n    \n    \n      바이오가스\n      0.128547\n      0.014150\n      0.155439\n      0.404113\n      1.000000\n      0.111376\n      -0.179308\n      0.075733\n      0.030059\n      -0.124442\n      ...\n      0.137084\n      -0.309213\n      0.031981\n      0.005401\n      NaN\n      -0.049047\n      -0.179717\n      -0.104911\n      NaN\n      -0.060774\n    \n    \n      부생가스\n      0.436785\n      -0.041218\n      0.112919\n      0.167845\n      0.111376\n      1.000000\n      -0.113504\n      0.232652\n      0.260266\n      -0.085046\n      ...\n      0.158328\n      -0.281481\n      0.092942\n      -0.021733\n      NaN\n      -0.156539\n      0.030290\n      0.046569\n      NaN\n      -0.105509\n    \n    \n      소수력\n      -0.048442\n      -0.558679\n      0.256629\n      0.060150\n      -0.179308\n      -0.113504\n      1.000000\n      0.387411\n      0.011456\n      -0.004284\n      ...\n      -0.533405\n      0.123801\n      -0.013632\n      -0.255623\n      NaN\n      0.230752\n      -0.348979\n      0.303690\n      NaN\n      -0.056167\n    \n    \n      수력\n      0.300876\n      -0.379042\n      0.188037\n      0.159281\n      0.075733\n      0.232652\n      0.387411\n      1.000000\n      0.371284\n      -0.151029\n      ...\n      -0.204712\n      -0.211886\n      0.074191\n      0.000724\n      NaN\n      0.082424\n      -0.331391\n      0.062833\n      NaN\n      0.053906\n    \n    \n      양수\n      0.473086\n      0.006063\n      0.117159\n      0.033684\n      0.030059\n      0.260266\n      0.011456\n      0.371284\n      1.000000\n      -0.023523\n      ...\n      0.054439\n      -0.021590\n      0.201666\n      -0.006593\n      NaN\n      0.021969\n      0.008028\n      0.136868\n      NaN\n      -0.000402\n    \n    \n      매립가스\n      -0.063006\n      0.173488\n      -0.105392\n      0.086679\n      -0.124442\n      -0.085046\n      -0.004284\n      -0.151029\n      -0.023523\n      1.000000\n      ...\n      -0.135477\n      0.303327\n      -0.085124\n      0.049093\n      NaN\n      0.167088\n      0.066110\n      -0.041567\n      NaN\n      -0.119972\n    \n    \n      중유\n      0.416412\n      -0.049303\n      0.273959\n      0.091773\n      0.013707\n      0.094515\n      -0.002301\n      0.037629\n      0.066466\n      -0.055472\n      ...\n      0.292509\n      -0.013728\n      0.155376\n      -0.006472\n      NaN\n      0.064914\n      0.053236\n      -0.024562\n      NaN\n      0.076558\n    \n    \n      태양광\n      0.173617\n      -0.121310\n      -0.045160\n      0.016793\n      0.011713\n      0.387557\n      0.121132\n      0.500851\n      0.143036\n      -0.016571\n      ...\n      -0.123493\n      -0.060539\n      -0.078801\n      -0.076767\n      NaN\n      0.017563\n      -0.088462\n      -0.141975\n      NaN\n      -0.017365\n    \n    \n      폐기물\n      -0.230366\n      -0.220905\n      0.024739\n      -0.187606\n      -0.250489\n      -0.193278\n      0.345977\n      0.074221\n      -0.083607\n      0.011424\n      ...\n      -0.356560\n      0.194064\n      -0.034074\n      0.039082\n      NaN\n      -0.071907\n      -0.408427\n      0.141514\n      NaN\n      0.100823\n    \n    \n      풍력\n      0.096913\n      0.356436\n      -0.093208\n      -0.050836\n      0.007739\n      0.025798\n      -0.363496\n      -0.185716\n      0.017691\n      0.053406\n      ...\n      0.337577\n      0.035373\n      -0.076536\n      -0.031328\n      NaN\n      -0.036755\n      0.291615\n      -0.209720\n      NaN\n      0.033044\n    \n    \n      LNG\n      0.888073\n      0.036543\n      0.313039\n      0.144186\n      0.088653\n      0.478621\n      -0.107138\n      0.289586\n      0.456186\n      -0.074251\n      ...\n      0.332237\n      -0.151000\n      0.251316\n      0.002780\n      NaN\n      -0.081685\n      0.059137\n      0.162766\n      NaN\n      0.000552\n    \n    \n      LPG\n      0.357447\n      0.464514\n      0.100317\n      0.003989\n      0.137084\n      0.158328\n      -0.533405\n      -0.204712\n      0.054439\n      -0.135477\n      ...\n      1.000000\n      -0.218873\n      0.086300\n      0.010474\n      NaN\n      -0.155920\n      0.257420\n      -0.100595\n      NaN\n      0.106544\n    \n    \n      연료전지\n      0.056724\n      0.374126\n      0.200459\n      -0.384960\n      -0.309213\n      -0.281481\n      0.123801\n      -0.211886\n      -0.021590\n      0.303327\n      ...\n      -0.218873\n      1.000000\n      -0.102583\n      -0.008626\n      NaN\n      0.202642\n      0.222508\n      0.333846\n      NaN\n      -0.015694\n    \n    \n      경유\n      0.165617\n      -0.065265\n      -0.023079\n      0.083641\n      0.031981\n      0.092942\n      -0.013632\n      0.074191\n      0.201666\n      -0.085124\n      ...\n      0.086300\n      -0.102583\n      1.000000\n      0.015688\n      NaN\n      -0.014942\n      0.005110\n      0.055267\n      NaN\n      -0.055727\n    \n    \n      해양에너지\n      -0.023999\n      -0.018044\n      -0.024360\n      -0.025557\n      0.005401\n      -0.021733\n      -0.255623\n      0.000724\n      -0.006593\n      0.049093\n      ...\n      0.010474\n      -0.008626\n      0.015688\n      1.000000\n      NaN\n      0.019100\n      -0.008345\n      0.033866\n      NaN\n      -0.008706\n    \n    \n      가스압\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      바이오매스\n      -0.028204\n      0.065617\n      -0.001422\n      -0.004082\n      -0.049047\n      -0.156539\n      0.230752\n      0.082424\n      0.021969\n      0.167088\n      ...\n      -0.155920\n      0.202642\n      -0.014942\n      0.019100\n      NaN\n      1.000000\n      0.047592\n      0.158491\n      NaN\n      -0.073955\n    \n    \n      IGCC\n      0.040418\n      0.357635\n      -0.182270\n      -0.117133\n      -0.179717\n      0.030290\n      -0.348979\n      -0.331391\n      0.008028\n      0.066110\n      ...\n      0.257420\n      0.222508\n      0.005110\n      -0.008345\n      NaN\n      0.047592\n      1.000000\n      -0.001049\n      NaN\n      -0.212192\n    \n    \n      바이오중유\n      0.302660\n      0.057229\n      0.403813\n      -0.151878\n      -0.104911\n      0.046569\n      0.303690\n      0.062833\n      0.136868\n      -0.041567\n      ...\n      -0.100595\n      0.333846\n      0.055267\n      0.033866\n      NaN\n      0.158491\n      -0.001049\n      1.000000\n      NaN\n      -0.158646\n    \n    \n      RPS\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      기타\n      0.071249\n      -0.049923\n      0.189003\n      -0.066913\n      -0.060774\n      -0.105509\n      -0.056167\n      0.053906\n      -0.000402\n      -0.119972\n      ...\n      0.106544\n      -0.015694\n      -0.055727\n      -0.008706\n      NaN\n      -0.073955\n      -0.212192\n      -0.158646\n      NaN\n      1.000000\n    \n  \n\n25 rows × 25 columns\n\n\n\n\n### 상관관계를 직관적으로 살펴보기 위해 Heatmap 으로 돌려봅니다.\n### heatmap (seaborn): 여기서는 seaborn 시각화 라이브러리를 사용해서 표현합니다. \n### 시각화의 대표적인 라이브러리가 matplot(https://matplotlib.org/)과 seaborn(https://seaborn.pydata.org/)이 있습니다.\n\nfig = plt.figure(figsize = (16, 12))\nax = fig.gca()\n\n# https://seaborn.pydata.org/generated/seaborn.heatmap.html\nsns.set(font_scale = 1.5)  # heatmap 안의 font-size 설정 \nheatmap = sns.heatmap(corr.values, annot = True, fmt='.2f', annot_kws={'size':15},\n                      yticklabels = cols, xticklabels = cols, ax=ax, cmap = \"RdYlBu\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n### 해설) \n### 우리의 관심사인 target variable **“CMEDV - 주택 가격”**과 다른 변수간의 상관관계를 살펴보면,\n### “CMEDV - 주택 가격”\n### “RM - 자택당 평균 방 갯수”(0.7) (양의 상관관계)\n### “LSTAT - 빈곤층의 비율”(-0.74) (음의 상관관게)\n## 과 강한 상관관계를 보이고 있다는 것을 알 수 있습니다.\n\n\n2.4.1 CMEDV - 주택 가격~ 방 갯수(“RM”)의 관계\n\n## 폰트 https://whiplash-bd.tistory.com/33\n\nfrom matplotlib import font_manager, rc # 폰트 세팅을 위한 모듈 추가\nfont_path = \"C:/Windows/Fonts/malgun.ttf\" # 사용할 폰트명 경로 삽입\nfont = font_manager.FontProperties(fname = font_path).get_name()\nrc('font', family = font)\n\n\n### scatter plot 산점도, https://seaborn.pydata.org/generated/seaborn.scatterplot.html\nsns.scatterplot(data=df, x='LNG', y='전력수요', markers='o', color='blue', alpha=0.6)\nplt.title('Scatter Plot')\nplt.show()\n\n\n\n\n\n### 해설) \n### 주택 가격이 방 갯수와 양의 상관관계(positive correlation)를 갖고 있습니다. \n### 즉, 방 갯수가 많은 주택들이 상대적으로 더 높은 가격을 갖고 있습니다.\n\n\n\n2.4.2 CMEDV - 빈곤층의 비율(“LSTAT”)의 관계\n\n# scatter plot\nsns.scatterplot(data=df, x='원자력', y='전력수요', markers='o', color='blue', alpha=0.6)\nplt.title('Scatter Plot')\nplt.show()\n\n\n\n\n\n### 해설)\n### 주택 가격이 빈곤층의 비율과 음의 상관관계(negative correlation)를 갖고 있습니다. \n### 즉, 빈곤층의 비율이 높은 지역의 주택 가격이 상대적으로 낮은 경향이 있습니다.\n\n\n\n2.4.3 도시별 차이 탐색\n\n데이터를 살펴보면 여러 지역이 같은 도시에 속한 경우가 있습니다.\n변수 중에서도 도시 단위로 측정되는 변수가 많고요. 따라서 우리는 자연스럽게 도시 간의 차이를 궁금하게 됩니다.\n다시한번 변수의 설명을 열거해 봅니다.\n\n\nTOWN: 소속 도시 이름\nLON, LAT: 해당 지역의 경도(Longitudes) 위도(Latitudes) 정보\nCMEDV: 해당 지역의 주택 가격 (중앙값) (corrected median values of housing)(단위: USD 1000)\nCRIM: 지역 범죄율 (per capita crime)\nZN: 소속 도시에 25,000 제곱 피트(sq.ft) 이상의 주택지 비율\nINDUS: 소속 도시에 상업적 비즈니스에 활용되지 않는 농지 면적\nCHAS: 해당 지역이 Charles 강과 접하고 있는지 여부 (dummy variable)\nNOX: 소속 도시의 산화질소 농도\nRM: 해당 지역의 자택당 평균 방 갯수\nAGE: 해당 지역에 1940년 이전에 건설된 주택의 비율\nDIS: 5개의 보스턴 고용 센터와의 거리에 따른 가중치 부여\nRAD: 소속 도시가 Radial 고속도로와의 접근성 지수\nTAX: 소속 도시의 10000달러당 재산세\nPTRATIO: 소속 도시의 학생-교사 비율\nB: 해당 지역의 흑인 지수 (1000(Bk - 0.63)^2), Bk는 흑인의 비\nLSTAT: 해당 지역의 빈곤층 비율\n\n\n### 각 도시 데이터 갯수 살펴보기 \ndf['TOWN'].value_counts()\n\nCambridge            30\nBoston Savin Hill    23\nLynn                 22\nBoston Roxbury       19\nNewton               18\n                     ..\nMedfield              1\nDover                 1\nLincoln               1\nSherborn              1\nNahant                1\nName: TOWN, Length: 92, dtype: int64\n\n\n\n### 해설) 도시 갯수가 92개 입니다.\n\n\n### 각 도시 데이터 갯수 살펴보기 (bar plot)을 활용해서 시각화를 합니다.\n### 위의 각 도시 데이터 갯수를 x 축으로, 합니다. \n### x 축을 50개로 나누었습니다.\n\ndf['TOWN'].value_counts().hist(bins=50)\n\n<Axes: >\n\n\n\n\n\n\n### 도시별 주택 가격 특징 (boxplot 이용)\nfig = plt.figure(figsize = (12, 20))\nsns.boxplot(x='CMEDV', y='TOWN', data=df)\n\n<Axes: xlabel='CMEDV', ylabel='TOWN'>\n\n\n\n### 해설) Boston 지역(Boston으로 시작하는 도시)의 주택 가격이 전반적으로 다른 지역보다 낮다는 것을 알 수 있습니다.\n\n\n### 도시별 범죄율을 확인해 보겠습니다.\n### 도시별 범죄율 특징\nfig = plt.figure(figsize = (12, 20))\nsns.boxplot(x='CRIM', y='TOWN', data=df)\n\n\n### 해설) Boston 지역의 범죄율이 유독 높다는 것을 확인할 수 있고, 따라서 범죄율이 높은 지역의 주택 가격이 상대적으로 낮다는 것을 추측해볼 수 있겠습니다.\n\n1. Library & Data Import\n2. 데이터 파악 (EDA: 탐색적 데이터 분석)\n    2-1. 데이터셋 기본 정보 파악\n    2-2. 종속 변수(목표 변수) 탐색\n    2-3. 설명 변수 탐색\n    2-4. 설명변수와 종속변수 간의 관계 탐색\n3. 주택 가격 예측 모델링: 회귀 분석\n    3-1. 데이터 전처리\n    3-2. 회귀 모델링\n    3-3. 모델 해석\n    3-4. 모델 예측 결과 및 성능 평가\n\n\n\n\n3. 주택가격 예측 모델링: 회귀분석\n\n이제 변수들을 활용하여 매사추세츠주 각 지역의 주택 가격을 예측하는 회귀 모델을 만들어 보겠습니다.\n\n\n3-1 데이터 전처리\n\n먼저 Feature 들의 scale 차이를 없애기 위해 수치형 Feature에 대해서 표준화를 진행해야 합니다.\n\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      전력수요\n      원자력\n      유연탄\n      무연탄\n      바이오가스\n      부생가스\n      소수력\n      수력\n      양수\n      매립가스\n      ...\n      LPG\n      연료전지\n      경유\n      해양에너지\n      가스압\n      바이오매스\n      IGCC\n      바이오중유\n      RPS\n      기타\n    \n  \n  \n    \n      0\n      64942\n      19801.59614\n      20648.61961\n      275.084752\n      46.411899\n      21.987878\n      52.974962\n      42.006667\n      786.187487\n      20.599213\n      ...\n      39.839744\n      484.648144\n      0.0\n      0.000000\n      0\n      554.207971\n      185.33200\n      101.170048\n      0\n      70.790582\n    \n    \n      1\n      62593\n      19800.37141\n      20388.47312\n      278.730595\n      46.148465\n      22.411095\n      52.292313\n      42.014917\n      239.793783\n      20.721555\n      ...\n      43.216096\n      484.893300\n      0.0\n      0.000000\n      0\n      553.769543\n      235.33104\n      101.179008\n      0\n      71.087030\n    \n    \n      2\n      60905\n      19800.32776\n      20365.38723\n      276.932504\n      46.408393\n      22.537354\n      48.794418\n      42.032154\n      0.048300\n      20.774781\n      ...\n      40.816832\n      484.925545\n      0.0\n      0.000000\n      0\n      554.970371\n      238.34664\n      101.177216\n      0\n      71.040694\n    \n    \n      3\n      59889\n      19798.40984\n      20298.30731\n      275.579426\n      46.313306\n      22.719873\n      44.005635\n      42.015553\n      0.000000\n      20.772397\n      ...\n      37.220624\n      485.075876\n      0.0\n      61.221048\n      0\n      552.863967\n      238.60592\n      101.183936\n      0\n      70.422899\n    \n    \n      4\n      59638\n      19795.97617\n      20289.56329\n      275.375931\n      46.585862\n      22.678411\n      40.841085\n      41.986467\n      0.000000\n      21.046917\n      ...\n      33.687808\n      485.112767\n      0.0\n      146.600496\n      0\n      550.342585\n      239.09256\n      101.180576\n      0\n      71.277898\n    \n  \n\n5 rows × 25 columns\n\n\n\n\n데이터를 살펴보면 여러 지역이 같은 도시에 속한 경우가 있습니다.\n변수 중에서도 도시 단위로 측정되는 변수가 많고요. 따라서 우리는 자연스럽게 도시 간의 차이를 궁금하게 됩니다.\n다시한번 변수의 설명을 열거해 봅니다.\n\nTOWN: 소속 도시 이름\nLON, LAT: 해당 지역의 경도(Longitudes) 위도(Latitudes) 정보\nCMEDV: 해당 지역의 주택 가격 (중앙값) (corrected median values of housing)(단위: USD 1000)\nCRIM: 지역 범죄율 (per capita crime)\nZN: 소속 도시에 25,000 제곱 피트(sq.ft) 이상의 주택지 비율\nINDUS: 소속 도시에 상업적 비즈니스에 활용되지 않는 농지 면적\nCHAS: 해당 지역이 Charles 강과 접하고 있는지 여부 (dummy variable)\nNOX: 소속 도시의 산화질소 농도\nRM: 해당 지역의 자택당 평균 방 갯수\nAGE: 해당 지역에 1940년 이전에 건설된 주택의 비율\nDIS: 5개의 보스턴 고용 센터와의 거리에 따른 가중치 부여\nRAD: 소속 도시가 Radial 고속도로와의 접근성 지수\nTAX: 소속 도시의 10000달러당 재산세\nPTRATIO: 소속 도시의 학생-교사 비율\nB: 해당 지역의 흑인 지수 (1000(Bk - 0.63)^2), Bk는 흑인의 비\nLSTAT: 해당 지역의 빈곤층 비율\n\n\n\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 8760 entries, 0 to 8759\nData columns (total 25 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   전력수요    8760 non-null   int64  \n 1   원자력     8760 non-null   float64\n 2   유연탄     8760 non-null   float64\n 3   무연탄     8760 non-null   float64\n 4   바이오가스   8760 non-null   float64\n 5   부생가스    8760 non-null   float64\n 6   소수력     8760 non-null   float64\n 7   수력      8760 non-null   float64\n 8   양수      8760 non-null   float64\n 9   매립가스    8760 non-null   float64\n 10  중유      8760 non-null   float64\n 11  태양광     8760 non-null   float64\n 12  폐기물     8760 non-null   float64\n 13  풍력      8760 non-null   float64\n 14  LNG     8760 non-null   float64\n 15  LPG     8760 non-null   float64\n 16  연료전지    8760 non-null   float64\n 17  경유      8760 non-null   float64\n 18  해양에너지   8760 non-null   float64\n 19  가스압     8760 non-null   int64  \n 20  바이오매스   8760 non-null   float64\n 21  IGCC    8760 non-null   float64\n 22  바이오중유   8760 non-null   float64\n 23  RPS     8760 non-null   int64  \n 24  기타      8760 non-null   float64\ndtypes: float64(22), int64(3)\nmemory usage: 1.7 MB\n\n\n\n### 문자형 변수인 \"TOWN\"와 범주형 변수인 “CHAS” (Dummy variable), 위도(LON), 경도(LAT)를 제외하고 모든 수치형 변수에 대해서 표준화를 진행합니다.\n### 참고) CHAS: 해당 지역이 Charles 강과 접하고 있는지 여부 (dummy variable)\n### 사이킷런은 파이썬에서 머신러닝 분석을 할 때 유용하게 사용할 수 있는 라이브러리입니다. 여러가지 머신러닝 모듈로 구성되어있습니다.\n\nfrom sklearn.preprocessing import StandardScaler\n\n# feature standardization  (numerical_columns except dummy var.-\"CHAS\")\n\nscaler = StandardScaler()  # 평균 0, 표준편차 1\n## numerical_columns=['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']\nscale_columns = ['전력수요', '원자력', '유연탄', '무연탄', '바이오가스', '부생가스', '소수력', '수력', '양수', '매립가스', '중유', '태양광', '폐기물',\n        '풍력', 'LNG', 'LPG', '연료전지', '경유', '해양에너지',  '바이오매스', 'IGCC', '바이오중유', '기타']\ndf[scale_columns] = scaler.fit_transform(df[scale_columns])\n\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      전력수요\n      원자력\n      유연탄\n      무연탄\n      바이오가스\n      부생가스\n      소수력\n      수력\n      양수\n      매립가스\n      ...\n      LPG\n      연료전지\n      경유\n      해양에너지\n      가스압\n      바이오매스\n      IGCC\n      바이오중유\n      RPS\n      기타\n    \n  \n  \n    \n      0\n      0.131574\n      1.533286\n      -0.187856\n      0.766991\n      0.520811\n      -0.805093\n      -0.791950\n      -1.409010\n      0.693333\n      -0.043777\n      ...\n      2.906348\n      -1.256522\n      -0.388147\n      -0.691777\n      0\n      0.272315\n      -0.055644\n      -1.105328\n      0\n      0.528951\n    \n    \n      1\n      -0.124968\n      1.532571\n      -0.255639\n      0.799543\n      0.496631\n      -0.801767\n      -0.833962\n      -1.408960\n      -0.339551\n      -0.029497\n      ...\n      3.194672\n      -1.250060\n      -0.388147\n      -0.691777\n      0\n      0.267874\n      0.394029\n      -1.105110\n      0\n      0.558226\n    \n    \n      2\n      -0.309320\n      1.532546\n      -0.261654\n      0.783489\n      0.520489\n      -0.800775\n      -1.049230\n      -1.408856\n      -0.792758\n      -0.023284\n      ...\n      2.989786\n      -1.249210\n      -0.388147\n      -0.691777\n      0\n      0.280036\n      0.421150\n      -1.105153\n      0\n      0.553650\n    \n    \n      3\n      -0.420281\n      1.531426\n      -0.279132\n      0.771408\n      0.511762\n      -0.799340\n      -1.343942\n      -1.408956\n      -0.792849\n      -0.023562\n      ...\n      2.682687\n      -1.245248\n      -0.388147\n      0.123660\n      0\n      0.258703\n      0.423482\n      -1.104990\n      0\n      0.492640\n    \n    \n      4\n      -0.447694\n      1.530005\n      -0.281410\n      0.769591\n      0.536779\n      -0.799666\n      -1.538695\n      -1.409131\n      -0.792849\n      0.008482\n      ...\n      2.381001\n      -1.244275\n      -0.388147\n      1.260876\n      0\n      0.233166\n      0.427858\n      -1.105072\n      0\n      0.577075\n    \n  \n\n5 rows × 25 columns\n\n\n\n\ndf[scale_columns].head()\n\n\n\n\n\n  \n    \n      \n      전력수요\n      원자력\n      유연탄\n      무연탄\n      바이오가스\n      부생가스\n      소수력\n      수력\n      양수\n      매립가스\n      ...\n      풍력\n      LNG\n      LPG\n      연료전지\n      경유\n      해양에너지\n      바이오매스\n      IGCC\n      바이오중유\n      기타\n    \n  \n  \n    \n      0\n      0.131574\n      1.533286\n      -0.187856\n      0.766991\n      0.520811\n      -0.805093\n      -0.791950\n      -1.409010\n      0.693333\n      -0.043777\n      ...\n      1.199196\n      -0.125749\n      2.906348\n      -1.256522\n      -0.388147\n      -0.691777\n      0.272315\n      -0.055644\n      -1.105328\n      0.528951\n    \n    \n      1\n      -0.124968\n      1.532571\n      -0.255639\n      0.799543\n      0.496631\n      -0.801767\n      -0.833962\n      -1.408960\n      -0.339551\n      -0.029497\n      ...\n      1.133030\n      -0.396063\n      3.194672\n      -1.250060\n      -0.388147\n      -0.691777\n      0.267874\n      0.394029\n      -1.105110\n      0.558226\n    \n    \n      2\n      -0.309320\n      1.532546\n      -0.261654\n      0.783489\n      0.520489\n      -0.800775\n      -1.049230\n      -1.408856\n      -0.792758\n      -0.023284\n      ...\n      1.163823\n      -0.647912\n      2.989786\n      -1.249210\n      -0.388147\n      -0.691777\n      0.280036\n      0.421150\n      -1.105153\n      0.553650\n    \n    \n      3\n      -0.420281\n      1.531426\n      -0.279132\n      0.771408\n      0.511762\n      -0.799340\n      -1.343942\n      -1.408956\n      -0.792849\n      -0.023562\n      ...\n      1.051797\n      -0.822836\n      2.682687\n      -1.245248\n      -0.388147\n      0.123660\n      0.258703\n      0.423482\n      -1.104990\n      0.492640\n    \n    \n      4\n      -0.447694\n      1.530005\n      -0.281410\n      0.769591\n      0.536779\n      -0.799666\n      -1.538695\n      -1.409131\n      -0.792849\n      0.008482\n      ...\n      0.868320\n      -0.870349\n      2.381001\n      -1.244275\n      -0.388147\n      1.260876\n      0.233166\n      0.427858\n      -1.105072\n      0.577075\n    \n  \n\n5 rows × 23 columns\n\n\n\n\ntraining/test set 나누기\n\n나중에 도출될 예측 모델의 예측 성능을 평가하기 위해, 먼저 전체 데이터셋을 “Training set”과 “Test set”으로 나누겠습니다.\nTraining set에서 모델을 학습하고 Test set에서 모델의 예측 성능을 검증할 겁니다.\n\n\n\n### features for linear regression model\n### 참고) CHAS: 해당 지역이 Charles 강과 접하고 있는지 여부 (dummy variable)\n### numerical features(except \"LON\" & \"LAT\") :  LON, LAT: 해당 지역의 경도(Longitudes) 위도(Latitudes) 정보\nnumerical_columns = ['원자력', '유연탄', '무연탄', '바이오가스', '부생가스', '소수력', '수력', '양수', '매립가스', '중유', '태양광', '폐기물',\n        '풍력', 'LNG', 'LPG', '연료전지', '경유', '해양에너지',  '바이오매스', 'IGCC', '바이오중유',  '기타' ]\ndf[numerical_columns].head()\n\n\n\n\n\n  \n    \n      \n      원자력\n      유연탄\n      무연탄\n      바이오가스\n      부생가스\n      소수력\n      수력\n      양수\n      매립가스\n      중유\n      ...\n      풍력\n      LNG\n      LPG\n      연료전지\n      경유\n      해양에너지\n      바이오매스\n      IGCC\n      바이오중유\n      기타\n    \n  \n  \n    \n      0\n      1.533286\n      -0.187856\n      0.766991\n      0.520811\n      -0.805093\n      -0.791950\n      -1.409010\n      0.693333\n      -0.043777\n      -0.408537\n      ...\n      1.199196\n      -0.125749\n      2.906348\n      -1.256522\n      -0.388147\n      -0.691777\n      0.272315\n      -0.055644\n      -1.105328\n      0.528951\n    \n    \n      1\n      1.532571\n      -0.255639\n      0.799543\n      0.496631\n      -0.801767\n      -0.833962\n      -1.408960\n      -0.339551\n      -0.029497\n      -0.406187\n      ...\n      1.133030\n      -0.396063\n      3.194672\n      -1.250060\n      -0.388147\n      -0.691777\n      0.267874\n      0.394029\n      -1.105110\n      0.558226\n    \n    \n      2\n      1.532546\n      -0.261654\n      0.783489\n      0.520489\n      -0.800775\n      -1.049230\n      -1.408856\n      -0.792758\n      -0.023284\n      -0.404731\n      ...\n      1.163823\n      -0.647912\n      2.989786\n      -1.249210\n      -0.388147\n      -0.691777\n      0.280036\n      0.421150\n      -1.105153\n      0.553650\n    \n    \n      3\n      1.531426\n      -0.279132\n      0.771408\n      0.511762\n      -0.799340\n      -1.343942\n      -1.408956\n      -0.792849\n      -0.023562\n      -0.406326\n      ...\n      1.051797\n      -0.822836\n      2.682687\n      -1.245248\n      -0.388147\n      0.123660\n      0.258703\n      0.423482\n      -1.104990\n      0.492640\n    \n    \n      4\n      1.530005\n      -0.281410\n      0.769591\n      0.536779\n      -0.799666\n      -1.538695\n      -1.409131\n      -0.792849\n      0.008482\n      -0.403995\n      ...\n      0.868320\n      -0.870349\n      2.381001\n      -1.244275\n      -0.388147\n      1.260876\n      0.233166\n      0.427858\n      -1.105072\n      0.577075\n    \n  \n\n5 rows × 22 columns\n\n\n\n\nfrom sklearn.model_selection import train_test_split\n\n# split dataset into training & test\nX = df[numerical_columns]\ny = df['전력수요']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n\n\nX_train.shape, y_train.shape\n\n((7008, 24), (7008,))\n\n\n\nX_test.shape, y_test.shape\n\n((1752, 22), (1752,))\n\n\n\ny_train\n### 전력수요 값을 의미합니다.\n\n54     -0.530805\n5748   -0.382493\n8382    0.301837\n4228   -1.142727\n7437   -0.615008\n          ...   \n2895   -1.382996\n7813    0.902073\n905    -0.315655\n5192    0.719250\n235     0.674473\nName: 전력수요, Length: 7008, dtype: float64\n\n\n\nX_train\n\n\n\n\n\n  \n    \n      \n      원자력\n      유연탄\n      무연탄\n      바이오가스\n      부생가스\n      소수력\n      수력\n      양수\n      매립가스\n      중유\n      ...\n      LPG\n      연료전지\n      경유\n      해양에너지\n      가스압\n      바이오매스\n      IGCC\n      바이오중유\n      RPS\n      기타\n    \n  \n  \n    \n      54\n      1.529375\n      -0.928721\n      0.805475\n      0.599234\n      -0.808364\n      -1.995769\n      -1.414636\n      -0.672495\n      0.022800\n      -0.412604\n      ...\n      2.643301\n      -1.422684\n      1.027149\n      1.558248\n      0\n      0.123250\n      0.418269\n      -1.085906\n      0\n      0.445297\n    \n    \n      5748\n      -0.073489\n      0.368325\n      0.759125\n      -1.592599\n      1.297155\n      1.939306\n      0.289497\n      -0.792649\n      -0.015176\n      -0.697063\n      ...\n      -0.495782\n      0.719577\n      -0.388147\n      -0.691777\n      0\n      0.587450\n      0.685356\n      1.409862\n      0\n      -2.490523\n    \n    \n      8382\n      2.249333\n      0.721306\n      -1.689075\n      0.639275\n      -0.507397\n      -0.631479\n      -1.401082\n      -0.792687\n      0.147637\n      -0.476603\n      ...\n      0.487732\n      1.840264\n      -0.388147\n      -0.691777\n      0\n      -0.254562\n      0.521073\n      0.957480\n      0\n      -0.181800\n    \n    \n      4228\n      -1.214517\n      0.028157\n      -0.455666\n      0.691835\n      -0.594045\n      -0.179975\n      -0.102232\n      -0.792849\n      -2.200085\n      -0.737480\n      ...\n      -0.495782\n      -0.753360\n      -0.388147\n      2.626899\n      0\n      -1.103016\n      -1.722452\n      -0.286802\n      0\n      0.784771\n    \n    \n      7437\n      0.537027\n      -1.235642\n      -0.471834\n      0.662118\n      -0.426955\n      -0.259979\n      -1.217471\n      -0.262759\n      1.248346\n      -0.581955\n      ...\n      -0.495782\n      0.805896\n      -0.388147\n      -0.536074\n      0\n      -1.024238\n      0.397927\n      -1.279522\n      0\n      -0.011921\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2895\n      -1.202214\n      -1.253771\n      -0.473847\n      0.761325\n      -0.570862\n      0.306989\n      1.618863\n      -0.792849\n      0.218146\n      -0.749983\n      ...\n      -0.495782\n      -0.107035\n      -0.388147\n      -0.691777\n      0\n      1.230738\n      0.280155\n      -1.945167\n      0\n      0.699638\n    \n    \n      7813\n      0.158908\n      0.587843\n      1.255900\n      0.505883\n      -0.058851\n      -0.501427\n      -0.650621\n      0.405314\n      0.883570\n      2.215578\n      ...\n      0.521293\n      1.365267\n      -0.388147\n      -0.691450\n      0\n      -1.174935\n      0.563802\n      -0.999401\n      0\n      0.080016\n    \n    \n      905\n      0.232520\n      0.160219\n      0.777619\n      0.910048\n      -0.525019\n      -1.515832\n      0.295975\n      -0.762081\n      0.305957\n      -0.454695\n      ...\n      0.649809\n      -1.086574\n      -0.388147\n      -0.691777\n      0\n      -0.402271\n      0.673853\n      -1.099200\n      0\n      0.774011\n    \n    \n      5192\n      -0.664592\n      1.245532\n      0.730008\n      0.669742\n      0.765423\n      1.482855\n      0.635872\n      -0.589718\n      0.488304\n      -0.758893\n      ...\n      -0.495782\n      0.513958\n      -0.388147\n      -0.691777\n      0\n      -0.053258\n      -0.013122\n      0.912406\n      0\n      0.062048\n    \n    \n      235\n      0.369275\n      -0.411297\n      0.772094\n      0.343111\n      -0.498102\n      -1.012083\n      0.049939\n      2.068376\n      -1.479749\n      -0.421272\n      ...\n      4.031078\n      -1.692856\n      -0.388147\n      -0.691777\n      0\n      -0.856501\n      0.422812\n      -1.098112\n      0\n      0.433936\n    \n  \n\n7008 rows × 24 columns\n\n\n\n\n다중공산성\n\n회귀 분석에서 하나의 feature(예측 변수)가 다른 feature와의 상관 관계가 높으면 (즉, 다중공선성이 존재하면), 회귀 분석 시 부정적인 영향을 미칠 수 있기 때문에, 모델링 하기 전에 먼저 다중공선성의 존재 여부를 확인해야합니다.\n보통 다중공선성을 판단할 때 VIF값을 확인합니다. 일반적으로, VIF > 10인 feature들은 다른 변수와의 상관관계가 높아, 다중공선성이 존재하는 것으로 판단합니다.\n즉, VIF > 10인 feature들은 설명변수에서 제거하는 것이 좋을 수도 있습니다.\n\n\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\nvif['features'] = X_train.columns\nvif[\"VIF Factor\"] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])] ## X_train.shape[1]=13\nvif.round(1) \n### 소수점 첫째자리까지 표시합니다. 즉, 소수점 둘째짜리에서 반올림 합니다.\n\n\n\n\n\n  \n    \n      \n      features\n      VIF Factor\n    \n  \n  \n    \n      0\n      원자력\n      3.3\n    \n    \n      1\n      유연탄\n      2.8\n    \n    \n      2\n      무연탄\n      2.3\n    \n    \n      3\n      바이오가스\n      1.5\n    \n    \n      4\n      부생가스\n      1.7\n    \n    \n      5\n      소수력\n      3.4\n    \n    \n      6\n      수력\n      2.4\n    \n    \n      7\n      양수\n      1.5\n    \n    \n      8\n      매립가스\n      1.4\n    \n    \n      9\n      중유\n      1.5\n    \n    \n      10\n      태양광\n      1.9\n    \n    \n      11\n      폐기물\n      1.7\n    \n    \n      12\n      풍력\n      1.4\n    \n    \n      13\n      LNG\n      2.3\n    \n    \n      14\n      LPG\n      2.5\n    \n    \n      15\n      연료전지\n      3.2\n    \n    \n      16\n      경유\n      1.2\n    \n    \n      17\n      해양에너지\n      1.3\n    \n    \n      18\n      바이오매스\n      1.3\n    \n    \n      19\n      IGCC\n      1.9\n    \n    \n      20\n      바이오중유\n      1.9\n    \n    \n      21\n      기타\n      1.3\n    \n  \n\n\n\n\n\n### 해설) VIF값을 확인해보면, 모든 변수의 VIF값이 다 10 이하입니다. 따라서 다중공선성 문제가 존재하지 않아 모든 feature을 활용하여 회귀 모델링을 진행하면 됩니다.\n\n\n\n3-2 회귀모델링\n\n먼저 Training set에서 선형 회귀 예측 모델을 학습합니다.\n그 다음 도출된 모델을 Test set에 적용해 주택 가격(“CMEDV”)을 예측합니다.\n이 결과는 실제 “CMEDV” 값과 비교하여 모델의 예측 성능을 평가하는 데 활용하게 됩니다.\n\n\nfrom sklearn import linear_model\n\n# fit regression model in training set\nlr = linear_model.LinearRegression()\nmodel = lr.fit(X_train, y_train)\n\n# predict in test set\npred_test = lr.predict(X_test)\n\n\n\n3-3 모델해석하기\n\n각 feature, 설명변수, x, 컬럼에 대한 회귀계수를 확인해 보겠습니다.\n\n\n### print coef \n### 계수를 출력합니다.\nprint(lr.coef_)\n\n[ 0.19027638  0.43163873  0.0147826  -0.0038524   0.01652054  0.00823441\n  0.02935437  0.0875245   0.00211112  0.02383963  0.08366638  0.00479943\n  0.02795186  0.66184733  0.00227071  0.01042959  0.00539486 -0.00123438\n  0.00481722  0.02122935  0.00883865  0.00568356]\n\n\n\n### \"feature - coefficients\" DataFrame 만들기\n### zip 키워드는 리스트 2개를 하나로 묶습니다.(결과는 튜플로 됩니다.)\n\ncoefs = pd.DataFrame(zip(df[numerical_columns].columns, lr.coef_), columns = ['feature', 'coefficients'])\ncoefs.round(3)\n\n### from sklearn.model_selection import train_test_split\n\n### split dataset into training & test\n# X = df[numerical_columns]\n# y = df['CMEDV']\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n\n\n\n\n\n  \n    \n      \n      feature\n      coefficients\n    \n  \n  \n    \n      0\n      원자력\n      0.190\n    \n    \n      1\n      유연탄\n      0.432\n    \n    \n      2\n      무연탄\n      0.015\n    \n    \n      3\n      바이오가스\n      -0.004\n    \n    \n      4\n      부생가스\n      0.017\n    \n    \n      5\n      소수력\n      0.008\n    \n    \n      6\n      수력\n      0.029\n    \n    \n      7\n      양수\n      0.088\n    \n    \n      8\n      매립가스\n      0.002\n    \n    \n      9\n      중유\n      0.024\n    \n    \n      10\n      태양광\n      0.084\n    \n    \n      11\n      폐기물\n      0.005\n    \n    \n      12\n      풍력\n      0.028\n    \n    \n      13\n      LNG\n      0.662\n    \n    \n      14\n      LPG\n      0.002\n    \n    \n      15\n      연료전지\n      0.010\n    \n    \n      16\n      경유\n      0.005\n    \n    \n      17\n      해양에너지\n      -0.001\n    \n    \n      18\n      바이오매스\n      0.005\n    \n    \n      19\n      IGCC\n      0.021\n    \n    \n      20\n      바이오중유\n      0.009\n    \n    \n      21\n      기타\n      0.006\n    \n  \n\n\n\n\n\n### 크기 순서대로 나열합니다. \n### 크기 순서로 나열 : 내림차순으로 합니다. 다만 양, 음을 가리지 않습니다. 절대값을 기준으로 합니다. \n### 절대값 기준 함수: coefficients.abs().sort_values \n\ncoefs_new = coefs.reindex(coefs.coefficients.abs().sort_values(ascending=False).index)\ncoefs_new.round(2)\n\n\n\n\n\n  \n    \n      \n      feature\n      coefficients\n    \n  \n  \n    \n      13\n      LNG\n      0.66\n    \n    \n      1\n      유연탄\n      0.43\n    \n    \n      0\n      원자력\n      0.19\n    \n    \n      7\n      양수\n      0.09\n    \n    \n      10\n      태양광\n      0.08\n    \n    \n      6\n      수력\n      0.03\n    \n    \n      12\n      풍력\n      0.03\n    \n    \n      9\n      중유\n      0.02\n    \n    \n      19\n      IGCC\n      0.02\n    \n    \n      4\n      부생가스\n      0.02\n    \n    \n      2\n      무연탄\n      0.01\n    \n    \n      15\n      연료전지\n      0.01\n    \n    \n      20\n      바이오중유\n      0.01\n    \n    \n      5\n      소수력\n      0.01\n    \n    \n      21\n      기타\n      0.01\n    \n    \n      16\n      경유\n      0.01\n    \n    \n      18\n      바이오매스\n      0.00\n    \n    \n      11\n      폐기물\n      0.00\n    \n    \n      3\n      바이오가스\n      -0.00\n    \n    \n      14\n      LPG\n      0.00\n    \n    \n      8\n      매립가스\n      0.00\n    \n    \n      17\n      해양에너지\n      -0.00\n    \n  \n\n\n\n\n1. TOWN: 소속 도시 이름\n2. LON, LAT: 해당 지역의 경도(Longitudes) 위도(Latitudes) 정보\n3. CMEDV: 해당 지역의 주택 가격 (중앙값) (corrected median values of housing)(단위: USD 1000)\n4. CRIM: 지역 범죄율 (per capita crime)\n5. ZN: 소속 도시에 25,000 제곱 피트(sq.ft) 이상의 주택지 비율\n6. INDUS: 소속 도시에 상업적 비즈니스에 활용되지 않는 농지 면적\n7. CHAS: 해당 지역이 Charles 강과 접하고 있는지 여부 (dummy variable)\n8. NOX: 소속 도시의 산화질소 농도\n9. RM: 해당 지역의 자택당 평균 방 갯수\n10. AGE: 해당 지역에 1940년 이전에 건설된 주택의 비율\n11. DIS: 5개의 보스턴 고용 센터와의 거리에 따른 가중치 부여\n12. RAD: 소속 도시가 Radial 고속도로와의 접근성 지수\n13. TAX: 소속 도시의 10000달러당 재산세\n14. PTRATIO: 소속 도시의 학생-교사 비율\n15. B: 해당 지역의 흑인 지수 (1000(Bk - 0.63)^2), Bk는 흑인의 비\n16. LSTAT: 해당 지역의 빈곤층 비율\n\n### coefficients 를 시각화 합니다. \n\n### figure size\nplt.figure(figsize = (8, 8))\n\n### bar plot : matplotlib.pyplot 모듈의 barh() 함수를 사용해서 수평 막대 그래프를 그릴 수 있습니다. \nplt.barh(coefs_new['feature'], coefs_new['coefficients'])\nplt.title('\"feature - coefficient\" Graph')\nplt.xlabel('coefficients')\nplt.ylabel('features')\nplt.show()\n\n\n\n\n\n유의성 검정을 합니다 (통계에서 변수가 유의하다는 의미입니다).\nX=sm.add_constant(X)\nmodeling - model = sm.OLS(y,X) - result = model.fit() - print(results.summary())\n\n\n## https://stackoverflow.com/questions/68463220/pandas-importing-error-importerror-cannot-import-name-dtypearg-from-pandas\n\n\nimport statsmodels.api as sm\n\nX_train2 = sm.add_constant(X_train)\n### 회귀분석모형 수식을 간단하게 만들기 위해 다음과 같이 상수항을 독립변수 데이터에 추가하는 것을 상수항 결합(bias augmentation)작업이라고 합니다.\n\n### ordinary least square 의 약자로, 거리의 최소값을 기준으로 구하는 함수입니다. \n### 상수항이 추간된 독립변수와 그에 대한 y 값으로 학습을 합니다. \nmodel2 = sm.OLS(y_train, X_train2).fit()\nmodel2.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:          전력수요         R-squared:              0.994 \n\n\n  Model:                   OLS         Adj. R-squared:         0.994 \n\n\n  Method:             Least Squares    F-statistic:         5.494e+04\n\n\n  Date:             Tue, 19 Sep 2023   Prob (F-statistic):     0.00  \n\n\n  Time:                 23:17:01       Log-Likelihood:        8206.1 \n\n\n  No. Observations:        7008        AIC:                -1.637e+04\n\n\n  Df Residuals:            6985        BIC:                -1.621e+04\n\n\n  Df Model:                  22                                      \n\n\n  Covariance Type:      nonrobust                                    \n\n\n\n\n           coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  const    -0.0004     0.001    -0.457  0.648    -0.002     0.001\n\n\n  원자력       0.1903     0.002   116.506  0.000     0.187     0.193\n\n\n  유연탄       0.4316     0.002   286.644  0.000     0.429     0.435\n\n\n  무연탄       0.0148     0.001    10.875  0.000     0.012     0.017\n\n\n  바이오가스    -0.0039     0.001    -3.489  0.000    -0.006    -0.002\n\n\n  부생가스      0.0165     0.001    13.891  0.000     0.014     0.019\n\n\n  소수력       0.0082     0.002     4.977  0.000     0.005     0.011\n\n\n  수력        0.0294     0.001    21.367  0.000     0.027     0.032\n\n\n  양수        0.0875     0.001    80.917  0.000     0.085     0.090\n\n\n  매립가스      0.0021     0.001     1.964  0.050  4.26e-06     0.004\n\n\n  중유        0.0238     0.001    21.488  0.000     0.022     0.026\n\n\n  태양광       0.0837     0.001    67.585  0.000     0.081     0.086\n\n\n  폐기물       0.0048     0.001     4.068  0.000     0.002     0.007\n\n\n  풍력        0.0280     0.001    26.401  0.000     0.026     0.030\n\n\n  LNG       0.6618     0.001   487.027  0.000     0.659     0.665\n\n\n  LPG       0.0023     0.001     1.596  0.110    -0.001     0.005\n\n\n  연료전지      0.0104     0.002     6.496  0.000     0.007     0.014\n\n\n  경유        0.0054     0.001     5.513  0.000     0.003     0.007\n\n\n  해양에너지    -0.0012     0.001    -1.208  0.227    -0.003     0.001\n\n\n  바이오매스     0.0048     0.001     4.683  0.000     0.003     0.007\n\n\n  IGCC      0.0212     0.001    17.195  0.000     0.019     0.024\n\n\n  바이오중유     0.0088     0.001     7.075  0.000     0.006     0.011\n\n\n  기타        0.0057     0.001     5.482  0.000     0.004     0.008\n\n\n\n\n  Omnibus:       154.923   Durbin-Watson:         2.014\n\n\n  Prob(Omnibus):  0.000    Jarque-Bera (JB):    161.740\n\n\n  Skew:          -0.361    Prob(JB):           7.56e-36\n\n\n  Kurtosis:       2.817    Cond. No.               4.69\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n##!pip install pandas==1.3.0\n##!pip install --force-reinstall pandas\n\n\n## https://stackoverflow.com/questions/71106940/cannot-import-name-centered-from-scipy-signal-signaltools\n\n\n##!pip install statsmodels --upgrade\n\n\nimport statsmodels.api as sm\n## import scipy.signal.signaltools as sm\n\nX_train2 = sm.add_constant(X_train)\n### 회귀분석모형 수식을 간단하게 만들기 위해 다음과 같이 상수항을 독립변수 데이터에 추가하는 것을 상수항 결합(bias augmentation)작업이라고 합니다.\n\n### ordinary least square 의 약자로, 거리의 최소값을 기준으로 구하는 함수입니다. \n### 상수항이 추간된 독립변수와 그에 대한 y 값으로 학습을 합니다. \nmodel2 = sm.OLS(y_train, X_train2).fit()\nmodel2.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:          전력수요         R-squared:              0.994 \n\n\n  Model:                   OLS         Adj. R-squared:         0.994 \n\n\n  Method:             Least Squares    F-statistic:         5.494e+04\n\n\n  Date:             Tue, 19 Sep 2023   Prob (F-statistic):     0.00  \n\n\n  Time:                 23:17:06       Log-Likelihood:        8206.1 \n\n\n  No. Observations:        7008        AIC:                -1.637e+04\n\n\n  Df Residuals:            6985        BIC:                -1.621e+04\n\n\n  Df Model:                  22                                      \n\n\n  Covariance Type:      nonrobust                                    \n\n\n\n\n           coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  const    -0.0004     0.001    -0.457  0.648    -0.002     0.001\n\n\n  원자력       0.1903     0.002   116.506  0.000     0.187     0.193\n\n\n  유연탄       0.4316     0.002   286.644  0.000     0.429     0.435\n\n\n  무연탄       0.0148     0.001    10.875  0.000     0.012     0.017\n\n\n  바이오가스    -0.0039     0.001    -3.489  0.000    -0.006    -0.002\n\n\n  부생가스      0.0165     0.001    13.891  0.000     0.014     0.019\n\n\n  소수력       0.0082     0.002     4.977  0.000     0.005     0.011\n\n\n  수력        0.0294     0.001    21.367  0.000     0.027     0.032\n\n\n  양수        0.0875     0.001    80.917  0.000     0.085     0.090\n\n\n  매립가스      0.0021     0.001     1.964  0.050  4.26e-06     0.004\n\n\n  중유        0.0238     0.001    21.488  0.000     0.022     0.026\n\n\n  태양광       0.0837     0.001    67.585  0.000     0.081     0.086\n\n\n  폐기물       0.0048     0.001     4.068  0.000     0.002     0.007\n\n\n  풍력        0.0280     0.001    26.401  0.000     0.026     0.030\n\n\n  LNG       0.6618     0.001   487.027  0.000     0.659     0.665\n\n\n  LPG       0.0023     0.001     1.596  0.110    -0.001     0.005\n\n\n  연료전지      0.0104     0.002     6.496  0.000     0.007     0.014\n\n\n  경유        0.0054     0.001     5.513  0.000     0.003     0.007\n\n\n  해양에너지    -0.0012     0.001    -1.208  0.227    -0.003     0.001\n\n\n  바이오매스     0.0048     0.001     4.683  0.000     0.003     0.007\n\n\n  IGCC      0.0212     0.001    17.195  0.000     0.019     0.024\n\n\n  바이오중유     0.0088     0.001     7.075  0.000     0.006     0.011\n\n\n  기타        0.0057     0.001     5.482  0.000     0.004     0.008\n\n\n\n\n  Omnibus:       154.923   Durbin-Watson:         2.014\n\n\n  Prob(Omnibus):  0.000    Jarque-Bera (JB):    161.740\n\n\n  Skew:          -0.361    Prob(JB):           7.56e-36\n\n\n  Kurtosis:       2.817    Cond. No.               4.69\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n### 해설) coeff를 봐야 합니다.\n### P>|t|를 봐야 합니다. p-value를 의미한다. 0.05 보다 작아야 유의하다!! \n### R-squre 를 봐야 합니다. 설명력을 의미합니다.\n\n\n|기억하기|\n\n결정 계수 (coefficient of determination)는 추정한 선형 모형이 주어진 자료에 적합한 정도를 재는 척도이다.\n\n종속 변수의 변동량 중에서 적용한 모형으로 설명 가능한 부분의 비율을 가리킨다.\n결정계수의 통상적인 기호는 R²이다.\n일반적으로 모형의 설명력으로 해석되지만 모형에 설명 변수가 들어갈수록 증가하기 때문에 해석에 주의해야 한다.\n결정계수의 값은 0에서 1 사이에 있으며, 종속 변인과 독립변인 사이에 상관관계가 높을수록 1에 가까워진다.\n즉, 결정계수가 0에 가까운 값을 가지는 회귀모형은 유용성이 낮은 반면, 결정계수의 값이 클수록 회귀모형의 유용성이 높다고 할 수 있다.\n\n\n\n\n| 해석하기 |\n\n“INDUS”(상업적 비즈니스에 활용되지 않는 농지 면적: p-value: 0.747)과 “AGE”(1940년 이전에 건설된 비율: p-value:0.771)은 유의하지 않습니다. (p value > 0.05)\n주택 가격에 Positive(coefficient > 0)한 영향을 미칩니다.\n\n\n“ZN”(25,000 제곱 피트(sq.ft) 이상의 주택지 비율),\n“CHAS”(Charles 강과 접하고 있는지 여부),\n“RM”(자택당 평균 방 갯수),\n“RAD”(소속 도시가 Radial 고속도로와의 접근성 지수),\n\n“B”(흑인 지수)는\n\n다른 변수의 값이 고정했을 때, 해당 변수의 값이 클수록 주택의 가격이 높을 것입니다.\n\n\n\n\n주택 가격에 Negative(coefficient < 0)한 영향을 미칩니다.\n\n\n“CRIM”(지역 범죄율),\n“NOX”(산화질소 농도),\n“DIS”(보스턴 고용 센터와의 거리),\n“TAX”(재산세),\n“PTRATIO”(학생-교사 비율),\n“LSTAT”(빈곤층 비율)은 :다른 변수의 값이 고정했을 때, 해당 변수의 값이 작을수록 주택의 가격이 높을 것입니다.\n\n\n\n\n3-4 모델 에측 결과 및 성능 평가\n\n예측 결과를 가시화 합니다\n\n학습한 모델을 Test set에 적용하여 y값(“CMEDV”)을 예측합니다.\n예측 결과를 확인하기 위해 실제값과 예측값을 한 plot에 출력해 시각화해보겠습니다.\n\nfrom sklearn import linear_model\n\n# fit regression model in training set\nlr = linear_model.LinearRegression()\nmodel = lr.fit(X_train, y_train)\n\n# predict in test set\npred_test = lr.predict(X_test)\n\n\n### 예측 결과 시각화 (test set)\ndf = pd.DataFrame({'actual': y_test, 'prediction': pred_test})\ndf = df.sort_values(by='actual').reset_index(drop=True)\ndf.head()\n\n### reset_index() : 아무래도 데이터프레임의 다양한 전처리 과정을 거치게 되면 인덱스가 뒤죽박죽인 경우가 많다. 이럴때 인덱스를 다시 처음부터 재배열 해주는 유용한 함수다.\n### drop=True옵션을 주면 기존 인덱스를 버리고 재배열해준다.\n### https://yganalyst.github.io/data_handling/Pd_2/\n\n\n\n\n\n  \n    \n      \n      actual\n      prediction\n    \n  \n  \n    \n      0\n      -2.296565\n      -2.198572\n    \n    \n      1\n      -2.231911\n      -2.058227\n    \n    \n      2\n      -2.230819\n      -2.017243\n    \n    \n      3\n      -2.222628\n      -2.052249\n    \n    \n      4\n      -2.196744\n      -2.111403\n    \n  \n\n\n\n\n\ndf.tail(20)\n\n\n\n\n\n  \n    \n      \n      actual\n      prediction\n    \n  \n  \n    \n      1732\n      2.510238\n      2.535799\n    \n    \n      1733\n      2.543985\n      2.491212\n    \n    \n      1734\n      2.574237\n      2.603024\n    \n    \n      1735\n      2.604708\n      2.624030\n    \n    \n      1736\n      2.616066\n      2.672456\n    \n    \n      1737\n      2.632448\n      2.691611\n    \n    \n      1738\n      2.674167\n      2.674725\n    \n    \n      1739\n      2.705293\n      2.683771\n    \n    \n      1740\n      2.712283\n      2.686997\n    \n    \n      1741\n      2.714576\n      2.780406\n    \n    \n      1742\n      2.736201\n      2.717661\n    \n    \n      1743\n      2.740460\n      2.719423\n    \n    \n      1744\n      2.753456\n      2.745240\n    \n    \n      1745\n      2.765142\n      2.775897\n    \n    \n      1746\n      2.814288\n      2.970564\n    \n    \n      1747\n      2.830452\n      2.877691\n    \n    \n      1748\n      2.841810\n      2.847767\n    \n    \n      1749\n      2.894451\n      2.896483\n    \n    \n      1750\n      2.928307\n      2.970032\n    \n    \n      1751\n      2.945562\n      2.990872\n    \n  \n\n\n\n\n\nplt.figure(figsize=(12, 9))\nplt.scatter(df.index, df['prediction'], marker='x', color='r')\nplt.scatter(df.index, df['actual'], alpha=0.3, marker='o', color='black')\nplt.title(\"Prediction Result in Test Set\", fontsize=20)\nplt.legend(['prediction', 'actual'], fontsize=12)\nplt.show()\n### x축은 집 index, y축이 집값\n\nC:\\ProgramData\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 8722 (\\N{MINUS SIGN}) missing from current font.\n  fig.canvas.print_figure(bytes_io, **kw)\n\n\n\n\n\n\n모델 성능 평가 (R squre 와 RMSE)\n\n\n### R square\nprint(model.score(X_train, y_train))  # training set\nprint(model.score(X_test, y_test))  # test set\n\n0.994254206255394\n0.99483809156575\n\n\n\n# RMSE\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\n# training set\npred_train = lr.predict(X_train)\nprint(sqrt(mean_squared_error(y_train, pred_train)))\n\n# test set\n\nprint(sqrt(mean_squared_error(y_test, pred_test)))\n\n0.07502749641007687\n0.0746899948132357\n\n\n\n### 해설) Test set에서 해당 예측 모델의 R square가 0.76이고, RMSE가 4.83입니다."
  },
  {
    "objectID": "ITM812.html#실습",
    "href": "ITM812.html#실습",
    "title": "머신러닝과 사업기회 - ITM812",
    "section": "4.2 실습",
    "text": "4.2 실습\n\npython 설치. Terminal.\n\n\n\n\n\n\nNote\n\n\n\npy -m pip install jupyter\n\n\n\n\n\n\n\n\nNote\n\n\n\npython -m pip install jupyter\n\npip install pandas\npip install numpy\npip install matplotlib\npip install seaborn\n\n\n\npython3 -m pip install matplotlib\n\n\n\n\n\nCode\nimport pandas as pd\n\n\n\n\nCode\nimport pandas as pd ### 데이터 분석을 하기 위한 파이썬 라이브러리 such as a table\nimport numpy as np ### 수치해석 라이브러리\nimport matplotlib.pyplot as plt ### 그래프 그리는 라이브러리\nimport seaborn as sns ### 그래프 그리는 라이브러리\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\nFigure 1: A line plot on a polar axis\n\n\n\n\n\n\nCode\ndf=pd.read_csv(\"./ITM812/BostonHousing2.csv\")\n\n\n\n\nCode\n#데이터셋을 불러와서 첫 세 줄을 출력하여 데이터의 구성을 한 번 살펴볼게요.\ndf.tail(10)\n#df.tail(3)\n\n\n\n\n\n\n  \n    \n      \n      TOWN\n      LON\n      LAT\n      CMEDV\n      CRIM\n      ZN\n      INDUS\n      CHAS\n      NOX\n      RM\n      AGE\n      DIS\n      RAD\n      TAX\n      PTRATIO\n      B\n      LSTAT\n    \n  \n  \n    \n      496\n      Revere\n      -71.0010\n      42.2525\n      19.7\n      0.28960\n      0.0\n      9.69\n      0\n      0.585\n      5.390\n      72.9\n      2.7986\n      6\n      391\n      19.2\n      396.90\n      21.14\n    \n    \n      497\n      Revere\n      -70.9947\n      42.2496\n      18.3\n      0.26838\n      0.0\n      9.69\n      0\n      0.585\n      5.794\n      70.6\n      2.8927\n      6\n      391\n      19.2\n      396.90\n      14.10\n    \n    \n      498\n      Revere\n      -71.0050\n      42.2455\n      21.2\n      0.23912\n      0.0\n      9.69\n      0\n      0.585\n      6.019\n      65.3\n      2.4091\n      6\n      391\n      19.2\n      396.90\n      12.92\n    \n    \n      499\n      Revere\n      -70.9985\n      42.2430\n      17.5\n      0.17783\n      0.0\n      9.69\n      0\n      0.585\n      5.569\n      73.5\n      2.3999\n      6\n      391\n      19.2\n      395.77\n      15.10\n    \n    \n      500\n      Revere\n      -70.9920\n      42.2380\n      16.8\n      0.22438\n      0.0\n      9.69\n      0\n      0.585\n      6.027\n      79.7\n      2.4982\n      6\n      391\n      19.2\n      396.90\n      14.33\n    \n    \n      501\n      Winthrop\n      -70.9860\n      42.2312\n      22.4\n      0.06263\n      0.0\n      11.93\n      0\n      0.573\n      6.593\n      69.1\n      2.4786\n      1\n      273\n      21.0\n      391.99\n      9.67\n    \n    \n      502\n      Winthrop\n      -70.9910\n      42.2275\n      20.6\n      0.04527\n      0.0\n      11.93\n      0\n      0.573\n      6.120\n      76.7\n      2.2875\n      1\n      273\n      21.0\n      396.90\n      9.08\n    \n    \n      503\n      Winthrop\n      -70.9948\n      42.2260\n      23.9\n      0.06076\n      0.0\n      11.93\n      0\n      0.573\n      6.976\n      91.0\n      2.1675\n      1\n      273\n      21.0\n      396.90\n      5.64\n    \n    \n      504\n      Winthrop\n      -70.9875\n      42.2240\n      22.0\n      0.10959\n      0.0\n      11.93\n      0\n      0.573\n      6.794\n      89.3\n      2.3889\n      1\n      273\n      21.0\n      393.45\n      6.48\n    \n    \n      505\n      Winthrop\n      -70.9825\n      42.2210\n      19.0\n      0.04741\n      0.0\n      11.93\n      0\n      0.573\n      6.030\n      80.8\n      2.5050\n      1\n      273\n      21.0\n      396.90\n      7.88"
  },
  {
    "objectID": "ITM501.html#types-and-patterns-of-innovation",
    "href": "ITM501.html#types-and-patterns-of-innovation",
    "title": "Innovation Management lecture",
    "section": "4.1 Types and Patterns of Innovation",
    "text": "4.1 Types and Patterns of Innovation\n\nProduct innovation\nProcess innovation"
  },
  {
    "objectID": "ITM501.html#assignment-2",
    "href": "ITM501.html#assignment-2",
    "title": "Innovation Management lecture",
    "section": "4.2 Assignment",
    "text": "4.2 Assignment\n\n\n\n\n\n\n이번주 과제는 두 개임\n\n\n\nProcess innovation과 Process innovation을 구분할 수 있는 방법? 정답은 없다. 아이디어 제출.\nProduct/Process innovation 다룬 논문 요약해서 제출"
  },
  {
    "objectID": "GGS621.html#water-systems",
    "href": "GGS621.html#water-systems",
    "title": "GGS621",
    "section": "4.1 Water systems",
    "text": "4.1 Water systems\n\nWater Demand"
  },
  {
    "objectID": "ITM501.html#diffusion-chasm-and-disruptive-innovation",
    "href": "ITM501.html#diffusion-chasm-and-disruptive-innovation",
    "title": "Innovation Management lecture",
    "section": "5.1 Diffusion, Chasm and Disruptive Innovation",
    "text": "5.1 Diffusion, Chasm and Disruptive Innovation\n\n강의\n\nㅇㅇ"
  },
  {
    "objectID": "ITM812/10242023_machinelearningclass_msineoapark/2023_Spring_Lecture9_Restaurant_Review_Classification_10242023_upload.html",
    "href": "ITM812/10242023_machinelearningclass_msineoapark/2023_Spring_Lecture9_Restaurant_Review_Classification_10242023_upload.html",
    "title": "Jiseok AHN",
    "section": "",
    "text": "[Practice 2] : Supervised Learning\n0. Problem Define \n1. Library import\n2. Data Collection\n3. EDA(Exploratory Data Analysis) \n4. Preprocessing\n    - 4.1 Stemmer \n    - 4.2 Bag of Words : 문장별 단어 벡터 만들기\n    - 4.3 Train : Test 로 나누기\n5. Modeling\n\n[0]: Problem Define\n레스토랑 리뷰 : 레스토랑 리뷰를 분석하여 Positive/Negative로 분류합니다. - Data: Kaggle (https://www.kaggle.com/akram24/restaurant-reviews)\n\n\n[1] Libraries setteting : import libraries\n\n### Import Libraries\nimport pandas as pd # data provessing, CSV file I/O(e.g.pd.read_csv)\nimport numpy as np\n\n## Visualization Libraries\nimport matplotlib.pyplot as plt  # https://matplotlib.org/stable/gallery/index\nimport seaborn as sns  # https://seaborn.pydata.org/\n%matplotlib inline\n\n#import warnings\n#warnings.filterwarnings(\"ignore\", category=FutureWarning)\n#warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n\n\n\n[2]: Data Collection\n\n\n[2.1]: 사용할 데이터 셋을 가져옵니다: Restaurant Review\n\ndataset=pd.read_csv(\"./data/Restaurant_Reviews.tsv\", delimiter='\\t')\n### TSV: 파일 형식 \"탭으로 구분 된 값\"을 의미하고,\n### 이 탭으로 구분 된 값 파일이 많은 스프레드 시트 응용 프로그램에 의해 만들어지고 사용\n### 1000개의 레스토랑 리뷰를 가져옵니다.\n\n\n#데이터셋을 불러와서 첫 세 줄을 출력하여 데이터의 구성을 한 번 살펴봅니다.\ndataset.head()\n\n\n\n\n\n  \n    \n      \n      Review\n      Liked\n    \n  \n  \n    \n      0\n      Wow... Loved this place.\n      1\n    \n    \n      1\n      Crust is not good.\n      0\n    \n    \n      2\n      Not tasty and the texture was just nasty.\n      0\n    \n    \n      3\n      Stopped by during the late May bank holiday of...\n      1\n    \n    \n      4\n      The selection on the menu was great and so wer...\n      1\n    \n  \n\n\n\n\n\n\n[3]: EDA(Exploratory Data Analysis) : 데이터 탐색\n\n통계량 확인하기 (Summary)\npandas 로 불러온 데이터 살펴보기: head(), shape(), info(), describe(), value_counts(), unique() 등\nhttps://hogni.tistory.com/5\n\n\n#데이터 셋 구조보기\ndataset.shape\n\n(1000, 2)\n\n\n\n### 해석) 행이 1000개, 열이 2, 리뷰를 통해 분류하는데 사용한 변수가 2개\n\n\ndataset.describe()\n#.describe() 함수는 데이터의 컬럼별 요약 통계량을 나타냅니다. \n# mean(), max(), median()등 개별 함수를 사용하여 통계량을 계산할 수도 있습니다\n\n\n\n\n\n  \n    \n      \n      Liked\n    \n  \n  \n    \n      count\n      1000.00000\n    \n    \n      mean\n      0.50000\n    \n    \n      std\n      0.50025\n    \n    \n      min\n      0.00000\n    \n    \n      25%\n      0.00000\n    \n    \n      50%\n      0.50000\n    \n    \n      75%\n      1.00000\n    \n    \n      max\n      1.00000\n    \n  \n\n\n\n\n\n### [data type]을 확인합니다.\ndataset.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1000 entries, 0 to 999\nData columns (total 2 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   Review  1000 non-null   object\n 1   Liked   1000 non-null   int64 \ndtypes: int64(1), object(1)\nmemory usage: 15.8+ KB\n\n\n\n### [결측치] : 비어있는 데이터를 찾습니다. \n### 여기서는 어떤 컬럼(변수, 특성, x)에 결측치가 많은지 봅니다.\ndataset.isnull().sum() # There has no null values\n\nReview    0\nLiked     0\ndtype: int64\n\n\n\nsns.countplot(x = dataset['Liked'], data = dataset)\n\n<Axes: xlabel='Liked', ylabel='count'>\n\n\n\n\n\n\n\n해석) Liked & Disliked are 500.\n\ndataset[dataset['Liked'] == 1][\"Liked\"].count()\n\n500\n\n\n\ndataset[dataset['Liked'] == 0][\"Liked\"].count()\n\n500\n\n\n0. Problem Define \n1. Library import\n2. Data Collection\n3. EDA(Exploratory Data Analysis) : 데이터 탐색\n4. Preprocessing\n    - 4.1 Stemmer \n    - 4.2 Bag of Words : 문장별 단어 벡터 만들기\n    - 4.3 Train : Test 로 나누기\n5. Modeling\n\n\n[4]: Preprocessing\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\nimport re # 정규화\n\n[nltk.corpus.stopword] - 자연어처리를 할때, 너무 빈번하게 나와서, 아무의미 없다고 판단되는 단어들을 보통 “stopword”라고 부릅니다. - 가령, ‘a’, ’the’와 같은 단어들은 모든 구문(phrase)들에 매우 많이 등장합니다. 따라서 아무 의미를 가지지 못합니다. - 사실 연구자 혹은 분석가가 자연어 처리중에 보면, “아 이 단어는 제외해야겠구나”라는 생각이 옵니다. - 앞에서 본 것처럼 너무 명확하고, 아무 의미가 없는 것이니까요. - Reference: https://frhyme.github.io/python-libs/nltk_stopwords/\n[TF-IDF] - 다른 방법으로는 TF-IDF 값을 사용할 수도 있습니다. - TF-IDF는 “Term Frequency - Inverse Document Frequency”를 말하며, 키워드의 빈도(TF)가 클수록 크고, 등장한 문서의 비율(DF)가 클수록 작아집니다. - 즉, DF가 클수록 작아집니다. - 보통 stopword는 DF(Document Frequency)가 굉장히 큽니다. 대부분의 문서에서 등장합니다. - nltk.corpus.stopwords.words(‘english’)를 사용해서 보편적으로 사용되는 stopwords를 가져올 수 있습니다. - nltk.corpus.stopword 에는 다음 단어들이 포함되어 있죠. 보면 대략 합당하고 느껴집니다. - Reference: https://wikidocs.net/22530\n[Stemming] - Stemming은 단어에서 접사(affix)를 제거하는 것을 의미합니다. - Stemming은 검색엔진에서 색인할 때 가장 많이 씁니다. - 모든 형태의 단어를 저장하는것 보다 Stemming한 단어를 저장하는 것이 색인 크기를 줄일 뿐만아니라 검색 정확성을 높일 수 있습니다. - 이런 알고리즘 stemmer/stemming algorithm 이라고 합니다. - Reference: https://excelsior-cjh.tistory.com/67\n\n\n[4-1 Stemmer] SnowballStemmer Class\n\nsteeming words를 지원하는 class가 여러개가 있습니다.\n그 중 본 강의에서는 영어 외의 13개 국가의 언어에대한 Stemming을 지원하는 SnowballStemmer를 사용합니다.\n\n\nstemmer = SnowballStemmer('english')\n\n\nstemmer\n\n<nltk.stem.snowball.SnowballStemmer at 0x256bcf230d0>\n\n\n\ncorpus=[]\n\n\n##stemmer = SnowballStemmer('english')\nimport nltk\nnltk.download('stopwords')\n\ncorpus = []\nfor i in range(0,1000):\n    review = re.sub('[^a-zA-Z]',' ',dataset['Review'][i]) \n    ## ^뒤의 문자열로 문자열이 시작됩니다.\n    ## sub = substitution.\n    ## 가정입니다. \n    ## 알파벳을 제외한 문자들을 ' '로 대체합니다.\n    ## 영어 문장에 각주 등과 같은 이유로 특수 문자가 섞여있습니다. \n    ## 자연어 처리를 위해 특수 문자를 제거하고 싶다면 알파벳 외의 문자는 공백으로 처리하는 등의 사용 용도로 쓸 수 있습니다.\n    \n    review = review.lower() ## 대문자를 소문자(lower)로 바꾸어 줍니다.\n    review = review.split()\n    review = [stemmer.stem(word) for word in review if word not in set(stopwords.words('english'))]\n    #stemmer 에 있는 모든 단어에서 접사를 빼고 review 에 넣어줍니다. \n    \n    review = ' '.join(review)\n    corpus.append(review)\n    #append를 사용해서 계속 붙여 나갑니다.\n\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\n[Regular Expression]\n- split(): 함수는 입력된 정규 표현식을 기준으로 문자열들을 분리하여 리스트로 리턴합니다. \n- 자연어 처리에 있어서 가장 많이 사용되는 정규 표현식 함수 중 하나인데, 토큰화에 유용하게 쓰일 수 있기 때문입니다.\n- import re\n    - text=\"사과 딸기 수박 메론 바나나\"\n    - re.split(\" \",text)\n- 결과\n    - ['사과', '딸기', '수박', '메론', '바나나']\n- Reference : https://wikidocs.net/21703\n\n### import nltk\n### nltk.download()\n\n### 또다른 방법\n### !pip install nltk (at jupyter)\n### pip install nltk(at python console)\n\n\ndataset.head()\n### dataset=pd.read_csv(\"./data/Restaurant_Reviews.tsv\", delimiter='\\t')\n\n\n\n\n\n  \n    \n      \n      Review\n      Liked\n    \n  \n  \n    \n      0\n      Wow... Loved this place.\n      1\n    \n    \n      1\n      Crust is not good.\n      0\n    \n    \n      2\n      Not tasty and the texture was just nasty.\n      0\n    \n    \n      3\n      Stopped by during the late May bank holiday of...\n      1\n    \n    \n      4\n      The selection on the menu was great and so wer...\n      1\n    \n  \n\n\n\n\n\n데이터 전처리가 잘 되어 있는지 확인해 보자!!\n\ncorpus[0]\n### corpus는 전처리 후의 데이터 셋\n\n'wow love place'\n\n\n\ncorpus[1]\n\n'crust good'\n\n\n\n\n\n해석)\n\n모든 데이터의 전처리를 잘 마쳤는지, corpus의 길이를 확인합니다.\n\ncorpus는 전처리된 리스트들이 들어 있는 테이블 입니다.\n\n\nlen(corpus)\n\n1000\n\n\n\nlen(dataset) ## 기존 데이터셋과 같음을 확인하기 위해 점검\n\n1000\n\n\n\ndataset.tail()\n\n\n\n\n\n  \n    \n      \n      Review\n      Liked\n    \n  \n  \n    \n      995\n      I think food should have flavor and texture an...\n      0\n    \n    \n      996\n      Appetite instantly gone.\n      0\n    \n    \n      997\n      Overall I was not impressed and would not go b...\n      0\n    \n    \n      998\n      The whole experience was underwhelming, and I ...\n      0\n    \n    \n      999\n      Then, as if I hadn't wasted enough of my life ...\n      0\n    \n  \n\n\n\n\n\ncorpus[999] ### 데이터 전처리 되 후의 데이터 확인\n\n'wast enough life pour salt wound draw time took bring check'\n\n\n\ndataset[\"Review\"][999] ### 원본 데이터 확인\n\n\"Then, as if I hadn't wasted enough of my life there, they poured salt in the wound by drawing out the time it took to bring the check.\"\n\n\n\n\n해석)\n\nraw data 와 preprocessed data 를 비교해 봅니다.\n\n\ndataset.head()\n\n\n\n\n\n  \n    \n      \n      Review\n      Liked\n    \n  \n  \n    \n      0\n      Wow... Loved this place.\n      1\n    \n    \n      1\n      Crust is not good.\n      0\n    \n    \n      2\n      Not tasty and the texture was just nasty.\n      0\n    \n    \n      3\n      Stopped by during the late May bank holiday of...\n      1\n    \n    \n      4\n      The selection on the menu was great and so wer...\n      1\n    \n  \n\n\n\n\n\ndataset[\"Review\"][0]\n\n'Wow... Loved this place.'\n\n\n\ncorpus[0]\n\n'wow love place'\n\n\n0. Problem Define \n1. Library import\n2. Data Collection\n3. EDA(Exploratory Data Analysis) : 데이터 탐색\n4. Preprocessing\n    - 4.1 Stemmer \n    - 4.2 Bag of Words : 문장별 단어 벡터 만들기\n    - 4.3 Train : Test 로 나누기\n5. Modeling\n\n\n[4-2 Bag of Words]\n\n[참고 0] Sentimantal Analysis: 단어 벡터 및 단어 갯수\n\nhttps://smlee729.github.io/python/natural%20language%20processing/2015/04/06/1-sentimental-analysis-2.html\n\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n### Convert a collection of text documents to a matrix of token counts\n### sklearn>>feature_extraction>>text>>CountVectorizer\n\n\n#디폴트 값으로 CountVectorizer를 만들어줍니다.\ncv = CountVectorizer(max_features=1500)\n\n\n\n[참고 1] max_features\n\nmax_feature는 최대 feature 갯수를 설정해주는 파라미터입니다.\n해당 파라미터를 이해하려면, feature의 개념에 대해 아셔야 합니다.\n머신러닝에서 feature란, 테이블의 컬럼에 해당하는 개념입니다. 또한 행렬의 열에 해당하는 것이기도 합니다.\n그럼 구체적인 예시를 코드로 한 번 살펴보겠습니다.\n만약 한 문장에서 15 종류의 단어가 사용되었다면, feature 값은 15이다. max_features 값은 최대 단어의 종류(15)가 될 것 이다.\n또 다른 의미로는 너무 많은 단어가 있을 경우, 가장 빈도수가 높은 단어 15개만 사용합니다.\n\n\n\n[참고 2] CountVectorizer\n\n단어들의 카운트(출현 빈도(frequency))로 여러 문서들을 벡터화\n카운트 행렬, 단어 문서 행렬 (Term-Document Matrix, TDM))\n모두 소문자로 변환시키기 때문에 me 와 Me 는 모두 같은 특성이 된다.\n\n\ncv.fit(corpus)\n### fit 함수를 통해서 각 리뷰들을 대입합니다. (단어 그대로 사용해서) 벡터로 만들어 줍니다.\n\nCountVectorizer(max_features=1500)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.CountVectorizerCountVectorizer(max_features=1500)\n\n\n\nprint('Vocabulary: ')\nprint(cv.vocabulary_)\n### cv에 있는 단어들과 들어있는 순서를 의미합니다.(다시 알파벳으로  ordering을 합니다.)\n\nVocabulary: \n{'wow': 1482, 'love': 762, 'place': 972, 'crust': 312, 'good': 581, 'tasti': 1299, 'textur': 1311, 'nasti': 845, 'stop': 1248, 'late': 740, 'may': 789, 'bank': 91, 'holiday': 652, 'rick': 1088, 'steve': 1241, 'recommend': 1055, 'select': 1143, 'menu': 798, 'great': 595, 'price': 1005, 'get': 565, 'angri': 33, 'want': 1433, 'damn': 321, 'pho': 960, 'honeslti': 655, 'tast': 1297, 'fresh': 540, 'potato': 997, 'like': 750, 'rubber': 1102, 'could': 285, 'tell': 1304, 'made': 771, 'ahead': 15, 'time': 1332, 'kept': 729, 'warmer': 1435, 'fri': 541, 'touch': 1350, 'servic': 1152, 'prompt': 1016, 'would': 1480, 'go': 575, 'back': 82, 'cashier': 201, 'care': 194, 'ever': 449, 'say': 1128, 'still': 1243, 'end': 433, 'wayyy': 1443, 'overpr': 912, 'tri': 1360, 'cape': 190, 'cod': 247, 'ravoli': 1045, 'chicken': 224, 'cranberri': 298, 'mmmm': 817, 'disgust': 372, 'pretti': 1004, 'sure': 1283, 'human': 671, 'hair': 615, 'shock': 1161, 'sign': 1172, 'indic': 693, 'cash': 199, 'high': 645, 'waitress': 1430, 'littl': 753, 'slow': 1184, 'worth': 1479, 'let': 747, 'alon': 23, 'vega': 1404, 'blah': 132, 'food': 526, 'amaz': 27, 'also': 24, 'cute': 319, 'interior': 707, 'beauti': 107, 'perform': 952, 'right': 1090, 'red': 1056, 'velvet': 1410, 'cake': 181, 'ohhh': 881, 'stuff': 1261, 'never': 854, 'brought': 163, 'salad': 1112, 'ask': 59, 'hole': 651, 'wall': 1432, 'mexican': 801, 'street': 1253, 'taco': 1289, 'friend': 543, 'staff': 1229, 'took': 1344, 'hour': 666, 'tabl': 1288, 'restaur': 1078, 'luke': 766, 'warm': 1434, 'sever': 1154, 'run': 1104, 'around': 55, 'total': 1349, 'overwhelm': 913, 'worst': 1478, 'salmon': 1113, 'sashimi': 1121, 'combo': 254, 'burger': 170, 'beer': 110, 'decent': 328, 'deal': 327, 'final': 502, 'blow': 141, 'found': 533, 'accid': 2, 'happier': 626, 'seem': 1141, 'quick': 1033, 'grab': 588, 'bite': 130, 'familiar': 479, 'pub': 1021, 'favor': 489, 'look': 759, 'elsewher': 429, 'overal': 909, 'lot': 761, 'redeem': 1057, 'qualiti': 1031, 'inexpens': 696, 'ampl': 31, 'portion': 993, 'poor': 990, 'waiter': 1429, 'feel': 492, 'stupid': 1262, 'everi': 450, 'came': 185, 'first': 509, 'visit': 1421, 'hiro': 649, 'delight': 343, 'suck': 1269, 'shrimp': 1169, 'tender': 1307, 'moist': 819, 'enough': 436, 'drag': 392, 'establish': 443, 'hard': 627, 'judg': 725, 'whether': 1456, 'side': 1171, 'gross': 605, 'melt': 797, 'styrofoam': 1264, 'eat': 412, 'fear': 491, 'sick': 1170, 'posit': 994, 'note': 868, 'server': 1151, 'attent': 66, 'provid': 1020, 'frozen': 545, 'puck': 1023, 'peopl': 949, 'behind': 112, 'regist': 1064, 'thing': 1318, 'prime': 1007, 'rib': 1084, 'dessert': 354, 'section': 1139, 'bad': 84, 'generic': 562, 'beef': 109, 'cook': 277, 'sandwich': 1119, 'firehous': 508, 'greek': 598, 'dress': 397, 'pita': 970, 'hummus': 673, 'refresh': 1061, 'order': 897, 'duck': 405, 'rare': 1039, 'pink': 968, 'insid': 701, 'nice': 857, 'char': 211, 'outsid': 905, 'us': 1395, 'realiz': 1049, 'husband': 677, 'left': 744, 'sunglass': 1280, 'chow': 232, 'horribl': 660, 'attitud': 67, 'toward': 1352, 'custom': 317, 'talk': 1293, 'one': 889, 'enjoy': 435, 'huge': 670, 'wonder': 1470, 'imagin': 684, 'heart': 635, 'attack': 65, 'grill': 601, 'downtown': 391, 'absolut': 0, 'flat': 513, 'excus': 459, 'much': 832, 'seafood': 1134, 'string': 1256, 'pasta': 935, 'bottom': 152, 'amount': 30, 'sauc': 1125, 'power': 1000, 'scallop': 1129, 'perfect': 951, 'rip': 1093, 'banana': 90, 'petrifi': 956, 'tasteless': 1298, 'least': 742, 'think': 1319, 'refil': 1059, 'water': 1440, 'struggl': 1259, 'wave': 1441, 'minut': 812, 'receiv': 1053, 'star': 1232, 'appet': 48, 'cocktail': 245, 'handmad': 622, 'delici': 341, 'definit': 337, 'glad': 570, 'give': 568, 'militari': 807, 'discount': 370, 'alway': 26, 'dos': 383, 'gringo': 602, 'updat': 1392, 'went': 1452, 'second': 1138, 'got': 585, 'appar': 46, 'heard': 634, 'salt': 1115, 'batter': 101, 'fish': 510, 'chewi': 223, 'way': 1442, 'finish': 506, 'includ': 689, 'drink': 400, 'jeff': 715, 'beyond': 119, 'expect': 461, 'realli': 1050, 'rice': 1086, 'meh': 796, 'min': 810, 'milkshak': 809, 'noth': 869, 'chocol': 229, 'milk': 808, 'guess': 610, 'known': 735, 'excalibur': 455, 'use': 1396, 'common': 257, 'sens': 1146, 'dish': 373, 'quit': 1034, 'appal': 45, 'valu': 1401, 'well': 1451, 'sweet': 1286, 'season': 1136, 'today': 1337, 'lunch': 768, 'buffet': 166, 'cheat': 216, 'wast': 1438, 'opportun': 894, 'compani': 258, 'come': 255, 'experienc': 464, 'underwhelm': 1379, 'relationship': 1067, 'parti': 931, 'wait': 1428, 'person': 955, 'break': 158, 'walk': 1431, 'smell': 1189, 'old': 884, 'greas': 593, 'trap': 1358, 'other': 899, 'turkey': 1370, 'roast': 1095, 'bland': 134, 'pan': 923, 'everyon': 451, 'rave': 1044, 'sugari': 1273, 'disast': 368, 'tailor': 1290, 'palat': 920, 'six': 1180, 'year': 1490, 'spring': 1227, 'roll': 1097, 'oh': 880, 'yummi': 1498, 'meat': 794, 'ratio': 1043, 'unsatisfi': 1388, 'omelet': 887, 'die': 357, 'everyth': 452, 'summari': 1276, 'larg': 738, 'disappoint': 366, 'dine': 360, 'experi': 463, 'sexi': 1156, 'mouth': 828, 'outrag': 903, 'flirt': 518, 'hottest': 665, 'rock': 1096, 'casino': 202, 'step': 1240, 'forward': 532, 'best': 117, 'breakfast': 159, 'bye': 176, 'tip': 1334, 'ladi': 737, 'arriv': 57, 'cafe': 180, 'serv': 1150, 'fantast': 483, 'wife': 1461, 'garlic': 557, 'bone': 147, 'marrow': 788, 'ad': 11, 'extra': 468, 'meal': 792, 'anoth': 35, 'help': 642, 'bloddi': 138, 'mari': 786, 'town': 1353, 'cannot': 188, 'beat': 105, 'mussel': 838, 'wine': 1463, 'reduct': 1058, 'better': 118, 'tigerlilli': 1331, 'afternoon': 13, 'bartend': 95, 'ambienc': 29, 'music': 837, 'play': 979, 'next': 856, 'trip': 1363, 'sooooo': 1206, 'real': 1048, 'sushi': 1285, 'lover': 763, 'honest': 656, 'yama': 1487, 'pass': 933, 'busi': 172, 'thai': 1312, 'spici': 1221, 'check': 217, 'atmospher': 62, 'kind': 733, 'steak': 1237, 'although': 25, 'sound': 1210, 'actual': 10, 'bit': 128, 'know': 734, 'manag': 781, 'blandest': 135, 'eaten': 413, 'prepar': 1002, 'indian': 692, 'cuisin': 315, 'boot': 149, 'worri': 1476, 'fine': 504, 'guy': 612, 'son': 1202, 'said': 1110, 'thought': 1324, 'ventur': 1412, 'away': 74, 'hit': 650, 'spot': 1225, 'night': 859, 'host': 662, 'lack': 736, 'word': 1472, 'bitch': 129, 'number': 871, 'reason': 1051, 'review': 1080, 'leav': 743, 'phenomen': 958, 'ambianc': 28, 'return': 1079, 'strip': 1257, 'pork': 992, 'belli': 115, 'mediocr': 795, 'penn': 948, 'vodka': 1422, 'excel': 457, 'crispi': 307, 'wrap': 1483, 'delish': 344, 'tuna': 1369, 'rude': 1103, 'nyc': 874, 'bagel': 85, 'cream': 302, 'chees': 219, 'caper': 191, 'even': 447, 'subway': 1267, 'fact': 473, 'serious': 1148, 'solid': 1195, 'bar': 92, 'extrem': 470, 'mani': 784, 'weekend': 1448, 'empti': 432, 'suggest': 1274, 'ate': 61, 'curri': 316, 'bamboo': 89, 'shoot': 1163, 'blanket': 136, 'moz': 830, 'top': 1345, 'done': 379, 'cover': 292, 'subpar': 1266, 'bathroom': 100, 'clean': 240, 'decor': 331, 'chang': 210, 'consid': 271, 'pace': 917, 'thumb': 1329, 'watch': 1439, 'pay': 941, 'ignor': 682, 'fianc': 497, 'middl': 804, 'day': 325, 'greet': 600, 'seat': 1137, 'mandalay': 782, 'bay': 102, 'forti': 531, 'five': 511, 'vain': 1399, 'crostini': 308, 'stale': 1230, 'highlight': 646, 'nigiri': 860, 'joint': 722, 'differ': 358, 'cut': 318, 'piec': 964, 'flavor': 514, 'voodoo': 1424, 'sinc': 1177, 'gluten': 574, 'free': 537, 'ago': 14, 'unfortun': 1381, 'must': 839, 'bakeri': 86, 'reloc': 1070, 'impress': 687, 'immedi': 685, 'divers': 376, 'avoid': 72, 'cost': 282, 'full': 550, 'hand': 620, 'phoenix': 961, 'metro': 800, 'area': 52, 'treat': 1359, 'bacon': 83, 'hella': 640, 'salti': 1116, 'spinach': 1223, 'avocado': 71, 'ingredi': 699, 'sad': 1107, 'liter': 752, 'zero': 1499, 'list': 751, 'khao': 731, 'soi': 1194, 'miss': 814, 'terrif': 1310, 'thrill': 1326, 'accommod': 3, 'vegetarian': 1407, 'daughter': 324, 'perhap': 953, 'caught': 204, 'inspir': 702, 'desir': 351, 'modern': 818, 'hip': 648, 'maintain': 777, 'cozi': 294, 'week': 1447, 'haunt': 630, 'sat': 1122, 'take': 1291, 'overcook': 910, 'charcoal': 212, 'decid': 329, 'send': 1145, 'verg': 1414, 'probabl': 1009, 'dirt': 363, 'someth': 1199, 'healthi': 633, 'quantiti': 1032, 'raspberri': 1040, 'ice': 680, 'incred': 691, 'interest': 706, 'crepe': 305, 'station': 1235, 'hot': 664, 'bread': 157, 'butter': 174, 'home': 653, 'chip': 226, 'origin': 898, 'egg': 423, 'gyro': 613, 'wing': 1464, 'satisfi': 1124, 'joey': 720, 'vote': 1425, 'dog': 377, 'valley': 1400, 'reader': 1047, 'magazin': 774, 'bowl': 153, 'live': 754, 'friday': 542, 'insult': 705, 'felt': 496, 'disrespect': 375, 'drive': 402, 'exceed': 456, 'hope': 659, 'dream': 395, 'serivc': 1149, 'brunch': 164, 'invit': 708, 'last': 739, 'foot': 527, 'mix': 816, 'mushroom': 836, 'yukon': 1496, 'gold': 578, 'pure': 1027, 'white': 1457, 'corn': 279, 'beateous': 106, 'bug': 167, 'show': 1167, 'given': 569, 'climb': 241, 'soon': 1204, 'tartar': 1296, 'though': 1323, 'soggi': 1193, 'jamaican': 713, 'mojito': 820, 'small': 1185, 'rich': 1087, 'accord': 5, 'shower': 1168, 'rins': 1092, 'unless': 1385, 'mind': 811, 'nude': 870, 'see': 1140, 'lobster': 755, 'bisqu': 127, 'bussel': 173, 'sprout': 1228, 'risotto': 1094, 'filet': 499, 'need': 849, 'pepper': 950, 'cours': 289, 'none': 865, 'bode': 145, 'someon': 1198, 'either': 425, 'cold': 249, 'date': 323, 'unbeliev': 1376, 'bargain': 94, 'folk': 524, 'otto': 901, 'make': 778, 'welcom': 1450, 'special': 1217, 'main': 776, 'uninspir': 1383, 'whenev': 1455, 'world': 1475, 'annoy': 34, 'drunk': 404, 'fun': 551, 'chef': 222, 'doubl': 384, 'cheeseburg': 220, 'singl': 1178, 'patti': 940, 'fall': 477, 'apart': 42, 'pictur': 963, 'upload': 1394, 'yeah': 1489, 'coupl': 287, 'sport': 1224, 'event': 448, 'tv': 1372, 'possibl': 995, 'descript': 349, 'yum': 1497, 'eel': 419, 'yet': 1494, 'mayo': 791, 'hardest': 628, 'decis': 330, 'suppos': 1282, 'eye': 471, 'stay': 1236, 'money': 822, 'flavour': 516, 'almost': 22, 'build': 168, 'freez': 538, 'close': 242, 'point': 986, 'ayc': 77, 'light': 749, 'dark': 322, 'set': 1153, 'mood': 825, 'base': 96, 'sub': 1265, 'par': 927, 'effort': 422, 'gratitud': 591, 'owner': 915, 'privileg': 1008, 'work': 1473, 'creami': 303, 'parent': 929, 'similar': 1174, 'complaint': 261, 'silent': 1173, 'pizza': 971, 'peanut': 945, 'fast': 486, 'godfath': 577, 'tough': 1351, 'short': 1165, 'stick': 1242, 'recal': 1052, 'charg': 213, 'tap': 1294, 'exquisit': 466, 'generous': 563, 'plus': 984, 'buck': 165, 'thus': 1330, 'far': 484, 'twice': 1373, 'self': 1144, 'proclaim': 1011, 'coffe': 248, 'wild': 1462, 'veggitarian': 1409, 'platter': 978, 'cant': 189, 'wrong': 1485, 'madison': 773, 'ironman': 709, 'job': 719, 'dedic': 332, 'boba': 144, 'tea': 1301, 'jenni': 716, 'patio': 938, 'outstand': 906, 'goat': 576, 'skimp': 1181, 'mac': 769, 'bachi': 81, 'stink': 1244, 'saganaki': 1109, 'hate': 629, 'disagre': 365, 'fellow': 495, 'yelper': 1493, 'later': 741, 'neighborhood': 852, 'conveni': 276, 'locat': 756, 'pull': 1024, 'soooo': 1205, 'gave': 559, 'rate': 1041, 'pleas': 980, 'third': 1320, 'write': 1484, 'stir': 1245, 'noodl': 866, 'count': 286, 'box': 154, 'bore': 150, 'greedi': 597, 'corpor': 280, 'dime': 359, 'atroci': 63, 'summer': 1277, 'charm': 214, 'outdoor': 902, 'toast': 1336, 'english': 434, 'muffin': 833, 'untoast': 1389, 'hous': 667, 'bus': 171, 'boy': 155, 'basic': 98, 'figur': 498, 'joke': 723, 'public': 1022, 'bbq': 103, 'fare': 485, 'two': 1374, 'happi': 625, 'downsid': 390, 'without': 1469, 'doubt': 385, 'except': 458, 'month': 824, 'favorit': 490, 'shawarrrrrrma': 1159, 'black': 131, 'pea': 943, 'unreal': 1387, 'vinaigrett': 1418, 'seen': 1142, 'especi': 442, 'mom': 821, 'pleasant': 981, 'honor': 657, 'hut': 678, 'coupon': 288, 'truli': 1366, 'dirti': 364, 'replenish': 1073, 'plain': 973, 'yucki': 1495, 'standard': 1231, 'omg': 888, 'delicioso': 342, 'authent': 69, 'spaghetti': 1216, 'whatsoev': 1453, 'veget': 1406, 'tucson': 1367, 'chipotl': 228, 'classi': 238, 'succul': 1268, 'basebal': 97, 'brick': 161, 'oven': 908, 'app': 44, 'multipl': 835, 'ten': 1306, 'terribl': 1309, 'equal': 441, 'pancak': 924, 'genuin': 564, 'enthusiast': 438, 'gordon': 584, 'ramsey': 1035, 'shall': 1157, 'sharpli': 1158, 'life': 748, 'door': 382, 'offer': 878, 'cool': 278, 'turn': 1371, 'els': 428, 'buy': 175, 'handl': 621, 'rowdi': 1101, 'find': 503, 'despic': 352, 'soup': 1212, 'lukewarm': 767, 'crave': 299, 'deserv': 350, 'stomach': 1246, 'ach': 7, 'rest': 1076, 'drop': 403, 'ball': 88, 'space': 1215, 'tini': 1333, 'eleg': 426, 'comfort': 256, 'usual': 1397, 'eggplant': 424, 'green': 599, 'bean': 104, 'outta': 907, 'part': 930, 'inconsider': 690, 'hi': 644, 'dinner': 361, 'outshin': 904, 'halibut': 617, 'told': 1339, 'happen': 624, 'car': 192, 'front': 544, 'starv': 1234, 'disgrac': 371, 'def': 335, 'ethic': 445, 'continu': 275, 'andddd': 32, 'anyon': 38, 'past': 934, 'stuf': 1260, 'crystal': 314, 'shop': 1164, 'mall': 779, 'aria': 54, 'summar': 1275, 'nay': 846, 'transcend': 1357, 'bring': 162, 'joy': 724, 'pneumat': 985, 'condiment': 268, 'dispens': 374, 'ian': 679, 'kid': 732, 'option': 896, 'famili': 478, 'impecc': 686, 'simpli': 1176, 'account': 6, 'screw': 1133, 'remind': 1072, 'pop': 991, 'san': 1118, 'francisco': 535, 'gourmet': 587, 'frustrat': 547, 'petti': 957, 'hungri': 675, 'assur': 60, 'teeth': 1303, 'sore': 1208, 'complet': 262, 'becom': 108, 'regular': 1065, 'profession': 1012, 'companion': 259, 'ground': 606, 'smear': 1188, 'track': 1354, 'everywher': 453, 'pile': 965, 'bird': 125, 'poop': 989, 'furthermor': 553, 'oper': 892, 'websit': 1445, 'mistak': 815, 'expert': 465, 'connisseur': 269, 'topic': 1346, 'jerk': 717, 'strike': 1255, 'rush': 1105, 'nicest': 858, 'across': 9, 'biscuit': 126, 'absolutley': 1, 'awkward': 76, 'cow': 293, 'ths': 1328, 'gristl': 603, 'fat': 487, 'steiner': 1239, 'dollar': 378, 'anyway': 41, 'fs': 548, 'combin': 253, 'pear': 946, 'almond': 21, 'big': 120, 'winner': 1465, 'spicier': 1222, 'prefer': 1001, 'ribey': 1085, 'mesquit': 799, 'anytim': 40, 'gooodd': 583, 'connoisseur': 270, 'certain': 207, 'contain': 274, 'driest': 399, 'relax': 1068, 'venu': 1413, 'group': 607, 'etc': 444, 'nargil': 844, 'tater': 1300, 'tot': 1348, 'southwest': 1214, 'paid': 919, 'vanilla': 1402, 'smooth': 1191, 'profiterol': 1013, 'choux': 231, 'pastri': 936, 'im': 683, 'az': 78, 'new': 855, 'due': 407, 'acknowledg': 8, 'forget': 529, 'margarita': 785, 'ventil': 1411, 'upgrad': 1393, 'rather': 1042, 'camelback': 186, 'flower': 520, 'cartel': 197, 'trim': 1362, 'claim': 236, 'bill': 123, 'jewel': 718, 'exact': 454, 'near': 847, 'boil': 146, 'crab': 296, 'leg': 745, 'toro': 1347, 'cavier': 206, 'extraordinari': 469, 'thin': 1317, 'slice': 1183, 'wagyu': 1427, 'truffl': 1365, 'dont': 380, 'long': 757, 'attach': 64, 'gas': 558, 'awesom': 75, 'wors': 1477, 'humili': 672, 'worker': 1474, 'bunch': 169, 'name': 842, 'call': 183, 'conclus': 267, 'fill': 500, 'daili': 320, 'tragedi': 1356, 'struck': 1258, 'crawfish': 300, 'monster': 823, 'funni': 552, 'multi': 834, 'grain': 589, 'pumpkin': 1025, 'pecan': 947, 'fluffi': 521, 'airlin': 16, 'noca': 863, 'thorough': 1322, 'homemad': 654, 'cheesecurd': 221, 'typic': 1375, 'glanc': 571, 'finger': 505, 'item': 711, 'gone': 580, 'greasi': 594, 'unhealthi': 1382, 'might': 805, 'deliveri': 346, 'man': 780, 'apolog': 43, 'expens': 462, 'pack': 918, 'tiramisu': 1335, 'cannoli': 187, 'sun': 1278, 'whole': 1458, 'choos': 230, 'frenchman': 539, 'opinion': 893, 'entre': 440, 'gc': 560, 'sampl': 1117, 'thirti': 1321, 'vacant': 1398, 'yellowtail': 1492, 'carpaccio': 195, 'stranger': 1251, 'hello': 641, 'strang': 1250, 'boyfriend': 156, 'recent': 1054, 'donut': 381, 'save': 1127, 'room': 1098, 'mayb': 790, 'howev': 668, 'particular': 932, 'suffer': 1271, 'tapa': 1295, 'vinegrett': 1419, 'babi': 80, 'palm': 922, 'believ': 113, 'hanker': 623, 'forth': 530, 'theft': 1315, 'eew': 420, 'overhaul': 911, 'wit': 1468, 'guest': 611, 'super': 1281, 'swung': 1287, 'deepli': 334, 'effici': 421, 'fan': 481, 'sucker': 1270, 'dri': 398, 'cheap': 215, 'oliv': 886, 'perpar': 954, 'present': 1003, 'giant': 566, 'dust': 409, 'powder': 999, 'sugar': 1272, 'fo': 522, 'accomod': 4, 'vegan': 1405, 'veggi': 1408, 'crumbi': 311, 'pale': 921, 'color': 252, 'instead': 704, 'crouton': 309, 'crema': 304, 'caf': 179, 'expand': 460, 'wish': 1467, 'philadelphia': 959, 'sit': 1179, 'fair': 475, 'crisp': 306, 'north': 867, 'scottsdal': 1131, 'soooooo': 1207, 'freak': 536, 'paper': 926, 'reheat': 1066, 'ok': 883, 'wedg': 1446, 'sorri': 1209, 'tongu': 1342, 'cheek': 218, 'bloodi': 139, 'despit': 353, 'yellow': 1491, 'saffron': 1108, 'thru': 1327, 'mean': 793, 'half': 616, 'somehow': 1197, 'luck': 765, 'non': 864, 'focus': 523, 'grandmoth': 590, 'hostess': 663, 'four': 534, 'blue': 143, 'shirt': 1160, 'vibe': 1417, 'drastic': 393, 'caesar': 178, 'madhous': 772, 'proven': 1019, 'dead': 326, 'greatest': 596, 'macaron': 770, 'insan': 700, 'inform': 698, 'somewhat': 1201, 'edibl': 416, 'promis': 1015, 'fail': 474, 'deliv': 345, 'averag': 70, 'plater': 977, 'togeth': 1338, 'construct': 273, 'italian': 710, 'scream': 1132, 'legit': 746, 'book': 148, 'somethat': 1200, 'duo': 408, 'violinist': 1420, 'song': 1203, 'request': 1074, 'baklava': 87, 'falafel': 476, 'baba': 79, 'ganoush': 555, 'mgm': 802, 'courteous': 291, 'eclect': 414, 'onion': 890, 'ring': 1091, 'nobu': 862, 'googl': 582, 'smashburg': 1187, 'gem': 561, 'plantain': 974, 'spend': 1219, 'panna': 925, 'cotta': 284, 'slaw': 1182, 'drench': 396, 'piano': 962, 'soundtrack': 1211, 'rge': 1082, 'fillet': 501, 'relleno': 1069, 'plate': 976, 'sergeant': 1147, 'auju': 68, 'hawaiian': 631, 'breez': 160, 'mango': 783, 'magic': 775, 'pineappl': 967, 'smoothi': 1192, 'mortifi': 826, 'needless': 850, 'drip': 401, 'most': 827, 'hospit': 661, 'industri': 695, 'paradis': 928, 'refrain': 1060, 'cibo': 234, 'longer': 758, 'famous': 480, 'read': 1046, 'pros': 1018, 'simpl': 1175, 'dough': 387, 'tonight': 1343, 'elk': 427, 'hook': 658, 'classic': 239, 'quaint': 1029, 'compliment': 263, 'thank': 1313, 'dylan': 410, 'tummi': 1368, 'gratuiti': 592, 'fli': 517, 'appl': 50, 'juic': 726, 'han': 619, 'nan': 843, 'bare': 93, 'ryan': 1106, 'edinburgh': 417, 'revisit': 1081, 'chines': 225, 'naan': 840, 'pine': 966, 'nut': 872, 'airport': 17, 'speedi': 1218, 'calligraphi': 184, 'anyth': 39, 'complain': 260, 'stood': 1247, 'begin': 111, 'open': 891, 'extens': 467, 'wide': 1459, 'array': 56, 'inflat': 697, 'smaller': 1186, 'grow': 608, 'rapid': 1038, 'fuzzi': 554, 'fabul': 472, 'wonton': 1471, 'thick': 1316, 'spice': 1220, 'whelm': 1454, 'crowd': 310, 'older': 885, 'mid': 803, 'arepa': 53, 'jalapeno': 712, 'that': 1314, 'shoe': 1162, 'defin': 336, 'block': 137, 'low': 764, 'key': 730, 'fanci': 482, 'afford': 12, 'sour': 1213, 'sunday': 1279, 'tradit': 1355, 'hunan': 674, 'style': 1263, 'bother': 151, 'flair': 512, 'nutshel': 873, 'restaraunt': 1077, 'sewer': 1155, 'girlfriend': 567, 'veal': 1403, 'satifi': 1123, 'join': 721, 'club': 243, 'via': 1416, 'email': 430, 'case': 198, 'colder': 250, 'flavorless': 515, 'describ': 348, 'tepid': 1308, 'chain': 209, 'easili': 411, 'nacho': 841, 'crazi': 301, 'juri': 727, 'court': 290, 'wienerschnitzel': 1460, 'idea': 681, 'herea': 643, 'tribut': 1361, 'held': 638, 'salsa': 1114, 'pissd': 969, 'surpris': 1284, 'golden': 579, 'fell': 493, 'devin': 356, 'employe': 431, 'mozzarella': 831, 'neglig': 851, 'unwelcom': 1390, 'consist': 272, 'fruit': 546, 'peach': 944, 'offici': 879, 'blown': 142, 'put': 1028, 'plastic': 975, 'oppos': 895, 'cram': 297, 'takeout': 1292, 'cr': 295, 'pe': 942, 'delic': 340, 'aw': 73, 'kabuki': 728, 'maria': 787, 'articl': 58, 'fuck': 549, 'caballero': 177, 'head': 632, 'oyster': 916, 'round': 1100, 'disbelief': 369, 'qualifi': 1030, 'version': 1415, 'toler': 1340, 'polit': 988, 'wash': 1437, 'otherwis': 900, 'heat': 636, 'coconut': 246, 'fella': 494, 'huevo': 669, 'ranchero': 1037, 'appeal': 47, 'pricey': 1006, 'temp': 1305, 'glove': 573, 'deep': 333, 'oil': 882, 'pleasur': 982, 'plethora': 983, 'seal': 1135, 'approv': 51, 'colleg': 251, 'class': 237, 'start': 1233, 'edit': 418, 'besid': 116, 'costco': 283, 'uniqu': 1384, 'weird': 1449, 'groceri': 604, 'store': 1249, 'japanes': 714, 'dude': 406, 'doughi': 388, 'inch': 688, 'wire': 1466, 'albondiga': 19, 'tomato': 1341, 'three': 1325, 'occas': 876, 'bloodiest': 140, 'refus': 1063, 'anymor': 37, 'chai': 208, 'allergi': 20, 'warn': 1436, 'clue': 244, 'rotat': 1099, 'concern': 266, 'strawberri': 1252, 'unprofession': 1386, 'patron': 939, 'occasion': 877, 'pat': 937, 'bellagio': 114, 'anticip': 36, 'weak': 1444, 'correct': 281, 'sal': 1111, 'fav': 488, 'unexperienc': 1380, 'steakhous': 1238, 'proper': 1017, 'understand': 1378, 'concept': 265, 'guacamol': 609, 'pur': 1026, 'ed': 415, 'postino': 996, 'poison': 987, 'batch': 99, 'yay': 1488, 'hilari': 647, 'christma': 233, 'eve': 446, 'rememb': 1071, 'biggest': 122, 'entir': 439, 'teamwork': 1302, 'degre': 338, 'ri': 1083, 'calamari': 182, 'fondu': 525, 'lost': 760, 'forev': 528, 'scene': 1130, 'denni': 347, 'downright': 389, 'waaaaaayyyyyyyyyi': 1426, 'sangria': 1120, 'glass': 572, 'ridicul': 1089, 'neat': 848, 'trippi': 1364, 'hurri': 676, 'reserv': 1075, 'stretch': 1254, 'cashew': 200, 'undercook': 1377, 'chipolt': 227, 'ranch': 1036, 'dip': 362, 'saus': 1126, 'douchey': 386, 'indoor': 694, 'garden': 556, 'con': 264, 'spotti': 1226, 'neither': 853, 'ensu': 437, 'bing': 124, 'carb': 193, 'profound': 1014, 'deuchebaggeri': 355, 'smoke': 1190, 'solidifi': 1196, 'ala': 18, 'cart': 196, 'blame': 133, 'del': 339, 'hamburg': 618, 'hell': 639, 'gotten': 586, 'ya': 1486, 'shot': 1166, 'firebal': 507, 'disapppoint': 367, 'heimer': 637, 'caus': 205, 'own': 914, 'vomit': 1423, 'circumst': 235, 'obvious': 875, 'movi': 829, 'ha': 614, 'flop': 519, 'problem': 1010, 'bigger': 121, 'unwrap': 1391, 'mile': 806, 'mirag': 813, 'refri': 1062, 'crusti': 313, 'caterpillar': 203, 'appetit': 49, 'instant': 703, 'ninja': 861, 'pour': 998, 'wound': 1481, 'draw': 394}\n\n\n\n\n[참고 3] 두개의 새로운 문장을 리뷰들로 fit했던 단어들을 vectorizer로 변형시킵니다.\n\n\n>>> X = cv.transform([‘A whole new text zzzzzzzzz’, ‘Something very new new new phrase uh ?’])\n\n\n>>> print X\n(0, 855) 1\n(0, 13428) 1\n(0, 14844) 1\n(0, 15164) 1\n(1, 855) 3\n(1, 9807) 1\n(1, 12374) 1\n(1, 13980) 1\n(1, 14482) 1\n\n\n해석)\n\n입력됐던 문장들이 sparse matrix형태로 변형됩니다.\n\n앞의 tuple은 (i번째 document, j번째 단어(cv.vocabury에서 확인)를 의미하고, 더 쉽게 설명하면, i번째 문서에서, 전체의 단어빈도수 기준으로 j번째 단어를 의미합니다.\nint값은 등장 횟수를 의미합니다.\n\nmatrix이다보니 숫자로만 index가 되어 무엇을 의미하는지는 헷갈리실 수 있습니다.\n\n\n\n[참고 4] fit 과 transform을 한 코드로 만들 수 있습니다.\n\nfit함수를 통해서 각 리뷰들을 대입합니다.\n두개의 새로운 문장을 –> fit(단어그대로) –> transform (vectorizer)로 변형시킵니다.\n먼저 fit 명령어를 통해서 text 변수\n\n(위의 설명에서….### fit 함수를 통해서 각 리뷰들을 대입합니다. (단어 그대로 사용해서) 벡터로 만들어 줍니다.)\n\ntransform 명령어를 통해서 vectorizer로\n따로 따로\n\ncv.fit(corpus)\ncv.transform(corpus)\n\n한번에\n\nx_1 =cv.fit_transform(corpus)\n\n\n\nx_1 =cv.fit_transform(corpus)\n\n\n### Our final vector:\nprint('Full vector: ')\nprint(x_1.toarray())\n### 한 행은 한 문서입니다. 여기서 한 문장, 한 row입니다.\n### 각 열(featrues, column)은 단어를 의미합니다. \n### 여기서는 빈도수가 높은 1500개의 단어를 기준으로 값을 표현한 것입니다. \n### 그것을 다시 알파벳으로 ordering 합니다.\n### 값이 0 해당 문장에 단어가 없다는 의미입니다. \n\nFull vector: \n[[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\n\n\n\nx_1.shape\n### 1000 개의 reviews를 1500개의 단어로 표현합니다.\n\n(1000, 1500)\n\n\n\n### Or if we wanted to get the vector for one word:\nprint('absolut vector: ')\nprint(cv.transform(['absolut']).toarray())\n\nprint('wow vector: ')\nprint(cv.transform(['wow']).toarray())\n### max_features=1500 개 이므로, wow는 그 중 wow가 해당하는 곳에 1로 변형될 것입니다.\n\n### one-hot encoding 개념입니다.\n\nabsolut vector: \n[[1 0 0 ... 0 0 0]]\nwow vector: \n[[0 0 0 ... 0 0 0]]\n\n\n\n### Or if we wanted to get multiple vectors at once to build matrices\nprint('absolut and wow and love: ')\nprint(cv.transform(['absolut','wow', 'love']).toarray())\n\nabsolut and wow and love: \n[[1 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\n\n\n\n\n[참고 5] + toarray() 까지 한번에!!\n\nWe could also do the whole thing at once with the fit_transform method:\n\nprint(‘One swoop:’)\nnew_text = [‘Today is the day that I do the thing today, today’]\nnew_vectorizer = CountVectorizer()\nprint(new_vectorizer.fit_transform(new_text).toarray())\n\n해석)\n\nfit함수를 통해서 각 리뷰(텍스트 기반 텍스트: 빈도수)들을 대입합니다.\n두개의 새로운 문장을 리뷰들로 fit했던 것을 transform 를 통해 vectorizer로 변형시킵니다.\n먼저 fit 명령어를 통해서 text 변수에 저장된 데이터를 학습시켜줘야 합니다.\n\n\n\nx = cv.fit_transform(corpus)\n\n\nx\n### 1000 개의 review에 대해서 one-hot encoding 적용해서 vector화 합니다.\n\n<1000x1500 sparse matrix of type '<class 'numpy.int64'>'\n    with 5324 stored elements in Compressed Sparse Row format>\n\n\n\nx = cv.fit_transform(corpus).toarray()\n### corpus에 들어있는 문장. 즉, 전처리 후에 들어 있는 문장을 의미합니다.\n\n\nprint(x)\n\n[[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\n\n\n\nx.shape\n### (row, column): 데이터 확인\n### 벡터의 모양\n### 1000개의 문장, 1500개 단어로 구성\n\n(1000, 1500)\n\n\n\ny = dataset['Liked'].values\n\n\nprint(y)\n### 1000개의 값(value)가 저장되어 있습니다.\n\n[1 0 0 1 1 0 0 0 1 1 1 0 0 1 0 0 1 0 0 0 0 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1\n 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 0\n 0 0 0 1 1 0 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0\n 0 1 1 1 1 0 0 0 0 0 0 1 1 1 0 0 1 0 1 0 1 1 0 1 1 1 1 0 1 0 0 0 0 1 1 0 0\n 0 0 1 1 0 0 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0 1 0 1 1 1 1 0 0 1 1 0 0 0 0 0 1\n 1 0 1 1 1 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0\n 0 0 0 1 1 1 0 1 1 0 1 0 1 0 0 1 0 1 0 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 1 0 1\n 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 0 1 1 0 0 1 0 0 0 1 0 1 1\n 0 1 0 1 1 0 0 0 1 0 0 0 1 1 1 0 1 0 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 0 0 1 1\n 0 1 1 0 0 1 0 0 1 1 1 0 1 1 1 1 1 0 0 1 0 1 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1\n 0 0 1 1 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1\n 1 1 0 1 1 0 1 0 0 0 1 1 1 1 0 0 0 0 1 1 0 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 0\n 1 1 0 0 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 0 0 0 1 1 1 1 0\n 1 0 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 1 0 0 0 0 0 1 1 1 1 1 1 0 1\n 0 1 1 1 0 0 1 0 0 1 1 1 1 1 1 1 0 1 0 1 0 1 1 0 1 0 0 1 1 0 0 1 1 1 1 0 0\n 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 0 0 0 1 1 0 0 1 1 1 0 0 0 1 0 1 1 0 1 0 1 1\n 1 0 0 1 0 1 1 0 1 0 1 1 1 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 0\n 1 1 1 0 1 0 1 0 1 0 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1\n 1 1 1 0 0 1 0 1 1 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 0 1 1 1 0 0 0 0 0 0 0 1\n 1 1 0 1 0 1 0 1 0 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1\n 1 0 0 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 0 1 0 0 1 0 1\n 0 0 0 1 1 1 0 0 1 0 1 1 1 1 0 0 1 0 1 1 1 0 1 0 1 0 1 1 0 1 0 1 1 0 0 0 0\n 1 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1\n 0 1 1 0 0 1 1 0 0 1 1 0 1 1 1 1 1 1 0 0 0 1 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0\n 1 0 1 1 0 0 1 1 1 1 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0]\n\n\n\ny.shape\n\n(1000,)\n\n\n0. Problem Define \n1. Library import\n2. Data Collection\n3. EDA(Exploratory Data Analysis) : 데이터 탐색\n4. Preprocessing\n    - 4.1 Stemmer \n    - 4.2 Bag of Words : 문장별 단어 벡터 만들기\n    - 4.3 Train : Test 로 나누기\n5. Modeling\n\n\n\n[4-3 데이터 셋 나누기] Train vs. Test\n\nfrom sklearn.model_selection import train_test_split\n\n\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2,random_state = 17)\n#train : test = 8:2\n#같은 랜덤 초기값 사용 같은 랜덤 데이터 추출 옵션 \n\n\n\n[5 Modeling]\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n\nrf = RandomForestClassifier(n_estimators=800) \n### 트리 800개로 구성된 랜덤 포레스트를 만든다.  \n### 즉, 숲을 만들 때 나무의 개수를 의미한다.\n### random forest 는 수많은 작은 decision tree가 모여서 생성된다. \n\n\nTraining the classifier\n\nrf.fit(x_train,y_train)\n\nRandomForestClassifier(n_estimators=800)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier(n_estimators=800)\n\n\n\n\nMaking Predictions\n\ny_train_pred = rf.predict(x_train)\n\n\ny_pred = rf.predict(x_test)\n\n\n\nEvaluating Predictions\n\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n#confusion matrix\n\n\nconfusion_matrix(y_test,y_pred)\n#    + -\n# + \n# - \n#? confusion_matrix\n\narray([[72, 23],\n       [35, 70]], dtype=int64)\n\n\n\nprint(classification_report(y_test,y_pred))\n\n              precision    recall  f1-score   support\n\n           0       0.67      0.76      0.71        95\n           1       0.75      0.67      0.71       105\n\n    accuracy                           0.71       200\n   macro avg       0.71      0.71      0.71       200\nweighted avg       0.71      0.71      0.71       200\n\n\n\n\nprint('Traning Accuracy --->',accuracy_score(y_train,y_train_pred))\nprint('Testing Accuracy --->',accuracy_score(y_test,y_pred))\n\nTraning Accuracy ---> 0.99875\nTesting Accuracy ---> 0.71"
  },
  {
    "objectID": "ITM812/10312023_upload_unsupervisedlearning/2023_Fall_Lecture10_unsupervised_customer_segmentation_10312023_upload.html",
    "href": "ITM812/10312023_upload_unsupervisedlearning/2023_Fall_Lecture10_unsupervised_customer_segmentation_10312023_upload.html",
    "title": "Jiseok AHN",
    "section": "",
    "text": "[Practice 3] : UnSupervised Learning\n0. Problem Define\n1. Library import\n2. Data upload\n3. EDA(Exploratory Data Analysis) : 데이터 탐색\n4. 데이터 전처리\n    - 4.1  Feature selection for the model\n5. 모델링 :: K-Means Algorithm\n    - 5.1 Building the Model: finding out optimal k \n    - 5.2 Visualizating the ELBOW method to get the optimal value of K\n    - 5.3 Building K-Means ALgorithm\n    - 5.4 Visualizing all the clusters \n\n[0]: Problem Define\n쇼핑몰 고객 그룹핑 - Data: Kaggle (https://www.kaggle.com/vjchoudhary7/customer-segmentation-tutorial-in-python)\n\n\n1. Libraries setteting : import libraries\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing\nimport matplotlib.pyplot as plt # visualization\nimport seaborn as sns #visualization\nfrom sklearn.cluster import KMeans\n\n\n\n2. Data Upload\n\n#import the dataset\ndataset = pd.read_csv('./data/Mall_Customers.csv')\n\n\ndataset.head(5) #Print first 10 rows of the dataset.\n\n\n\n\n\n  \n    \n      \n      CustomerID\n      Gender\n      Age\n      Annual Income (k$)\n      Spending Score (1-100)\n    \n  \n  \n    \n      0\n      1\n      Male\n      19\n      15\n      39\n    \n    \n      1\n      2\n      Male\n      21\n      15\n      81\n    \n    \n      2\n      3\n      Female\n      20\n      16\n      6\n    \n    \n      3\n      4\n      Female\n      23\n      16\n      77\n    \n    \n      4\n      5\n      Female\n      31\n      17\n      40\n    \n  \n\n\n\n\n\n#total rows and colums in the dataset\ndataset.shape\n\n(200, 5)\n\n\n\n\n3. 데이터 탐색(EDA: Exploratory Data Analysis) : 요약 통계량 확인하기\n\npandas 로 불러온 데이터 살펴보기: head(), shape(), info(), describe(), value_counts(), unique() 등\nhttps://hogni.tistory.com/5\n\n\ndataset.describe()\n\n\n\n\n\n  \n    \n      \n      CustomerID\n      Age\n      Annual Income (k$)\n      Spending Score (1-100)\n    \n  \n  \n    \n      count\n      200.000000\n      200.000000\n      200.000000\n      200.000000\n    \n    \n      mean\n      100.500000\n      38.850000\n      60.560000\n      50.200000\n    \n    \n      std\n      57.879185\n      13.969007\n      26.264721\n      25.823522\n    \n    \n      min\n      1.000000\n      18.000000\n      15.000000\n      1.000000\n    \n    \n      25%\n      50.750000\n      28.750000\n      41.500000\n      34.750000\n    \n    \n      50%\n      100.500000\n      36.000000\n      61.500000\n      50.000000\n    \n    \n      75%\n      150.250000\n      49.000000\n      78.000000\n      73.000000\n    \n    \n      max\n      200.000000\n      70.000000\n      137.000000\n      99.000000\n    \n  \n\n\n\n\n\ndataset.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 200 entries, 0 to 199\nData columns (total 5 columns):\n #   Column                  Non-Null Count  Dtype \n---  ------                  --------------  ----- \n 0   CustomerID              200 non-null    int64 \n 1   Gender                  200 non-null    object\n 2   Age                     200 non-null    int64 \n 3   Annual Income (k$)      200 non-null    int64 \n 4   Spending Score (1-100)  200 non-null    int64 \ndtypes: int64(4), object(1)\nmemory usage: 7.9+ KB\n\n\n\ndataset.isnull().sum()\n\nCustomerID                0\nGender                    0\nAge                       0\nAnnual Income (k$)        0\nSpending Score (1-100)    0\ndtype: int64\n\n\n\n\n4. 데이터 전처리\n\n\n4.1 Feature selection for the model : “특성/속성/컬럼/x/독립변수/설명변수”선정하기\n\n여기서는 2개의 변수만을 사용합니다.:Annual income and Spending Score\n라벨링/정답지가 없습니다.\n\n\ndataset.head()\n\n\n\n\n\n  \n    \n      \n      CustomerID\n      Gender\n      Age\n      Annual Income (k$)\n      Spending Score (1-100)\n    \n  \n  \n    \n      0\n      1\n      Male\n      19\n      15\n      39\n    \n    \n      1\n      2\n      Male\n      21\n      15\n      81\n    \n    \n      2\n      3\n      Female\n      20\n      16\n      6\n    \n    \n      3\n      4\n      Female\n      23\n      16\n      77\n    \n    \n      4\n      5\n      Female\n      31\n      17\n      40\n    \n  \n\n\n\n\n\n—————————————————————————————————\n\n\n\n(참고 1) 특정 행과 열을 선택해서 살펴보기\n\nloc\n\n\n데이터프레임의 행이나 컬럼에 label이나 boolean array로 접근.\nlocation의 약어로, 인간이 읽을 수 있는 label 값으로 데이터에 접근하는 것이다.\n\n\niloc\n\n\n데이터프레임의 행이나 컬럼에 인덱스 값으로 접근.\ninteger location의 약어로, 컴퓨터가 읽을 수 있는 indexing 값으로 데이터에 접근하는 것이다.\n\n\n\n(참고 1) 예제\n\ndf = pd.DataFrame({ \n    \"bid_id\": [1, 2, 3], \n    \"bidder_id\": [\"Gadi\", \"Conda\", \"Lion\"], \n    \"city\": [\"Seoul\", \"LA\", \"Sydney\"], \n    \"item\": [\"TV\", \"jewelry\", \"book\"]}).set_index(\"bid_id\") \ndf\n\n\n\n\n\n  \n    \n      \n      bidder_id\n      city\n      item\n    \n    \n      bid_id\n      \n      \n      \n    \n  \n  \n    \n      1\n      Gadi\n      Seoul\n      TV\n    \n    \n      2\n      Conda\n      LA\n      jewelry\n    \n    \n      3\n      Lion\n      Sydney\n      book\n    \n  \n\n\n\n\n\ndf.iloc[0] # 첫번째 행\n\nbidder_id     Gadi\ncity         Seoul\nitem            TV\nName: 1, dtype: object\n\n\n\ndf.iloc[1] # 두번째 행\n\nbidder_id      Conda\ncity              LA\nitem         jewelry\nName: 2, dtype: object\n\n\n\ndf.iloc[:,0] # 첫번째 열\n\nbid_id\n1     Gadi\n2    Conda\n3     Lion\nName: bidder_id, dtype: object\n\n\n\ndf.iloc[:,1] # 두번째 열\n\nbid_id\n1     Seoul\n2        LA\n3    Sydney\nName: city, dtype: object\n\n\n\ndf.loc[:, \"city\"] # 레이블이 city인 열\n\nbid_id\n1     Seoul\n2        LA\n3    Sydney\nName: city, dtype: object\n\n\n\n—————————————————————————————————\n\ndataset.head()\n\n\n\n\n\n  \n    \n      \n      CustomerID\n      Gender\n      Age\n      Annual Income (k$)\n      Spending Score (1-100)\n    \n  \n  \n    \n      0\n      1\n      Male\n      19\n      15\n      39\n    \n    \n      1\n      2\n      Male\n      21\n      15\n      81\n    \n    \n      2\n      3\n      Female\n      20\n      16\n      6\n    \n    \n      3\n      4\n      Female\n      23\n      16\n      77\n    \n    \n      4\n      5\n      Female\n      31\n      17\n      40\n    \n  \n\n\n\n\n\nX = dataset.iloc[:, [3,4]].values\n# Annualincome and Spending Score (4번째, 5번째 열)\n\n\nXX = dataset.loc[:, [\"Annual Income (k$)\", \"Spending Score (1-100)\"]]\n\n\nX\n# data checking\n\narray([[ 15,  39],\n       [ 15,  81],\n       [ 16,   6],\n       [ 16,  77],\n       [ 17,  40],\n       [ 17,  76],\n       [ 18,   6],\n       [ 18,  94],\n       [ 19,   3],\n       [ 19,  72],\n       [ 19,  14],\n       [ 19,  99],\n       [ 20,  15],\n       [ 20,  77],\n       [ 20,  13],\n       [ 20,  79],\n       [ 21,  35],\n       [ 21,  66],\n       [ 23,  29],\n       [ 23,  98],\n       [ 24,  35],\n       [ 24,  73],\n       [ 25,   5],\n       [ 25,  73],\n       [ 28,  14],\n       [ 28,  82],\n       [ 28,  32],\n       [ 28,  61],\n       [ 29,  31],\n       [ 29,  87],\n       [ 30,   4],\n       [ 30,  73],\n       [ 33,   4],\n       [ 33,  92],\n       [ 33,  14],\n       [ 33,  81],\n       [ 34,  17],\n       [ 34,  73],\n       [ 37,  26],\n       [ 37,  75],\n       [ 38,  35],\n       [ 38,  92],\n       [ 39,  36],\n       [ 39,  61],\n       [ 39,  28],\n       [ 39,  65],\n       [ 40,  55],\n       [ 40,  47],\n       [ 40,  42],\n       [ 40,  42],\n       [ 42,  52],\n       [ 42,  60],\n       [ 43,  54],\n       [ 43,  60],\n       [ 43,  45],\n       [ 43,  41],\n       [ 44,  50],\n       [ 44,  46],\n       [ 46,  51],\n       [ 46,  46],\n       [ 46,  56],\n       [ 46,  55],\n       [ 47,  52],\n       [ 47,  59],\n       [ 48,  51],\n       [ 48,  59],\n       [ 48,  50],\n       [ 48,  48],\n       [ 48,  59],\n       [ 48,  47],\n       [ 49,  55],\n       [ 49,  42],\n       [ 50,  49],\n       [ 50,  56],\n       [ 54,  47],\n       [ 54,  54],\n       [ 54,  53],\n       [ 54,  48],\n       [ 54,  52],\n       [ 54,  42],\n       [ 54,  51],\n       [ 54,  55],\n       [ 54,  41],\n       [ 54,  44],\n       [ 54,  57],\n       [ 54,  46],\n       [ 57,  58],\n       [ 57,  55],\n       [ 58,  60],\n       [ 58,  46],\n       [ 59,  55],\n       [ 59,  41],\n       [ 60,  49],\n       [ 60,  40],\n       [ 60,  42],\n       [ 60,  52],\n       [ 60,  47],\n       [ 60,  50],\n       [ 61,  42],\n       [ 61,  49],\n       [ 62,  41],\n       [ 62,  48],\n       [ 62,  59],\n       [ 62,  55],\n       [ 62,  56],\n       [ 62,  42],\n       [ 63,  50],\n       [ 63,  46],\n       [ 63,  43],\n       [ 63,  48],\n       [ 63,  52],\n       [ 63,  54],\n       [ 64,  42],\n       [ 64,  46],\n       [ 65,  48],\n       [ 65,  50],\n       [ 65,  43],\n       [ 65,  59],\n       [ 67,  43],\n       [ 67,  57],\n       [ 67,  56],\n       [ 67,  40],\n       [ 69,  58],\n       [ 69,  91],\n       [ 70,  29],\n       [ 70,  77],\n       [ 71,  35],\n       [ 71,  95],\n       [ 71,  11],\n       [ 71,  75],\n       [ 71,   9],\n       [ 71,  75],\n       [ 72,  34],\n       [ 72,  71],\n       [ 73,   5],\n       [ 73,  88],\n       [ 73,   7],\n       [ 73,  73],\n       [ 74,  10],\n       [ 74,  72],\n       [ 75,   5],\n       [ 75,  93],\n       [ 76,  40],\n       [ 76,  87],\n       [ 77,  12],\n       [ 77,  97],\n       [ 77,  36],\n       [ 77,  74],\n       [ 78,  22],\n       [ 78,  90],\n       [ 78,  17],\n       [ 78,  88],\n       [ 78,  20],\n       [ 78,  76],\n       [ 78,  16],\n       [ 78,  89],\n       [ 78,   1],\n       [ 78,  78],\n       [ 78,   1],\n       [ 78,  73],\n       [ 79,  35],\n       [ 79,  83],\n       [ 81,   5],\n       [ 81,  93],\n       [ 85,  26],\n       [ 85,  75],\n       [ 86,  20],\n       [ 86,  95],\n       [ 87,  27],\n       [ 87,  63],\n       [ 87,  13],\n       [ 87,  75],\n       [ 87,  10],\n       [ 87,  92],\n       [ 88,  13],\n       [ 88,  86],\n       [ 88,  15],\n       [ 88,  69],\n       [ 93,  14],\n       [ 93,  90],\n       [ 97,  32],\n       [ 97,  86],\n       [ 98,  15],\n       [ 98,  88],\n       [ 99,  39],\n       [ 99,  97],\n       [101,  24],\n       [101,  68],\n       [103,  17],\n       [103,  85],\n       [103,  23],\n       [103,  69],\n       [113,   8],\n       [113,  91],\n       [120,  16],\n       [120,  79],\n       [126,  28],\n       [126,  74],\n       [137,  18],\n       [137,  83]], dtype=int64)\n\n\n\nXX\n\n\n\n\n\n  \n    \n      \n      Annual Income (k$)\n      Spending Score (1-100)\n    \n  \n  \n    \n      0\n      15\n      39\n    \n    \n      1\n      15\n      81\n    \n    \n      2\n      16\n      6\n    \n    \n      3\n      16\n      77\n    \n    \n      4\n      17\n      40\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      195\n      120\n      79\n    \n    \n      196\n      126\n      28\n    \n    \n      197\n      126\n      74\n    \n    \n      198\n      137\n      18\n    \n    \n      199\n      137\n      83\n    \n  \n\n200 rows × 2 columns\n\n\n\n\n\n————————————————————————————\n0. Problem Define\n1. Library import\n2. Data upload\n3. EDA(Exploratory Data Analysis) : 데이터 탐색\n4. 데이터 전처리\n    - 4.1  Feature selection for the model\n5. 모델링 :: K-Means Algorithm\n    - 5.1 Building the Model: finding out optimal k \n    - 5.2 Visualizating the ELBOW method to get the optimal value of K\n    - 5.3 Building K-Means ALgorithm\n    - 5.4 Visualizing all the clusters \n\n\n————————————————————————————\n\n\n\n5. 모델링 : K-Means Algorithm\n\n\n5.1 Building the Model : k 값을 정합니다.\n\nk-means 알고리즘 최적의 k 값을 구해서 clustering 하는 것입니다. 최적의 k 값을 찾기위한 방법으로 팔꿈치(elbow) 법칙?? 을 사용합니다.\nelbow method 란 cluster 간의 거리의 합이 급격히 떨어지는 구간이 생기게 됩니다. 그때 k 값을 군집의 개수로 사용합니다.\n참고 사이트: https://steadiness-193.tistory.com/285\n\n\nfrom sklearn.cluster import KMeans\nWCSS=[]\n\n### Within-Cluster-Sum-of-Squares (WCSS)\n### https://analyticsindiamag.com/beginners-guide-to-k-means-clustering/\n### 설명과 애니매이션으로 clustering에 대해서 잘 설명 되어 있습니다. \n### 그림만 따라가도.. 아하!! 하고 이해가 되실 것 같습니다. \n\n\nWCSS\n\n[]\n\n\n\nfor i in range (1,11):\n    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=0)\n    kmeans.fit(X)\n    interia=kmeans.inertia_\n    #### inertia_ is the formula used to segregate the data points into clusters\n    #### inertia = 관성\n    #### Cluster 간의 거리의 합을 나타내는 inertia가 급격히 떨어지는 구간이 생기는데 \n    #### 이 지점의 K 값을 군집의 개수로 사용 inertia_속성으로 확인할 수 있다 \n    print('k:', i, 'inertia:', interia)\n    WCSS.append(kmeans.inertia_)\n\nC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n  warnings.warn(\n\n\nk: 1 inertia: 269981.28\n\n\nC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n  warnings.warn(\n\n\nk: 2 inertia: 181363.59595959596\n\n\nC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n  warnings.warn(\n\n\nk: 3 inertia: 106348.37306211119\n\n\nC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n  warnings.warn(\n\n\nk: 4 inertia: 73679.78903948834\n\n\nC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n  warnings.warn(\n\n\nk: 5 inertia: 44448.45544793371\n\n\nC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n  warnings.warn(\n\n\nk: 6 inertia: 37265.86520484346\n\n\nC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n  warnings.warn(\n\n\nk: 7 inertia: 30259.65720728547\n\n\nC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n  warnings.warn(\n\n\nk: 8 inertia: 25050.832307547527\n\n\nC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n  warnings.warn(\n\n\nk: 9 inertia: 21862.09267218289\n\n\nC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n  warnings.warn(\n\n\nk: 10 inertia: 19657.783608703958\n\n\n\n### numpy version should be downgraded 1.21.4 \n### https://www.datasciencelearner.com/how-to-downgrade-numpy-steps/\n### or \n### https://mydevjourney.tistory.com/76\n### !pip install threadpoolctl==3.1.0\n\n\n————————————————————————————\n\n\n참고 1) 변수 설명\n\nn_clusters: k-means의 k를 의미하는 군집형성의 개수를 뜻합니다.\nrandom_state를 통해서 난수 고정을하는 것을 의미합니다. 학습결과의 동일성을 위해서입니다.\n참고사이트 및 sklearn 에서 KMeans clustering site: https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n\ninit{‘k-means++’, ‘random’},\n\n‘k-means++’ : selects initial cluster centers for k-mean clustering in a smart way to speed up convergence.\n‘random’: choose n_clusters observations (rows) at random from data for the initial centroids.\ndefault는 ‘k-means++’ 입니다.\n\n\n\n\n\n————————————————————————————\n0. Problem Define\n1. Library import\n2. Data upload\n3. EDA(Exploratory Data Analysis) : 데이터 탐색\n4. 데이터 전처리\n    - 4.1  Feature selection for the model\n5. 모델링 :: K-Means Algorithm\n    - 5.1 Building the Model: finding out optimal k \n    - 5.2 Visualizating the ELBOW method to get the optimal value of K\n    - 5.3 Building K-Means ALgorithm\n    - 5.4 Visualizing all the clusters \n\n\n————————————————————————————\n\n\n\n5.2 Visualizating the ELBOW method to get the optimal value of K\n\n최적의 k 값을 찾기위한 방법으로 팔꿈치(elbow) 법칙?? 을 사용합니다.\nelbow method 란 cluster 간의 거리의 합이 급격히 떨어지는 구간이 생기게 됩니다. 그때 k 값을 군집의 개수로 사용합니다.\n\n\n### 가정 ###\n\n### The max number of cluster would be 10.\n\n### for i in range(1,11):\n###    kmeans = KMeans(n_clusters= i, init='k-means++', random_state=0)\n###    kmeans.fit(X)\n###    WCSS.append(kmeans.inertia_)\n\n\nplt.plot(range(1,11), WCSS)\n\nplt.title('The Elbow Method')\nplt.xlabel('no of clusters')\nplt.ylabel('WCSS')\nplt.show()\n\n\n\n\n\n해석 )\n\nIf you zoom out this curve then you will see that last elbow comes at k=5\nno matter what range we select\n\nex) (1,21) also i will see the same behavior but if we chose higher range it is little difficult to visualize the ELBOW\n\nFinally, we got that k=5\n\n\n\n————————————————————————————\n0. Problem Define\n1. Library import\n2. Data upload\n3. EDA(Exploratory Data Analysis) : 데이터 탐색\n4. 데이터 전처리\n    - 4.1  Feature selection for the model\n5. 모델링 :: K-Means Algorithm\n    - 5.1 Building the Model: finding out optimal k \n    - 5.2 Visualizating the ELBOW method to get the optimal value of K\n    - 5.3 Building K-Means ALgorithm\n    - 5.4 Visualizing all the clusters \n\n\n————————————————————————————\n\n\n\n5.3 Building K-Means ALgorithm\n\nkmeansmodel = KMeans(n_clusters= 5, init='k-means++', max_iter=300, random_state=0)\ny_kmeans= kmeansmodel.fit_predict(X)\ny_kmeans\n\nC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n  warnings.warn(\n\n\narray([3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4,\n       3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 0,\n       3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 0, 1, 2, 1, 2, 1,\n       0, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1,\n       2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1,\n       2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1,\n       2, 1])\n\n\n\nX[0:3,]\n\narray([[15, 39],\n       [15, 81],\n       [16,  6]], dtype=int64)\n\n\n\ndataset.head(3)\n\n\n\n\n\n  \n    \n      \n      CustomerID\n      Gender\n      Age\n      Annual Income (k$)\n      Spending Score (1-100)\n    \n  \n  \n    \n      0\n      1\n      Male\n      19\n      15\n      39\n    \n    \n      1\n      2\n      Male\n      21\n      15\n      81\n    \n    \n      2\n      3\n      Female\n      20\n      16\n      6\n    \n  \n\n\n\n\n\nXX.head(3)\n\n\n\n\n\n  \n    \n      \n      Annual Income (k$)\n      Spending Score (1-100)\n    \n  \n  \n    \n      0\n      15\n      39\n    \n    \n      1\n      15\n      81\n    \n    \n      2\n      16\n      6\n    \n  \n\n\n\n\n\n해석)\n\nFor unsupervised learning we use “fit_predict()” wherein for supervised learning we use “fit_tranform()”\ny_kmeans is the final model.\n\nincome: 15, score: 39 는 그룹 4에 속한다.\nX = dataset.iloc[:, [3,4]].values —-> Annualincome and Spending Score\n\n\n\n\n————————————————————————————\n0. Problem Define\n1. Library import\n2. Data upload\n3. EDA(Exploratory Data Analysis) : 데이터 탐색\n4. 데이터 전처리\n    - 4.1  Feature selection for the model\n5. 모델링 :: K-Means Algorithm\n    - 5.1 Building the Model: finding out optimal k \n    - 5.2 Visualizating the ELBOW method to get the optimal value of K\n    - 5.3 Building K-Means ALgorithm\n    - 5.4 Visualizing all the clusters \n\n\n————————————————————————————\n\n\n\n5.4 Visualizing all the clusters\n\n### 참고 사이트:  https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=tt2t2am1118&logNo=221183481798\n\nplt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0, 1], s = 10, c = 'red', label = 'Cluster 1')\nplt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1, 1], s = 100, c = 'blue', label = 'Cluster 2')\nplt.scatter(X[y_kmeans == 2, 0], X[y_kmeans == 2, 1], s = 100, c = 'green', label = 'Cluster 3')\nplt.scatter(X[y_kmeans == 3, 0], X[y_kmeans == 3, 1], s = 100, c = 'cyan', label = 'Cluster 4')\nplt.scatter(X[y_kmeans == 4, 0], X[y_kmeans == 4, 1], s = 100, c = 'magenta', label = 'Cluster 5')\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 300, c = 'yellow', label = 'Centroids')\n\nplt.title('Clusters of customers')\nplt.xlabel('Annual Income (k$)')\nplt.ylabel('Spending Score (1-100)')\nplt.legend()\nplt.show()\n\n\n\n\n\ny_kmeans\n\narray([3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4,\n       3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 0,\n       3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 0, 1, 2, 1, 2, 1,\n       0, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1,\n       2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1,\n       2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1,\n       2, 1])\n\n\n\nX[y_kmeans == 0, 0] ## Annual Income (k$)\n\narray([39, 40, 40, 40, 40, 42, 42, 43, 43, 43, 43, 44, 44, 46, 46, 46, 46,\n       47, 47, 48, 48, 48, 48, 48, 48, 49, 49, 50, 50, 54, 54, 54, 54, 54,\n       54, 54, 54, 54, 54, 54, 54, 57, 57, 58, 58, 59, 59, 60, 60, 60, 60,\n       60, 60, 61, 61, 62, 62, 62, 62, 62, 62, 63, 63, 63, 63, 63, 63, 64,\n       64, 65, 65, 65, 65, 67, 67, 67, 67, 69, 71, 72, 76], dtype=int64)\n\n\n\nX[y_kmeans == 4, 0] ## Annual Income (k$)\n\narray([15, 16, 17, 18, 19, 19, 20, 20, 21, 23, 24, 25, 28, 28, 29, 30, 33,\n       33, 34, 37, 38, 39], dtype=int64)\n\n\n\nXX.head(3)\n\n\n\n\n\n  \n    \n      \n      Annual Income (k$)\n      Spending Score (1-100)\n    \n  \n  \n    \n      0\n      15\n      39\n    \n    \n      1\n      15\n      81\n    \n    \n      2\n      16\n      6\n    \n  \n\n\n\n\n\nX[y_kmeans == 4, 1] ##Spending Score (1-100)\n\narray([81, 77, 76, 94, 72, 99, 77, 79, 66, 98, 73, 73, 82, 61, 87, 73, 92,\n       81, 73, 75, 92, 65], dtype=int64)\n\n\n\nX[y_kmeans == 0, 1] ##Spending Score (1-100)\n\narray([61, 55, 47, 42, 42, 52, 60, 54, 60, 45, 41, 50, 46, 51, 46, 56, 55,\n       52, 59, 51, 59, 50, 48, 59, 47, 55, 42, 49, 56, 47, 54, 53, 48, 52,\n       42, 51, 55, 41, 44, 57, 46, 58, 55, 60, 46, 55, 41, 49, 40, 42, 52,\n       47, 50, 42, 49, 41, 48, 59, 55, 56, 42, 50, 46, 43, 48, 52, 54, 42,\n       46, 48, 50, 43, 59, 43, 57, 56, 40, 58, 35, 34, 40], dtype=int64)\n\n\n\nfrom IPython.display import Image\n\nImage(\"kmeans.png\", width=600, height=400)\n\nFileNotFoundError: No such file or directory: 'kmeans.png'\n\n\nFileNotFoundError: No such file or directory: 'kmeans.png'\n\n\n<IPython.core.display.Image object>\n\n\n\n\n모델해석하기)\n\nCluster 1 (Red Color) -> earning high but spending less\ncluster 2 (Blue Colr) -> average in terms of earning and spending\ncluster 3 (Green Color) -> earning high and also spending high\n\n만약 쇼핑몰에서 매출을 올리면? 이 그룹을 타켓팅해서 올려야 될 것입니다.\n이 고객 그룹에게 마케팅을 해야 될 것입니다.\n분석의 insight 결론이 될 것입니다.\n\ncluster 4 (cyan Color) -> earning less but spending more\nCluster 5 (magenta Color) -> Earning less , spending less"
  },
  {
    "objectID": "research_slide.html#background",
    "href": "research_slide.html#background",
    "title": "Energy Transition: Delivering Energy Infrastructure Projects On Time and On Budget",
    "section": "Background",
    "text": "Background\nS.Korea reverses the nuclear phase-out policy of the previous administration.\n\nElectricity demand will increase with the rapid electrification and with the increase in investments in cutting-edge industries.\nSupply capacity needs to be expanded for stable electricity supply. Increasing the possibility of building new nuclear power plants.\nIt is estimated that nuclear power will make up a greater part of overall power generation for 2038 once the final version of the next basic plan contains plans to build new nuclear power plants.\nHowever, experts are pointing out that sufficient review and discussion should precede construction projects for large-scale nuclear power plants."
  },
  {
    "objectID": "research_slide.html#motivation",
    "href": "research_slide.html#motivation",
    "title": "Energy Transition: Delivering Energy Infrastructure Projects On Time and On Budget",
    "section": "Motivation",
    "text": "Motivation\nElectricity infrastructure is prone to cost overrun issues almost independently of technology or location.\n\n\nhydroelectric dams and nuclear reactors have the greatest amount and frequency of cost overruns. (Sovacool, Nugent, and Gilbert 2014).\nProjections of nuclear plant costs have repeatedly failed to predict the cost overruns observed(Sovacool, Gilbert, and Nugent 2014).\n(Eash-Gates et al. 2020) suggests a bottom-up cost modeling mechanisms to identify the rise in nuclear construction costs over the past five decades."
  },
  {
    "objectID": "research_slide.html#research-questions",
    "href": "research_slide.html#research-questions",
    "title": "Energy Transition: Delivering Energy Infrastructure Projects On Time and On Budget",
    "section": "Research questions",
    "text": "Research questions\n\n\n\n\n\n\nResearch Questions are as follows\n\n\n\nHow does cpaturing the cost of nuclear capacity expansion affects of its share of South Korea’s electricity generation over time?\nHow does the endogeneous modelling of nuclear power plants affect the electrification of end-use sectors?\nHow does the inclusion of historically observed cost overruns for South Korea’s nuclear power plants affect electricity generation and end-use sectors?"
  },
  {
    "objectID": "research_slide.html#research-objectives",
    "href": "research_slide.html#research-objectives",
    "title": "Energy Transition: Delivering Energy Infrastructure Projects On Time and On Budget",
    "section": "Research Objectives",
    "text": "Research Objectives\n\nFiscal Impacts of industry collapse\nLocal fossil fuel econommy\nPublic finance redistribution - Fiscal federalism"
  },
  {
    "objectID": "research_slide.html#literature-review",
    "href": "research_slide.html#literature-review",
    "title": "Energy Transition: Delivering Energy Infrastructure Projects On Time and On Budget",
    "section": "Literature Review",
    "text": "Literature Review\nElectricity infrastructure is prone to cost overrun issues almost independently of technology or location.\n\nhydroelectric dams and nuclear reactors have the greatest amount and frequency of cost overruns. (Sovacool, Nugent, and Gilbert 2014).\nProjections of nuclear plant costs have repeatedly failed to predict the cost overruns observed(Sovacool, Gilbert, and Nugent 2014).\nBottom-up cost modeling mechanisms to identify the rise in nuclear construction costs over the past five decades have also been studied(Eash-Gates et al. 2020)."
  },
  {
    "objectID": "research_slide.html#references",
    "href": "research_slide.html#references",
    "title": "Energy Transition: Delivering Energy Infrastructure Projects On Time and On Budget",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nArbuckle, Evan J., Matthew Binsted, Evan G. R. Davies, Diego V. Chiappori, Candelaria Bergero, Muhammad-Shahid Siddiqui, Christopher Roney, Haewon C. McJeon, Yuyu Zhou, and Nick Macaluso. 2021. “Insights for Canadian Electricity Generation Planning from an Integrated Assessment Model: Should We Be More Cautious about Hydropower Cost Overruns?” Energy Policy 150 (March): 112138. https://doi.org/10.1016/j.enpol.2021.112138.\n\n\nCarvajal, Pablo E., and Francis G. N. Li. 2019. “Challenges for Hydropower-Based Nationally Determined Contributions: A Case Study for Ecuador.” Climate Policy 19 (8): 974–87. https://doi.org/10.1080/14693062.2019.1617667.\n\n\nEash-Gates, Philip, Magdalena M. Klemun, Goksin Kavlak, James McNerney, Jacopo Buongiorno, and Jessika E. Trancik. 2020. “Sources of Cost Overrun in Nuclear Power Plant Construction Call for a New Approach to Engineering Design.” Joule 4 (11): 2348–73. https://doi.org/10.1016/j.joule.2020.10.001.\n\n\nKöberle, Alexandre C., Rafael Garaffa, Bruno S. L. Cunha, Pedro Rochedo, André F. P. Lucena, Alexandre Szklo, and Roberto Schaeffer. 2018. “Are Conventional Energy Megaprojects Competitive? Suboptimal Decisions Related to Cost Overruns in Brazil.” Energy Policy 122 (November): 689–700. https://doi.org/10.1016/j.enpol.2018.08.021.\n\n\nSovacool, Benjamin K., Alex Gilbert, and Daniel Nugent. 2014. “Risk, Innovation, Electricity Infrastructure and Construction Cost Overruns: Testing Six Hypotheses.” Energy 74 (September): 906–17. https://doi.org/10.1016/j.energy.2014.07.070.\n\n\nSovacool, Benjamin K., Daniel Nugent, and Alex Gilbert. 2014. “Construction Cost Overruns and Electricity Infrastructure: An Unavoidable Risk?” The Electricity Journal 27 (4): 112–20. https://doi.org/10.1016/j.tej.2014.03.015."
  },
  {
    "objectID": "research_slide.html#references-.scrollable-.smallerz",
    "href": "research_slide.html#references-.scrollable-.smallerz",
    "title": "Energy Transition: Delivering Energy Infrastructure Projects On Time and On Budget",
    "section": "References {.scrollable .smaller}z",
    "text": "References {.scrollable .smaller}z\n\n\n\n\n\n\nArbuckle, Evan J., Matthew Binsted, Evan G. R. Davies, Diego V. Chiappori, Candelaria Bergero, Muhammad-Shahid Siddiqui, Christopher Roney, Haewon C. McJeon, Yuyu Zhou, and Nick Macaluso. 2021. “Insights for Canadian Electricity Generation Planning from an Integrated Assessment Model: Should We Be More Cautious about Hydropower Cost Overruns?” Energy Policy 150 (March): 112138. https://doi.org/10.1016/j.enpol.2021.112138.\n\n\nEash-Gates, Philip, Magdalena M. Klemun, Goksin Kavlak, James McNerney, Jacopo Buongiorno, and Jessika E. Trancik. 2020. “Sources of Cost Overrun in Nuclear Power Plant Construction Call for a New Approach to Engineering Design.” Joule 4 (11): 2348–73. https://doi.org/10.1016/j.joule.2020.10.001.\n\n\nSovacool, Benjamin K., Alex Gilbert, and Daniel Nugent. 2014. “Risk, Innovation, Electricity Infrastructure and Construction Cost Overruns: Testing Six Hypotheses.” Energy 74 (September): 906–17. https://doi.org/10.1016/j.energy.2014.07.070.\n\n\nSovacool, Benjamin K., Daniel Nugent, and Alex Gilbert. 2014. “Construction Cost Overruns and Electricity Infrastructure: An Unavoidable Risk?” The Electricity Journal 27 (4): 112–20. https://doi.org/10.1016/j.tej.2014.03.015."
  },
  {
    "objectID": "research_slide.html#previous-studies",
    "href": "research_slide.html#previous-studies",
    "title": "Energy Transition: Delivering Energy Infrastructure Projects On Time and On Budget",
    "section": "Previous Studies",
    "text": "Previous Studies\nCost overrun expectations are usually not considered in IAMs, but few have been implemented in some regional analysis.\n\nA case study of Ecuador uses a partial equilibrium energy system optimization model(TIMES) to address uncertainties around investment cost overruns(Carvajal and Li 2019).\nUsing the MESSAGE model(COPPE-MSB), (Köberle et al. 2018) analyzes the effects of cost overruns and construction delays through 2050 on the results of a cost-optimal energy infrastructure expansion model for Brazil.\n(Arbuckle et al. 2021) provides the reason to be more cautious about cost overruns in electricity generation planning. -canada"
  },
  {
    "objectID": "research_slide.html#future-work",
    "href": "research_slide.html#future-work",
    "title": "Energy Transition: Delivering Energy Infrastructure Projects On Time and On Budget",
    "section": "Future Work",
    "text": "Future Work"
  }
]